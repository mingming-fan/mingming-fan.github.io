<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=generator content="Wowchemy 5.7.0 for Hugo"><link rel=stylesheet href=../css/vendor-bundle.min.16f785cdb553c8c4431db6775122af35.css media=print onload='this.media="all"'><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/academicons@1.9.2/css/academicons.min.css integrity="sha512-KlJCpRsLf+KKu2VQa5vmRuClRFjxc5lXO03ixZt82HZUk41+1I0bD8KBSA0fY290ayMfWYI9udIqeOWSu1/uZg==" crossorigin=anonymous media=print onload='this.media="all"'><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/leaflet@1.7.1/dist/leaflet.min.css integrity crossorigin=anonymous media=print onload='this.media="all"'><link rel=stylesheet href=../css/wowchemy.b9231f75dcc97a371ce6141b4d2aadaf.css><link rel=stylesheet href=../css/libs/chroma/github-light.min.css title=hl-light media=print onload='this.media="all"'><link rel=stylesheet href=../css/libs/chroma/dracula.min.css title=hl-dark media=print onload='this.media="all"' disabled><meta name=author content="Mingming Fan"><meta name=description content="A highly-customizable Hugo research group theme powered by Wowchemy website builder."><link rel=alternate hreflang=zh href=../zh/publication/><link rel=alternate hreflang=en-us href=../publication/><link rel=canonical href=../publication/><link rel=manifest href=../manifest.webmanifest><link rel=icon type=image/png href=../media/icon_hu9a55cf1972a19f5af2e1cfae94af68a2_11598_32x32_fill_lanczos_center_3.png><link rel=apple-touch-icon type=image/png href=../media/icon_hu9a55cf1972a19f5af2e1cfae94af68a2_11598_180x180_fill_lanczos_center_3.png><meta name=theme-color content="#1565c0"><meta property="twitter:card" content="summary"><meta property="twitter:site" content="@wowchemy"><meta property="twitter:creator" content="@wowchemy"><meta property="twitter:image" content="/media/logo_hue0215fe06ef33ffe1f8918f47f6e164c_38928_300x300_fit_lanczos_3.png"><meta property="og:site_name" content="APEX, HKUST(GZ) & HKUST"><meta property="og:url" content="/publication/"><meta property="og:title" content="Publications | APEX, HKUST(GZ) & HKUST"><meta property="og:description" content="A highly-customizable Hugo research group theme powered by Wowchemy website builder."><meta property="og:image" content="/media/logo_hue0215fe06ef33ffe1f8918f47f6e164c_38928_300x300_fit_lanczos_3.png"><meta property="og:locale" content="en-us"><meta property="og:updated_time" content="2025-02-22T00:00:00+00:00"><link rel=alternate href=../publication/index.xml type=application/rss+xml title="APEX, HKUST(GZ) & HKUST"><title>Publications | APEX, HKUST(GZ) & HKUST</title></head><body id=top data-spy=scroll data-offset=70 data-target=#TableOfContents class=page-wrapper data-wc-page-id=3a079e7dad19be978a318345a7749d34><script src=../js/wowchemy-init.min.fe8634e7d00f14d07fb33caf14cc8e55.js></script><aside class=search-modal id=search><div class=container><section class=search-header><div class="row no-gutters justify-content-between mb-3"><div class=col-6><h1>Search</h1></div><div class="col-6 col-search-close"><a class=js-search href=# aria-label=Close><i class="fas fa-times-circle text-muted" aria-hidden=true></i></a></div></div><div id=search-box><input name=q id=search-query placeholder=Search... autocapitalize=off autocomplete=off autocorrect=off spellcheck=false type=search class=form-control aria-label=Search...></div></section><section class=section-search-results><div id=search-hits></div></section></div></aside><div class="page-header header--fixed"><header><nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id=navbar-main><div class=container-xl><div class="d-none d-lg-inline-flex"><a class=navbar-brand href=../><img src=../media/logo_hue0215fe06ef33ffe1f8918f47f6e164c_38928_0x70_resize_lanczos_3.png alt="APEX, HKUST(GZ) & HKUST"></a></div><button type=button class=navbar-toggler data-toggle=collapse data-target=#navbar-content aria-controls=navbar-content aria-expanded=false aria-label="Toggle navigation">
<span><i class="fas fa-bars"></i></span></button><div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none"><a class=navbar-brand href=../><img src=../media/logo_hue0215fe06ef33ffe1f8918f47f6e164c_38928_0x70_resize_lanczos_3.png alt="APEX, HKUST(GZ) & HKUST"></a></div><div class="navbar-collapse main-menu-item collapse justify-content-end" id=navbar-content><ul class="navbar-nav d-md-inline-flex"><li class=nav-item><a class=nav-link href=../><span>Home</span></a></li><li class=nav-item><a class=nav-link href=../people><span>People</span></a></li><li class=nav-item><a class=nav-link href=../publication><span>Publications</span></a></li></ul></div><ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2"><li class=nav-item><a class="nav-link js-search" href=# aria-label=Search><i class="fas fa-search" aria-hidden=true></i></a></li><li class="nav-item dropdown i18n-dropdown"><a href=# class="nav-link dropdown-toggle" data-toggle=dropdown aria-haspopup=true aria-label=Languages><i class="fas fa-globe mr-1" aria-hidden=true></i><span class="d-none d-lg-inline">English</span></a><div class=dropdown-menu><div class="dropdown-item dropdown-item-active"><span>English</span></div><a class=dropdown-item href=../zh/publication/><span>中文 (简体)</span></a></div></li></ul></div></nav></header></div><div class=page-body><div class="universal-wrapper pt-3"><h1>Publications</h1></div><div class=universal-wrapper><div class=row><div class=col-lg-12><div class="form-row mb-4"><div class=col-auto><input type=search class="filter-search form-control form-control-sm" placeholder=Search... autocapitalize=off autocomplete=off autocorrect=off role=textbox spellcheck=false></div><div class=col-auto><select class="pub-filters pubtype-select form-control form-control-sm" data-filter-group=pubtype><option value=*>All Topics</option><option value=.pubtype-0>Uncategorized</option><option value=.pubtype-1>Accessibility & Aging</option><option value=.pubtype-2>VR/AR/Metaverse</option><option value=.pubtype-3>Human-AI Collaboration</option><option value=.pubtype-4>UX Methodology</option><option value=.pubtype-5>Social Computing</option><option value=.pubtype-6>Sensing</option></select></div><div class=col-auto><select class="pub-filters form-control form-control-sm" data-filter-group=year><option value=*>Date</option><option value=.year-2025>2025</option><option value=.year-2024>2024</option><option value=.year-2023>2023</option><option value=.year-2022>2022</option><option value=.year-2021>2021</option><option value=.year-2020>2020</option><option value=.year-2019>2019</option><option value=.year-2018>2018</option><option value=.year-2017>2017</option><option value=.year-2015>2015</option><option value=.year-2014>2014</option><option value=.year-2012>2012</option></select></div></div><div id=container-publications><div class="grid-sizer col-lg-12 isotope-item pubtype-3 year-2025"><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=../publication/chi25-ai-story-writing/>Toward Personalizable AI Node Graph Creative Writing Support: Insights on Preferences for Generative AI Features and Information Presentation Across Story Writing Processes</a></div><div class="stream-meta article-metadata"><div class=article-metadata><div><span><a href=../author/hua-xuan-qin/>Hua Xuan Qin</a></span>, <span><a href=../author/guangzhi-zhu/>Guangzhi Zhu</a></span>, <span><a href=../author/mingming-fan/>Mingming Fan</a></span>, <span><a href=../author/pan-hui/>Pan Hui</a></span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="corresponding author"></i></div><span class=article-date>February 2025
</span><span class=middot-divider></span>
<span class=pub-publication>CHI 2025</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://www.mingmingfan.com/papers/CHI25-Writing.pdf target=_blank rel=noopener>PDF</a></div></div><div class=ml-3><a href=../publication/chi25-ai-story-writing/><img src=../publication/chi25-ai-story-writing/featured_hub6addb2505a978145aceacbb2b49ed5d_451176_150x0_resize_q75_h2_lanczos_3.webp height=123 width=150 alt="Toward Personalizable AI Node Graph Creative Writing Support: Insights on Preferences for Generative AI Features and Information Presentation Across Story Writing Processes" loading=lazy></a></div></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2025"><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=../publication/chi25-nature-conversation/>Toward Enabling Natural Conversation with Older Adults via the Design of LLM-Powered Voice Agents that Support Interruptions and Backchannels</a></div><div class="stream-meta article-metadata"><div class=article-metadata><div><span><a href=../author/chao-liu/>Chao Liu</a></span>, <span><a href=../author/mingyang-su/>Mingyang Su</a></span>, <span><a href=../author/yan-xiang/>Yan Xiang</a></span>, <span><a href=../author/yuru-huang/>Yuru Huang</a></span>, <span><a href=../author/yiqian-yang/>Yiqian Yang</a></span>, <span><a href=../author/kang-zhang/>Kang Zhang</a></span>, <span><a href=../author/mingming-fan/>Mingming Fan</a></span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="corresponding author"></i></div><span class=article-date>February 2025
</span><span class=middot-divider></span>
<span class=pub-publication>CHI 2025</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://dl.acm.org/doi/full/10.1145/3706598.3714228 target=_blank rel=noopener>PDF</a></div></div><div class=ml-3><a href=../publication/chi25-nature-conversation/><img src=../publication/chi25-nature-conversation/featured_hu415a5d359b417b1a2b2d7327fdde8e65_244684_150x0_resize_q75_h2_lanczos_3.webp height=46 width=150 alt="Toward Enabling Natural Conversation with Older Adults via the Design of LLM-Powered Voice Agents that Support Interruptions and Backchannels" loading=lazy></a></div></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2025"><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=../publication/chi25-remotechess/>RemoteChess: Enhancing Older Adults’ Social Connectedness via Designing a Virtual Reality Chinese Chess (Xiangqi) Community</a></div><div class="stream-meta article-metadata"><div class=article-metadata><div><span><a href=../author/qianjie-wei/>Qianjie Wei</a></span>, <span><a href=../author/xiaoying-wei/>Xiaoying Wei</a></span>, <span><a href=../author/yiqi-liang/>Yiqi Liang</a></span>, <span><a href=../author/fan-lin/>Fan Lin</a></span>, <span><a href=../author/nuonan-si/>Nuonan Si</a></span>, <span><a href=../author/mingming-fan/>Mingming Fan</a></span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="corresponding author"></i></div><span class=article-date>February 2025
</span><span class=middot-divider></span>
<span class=pub-publication>CHI 2025</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/abs/2502.11627 target=_blank rel=noopener>PDF</a></div></div><div class=ml-3><a href=../publication/chi25-remotechess/><img src=../publication/chi25-remotechess/featured_hu2eb9836bc6ccfbe21b3fa96c072f76eb_149616_150x0_resize_q75_h2_lanczos_3.webp height=39 width=150 alt="RemoteChess: Enhancing Older Adults’ Social Connectedness via Designing a Virtual Reality Chinese Chess (Xiangqi) Community" loading=lazy></a></div></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2025"><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=../publication/chi25-journalalde/>JournalAIde: Empowering Older Adults in Digital Journal Writing</a></div><div class="stream-meta article-metadata"><div class=article-metadata><div><span><a href=../author/shixu-zhou/>Shixu Zhou</a></span>, <span><a href=../author/weiyue-lin/>Weiyue Lin</a></span>, <span><a href=../author/zuyu-xu/>Zuyu Xu</a></span>, <span><a href=../author/xiaoying-wei/>Xiaoying Wei</a></span>, <span><a href=../author/raoyi-huang/>Raoyi Huang</a></span>, <span><a href=../author/xiaojuan-ma/>Xiaojuan Ma</a></span>, <span><a href=../author/mingming-fan/>Mingming Fan</a></span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="corresponding author"></i></div><span class=article-date>February 2025
</span><span class=middot-divider></span>
<span class=pub-publication>CHI 2025</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://dl.acm.org/doi/10.1145/3706598.3713339 target=_blank rel=noopener>PDF</a></div></div><div class=ml-3><a href=../publication/chi25-journalalde/><img src=../publication/chi25-journalalde/featured_hu487ff789adde4aaf7507b29bc9a993df_568033_150x0_resize_q75_h2_lanczos_3.webp height=63 width=150 alt="JournalAIde: Empowering Older Adults in Digital Journal Writing" loading=lazy></a></div></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-2 year-2025"><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=../publication/chi25-interecon/>InteRecon: Towards Reconstructing Interactivity of Personal Memorable Items in Mixed Reality</a></div><div class="stream-meta article-metadata"><div class=article-metadata><div><span><a href=../author/zisu-li/>Zisu Li</a></span>, <span><a href=../author/jiawei-li/>Jiawei Li</a></span>, <span><a href=../author/zeyu-xiong/>Zeyu Xiong</a></span>, <span><a href=../author/shumeng-zhang/>Shumeng Zhang</a></span>, <span><a href=../author/faraz-faruqi/>Faraz Faruqi</a></span>, <span><a href=../author/stefanie-mueller/>Stefanie Mueller</a></span>, <span><a href=../author/chen-liang/>Chen Liang</a></span>, <span><a href=../author/xiaojuan-ma/>Xiaojuan Ma</a></span>, <span><a href=../author/mingming-fan/>Mingming Fan</a></span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="corresponding author"></i></div><span class=article-date>February 2025
</span><span class=middot-divider></span>
<span class=pub-publication>CHI 2025</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/abs/2502.09973 target=_blank rel=noopener>PDF</a></div></div><div class=ml-3><a href=../publication/chi25-interecon/><img src=../publication/chi25-interecon/featured_hue15a0749bc383756d696b1191b69ea43_399053_150x0_resize_q75_h2_lanczos_3.webp height=31 width=150 alt="InteRecon: Towards Reconstructing Interactivity of Personal Memorable Items in Mixed Reality" loading=lazy></a></div></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-2 year-2025"><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=../publication/chi25-flower-arrangement/>Facilitating Daily Practice in Intangible Cultural Heritage through Virtual Reality: A Case Study of Traditional Chinese Flower Arrangement</a></div><div class="stream-meta article-metadata"><div class=article-metadata><div><span><a href=../author/yingna-wang/>Yingna Wang</a></span>, <span><a href=../author/qingqin-liu/>Qingqin Liu</a></span>, <span><a href=../author/xiaoying-wei/>Xiaoying Wei</a></span>, <span><a href=../author/mingming-fan/>Mingming Fan</a></span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="corresponding author"></i></div><span class=article-date>February 2025
</span><span class=middot-divider></span>
<span class=pub-publication>CHI 2025</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/abs/2503.06122 target=_blank rel=noopener>PDF</a></div></div><div class=ml-3><a href=../publication/chi25-flower-arrangement/><img src=../publication/chi25-flower-arrangement/featured_hu26ec7528e83ed42e255b247ad8bc73f5_617940_150x0_resize_q75_h2_lanczos_3.webp height=59 width=150 alt="Facilitating Daily Practice in Intangible Cultural Heritage through Virtual Reality: A Case Study of Traditional Chinese Flower Arrangement" loading=lazy></a></div></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-6 year-2025"><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=../publication/chi25-massage-instruction/>Designing LLM-Powered Multimodal Instructions to Support Rich Hands-on Skills Remote Learning: A Case Study with Massage Instructors and Learners</a></div><div class="stream-meta article-metadata"><div class=article-metadata><div><span><a href=../author/chutian-jiang/>Chutian Jiang</a></span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="equal contribution"></i>, <span><a href=../author/yinan-fan/>Yinan Fan</a></span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="equal contribution"></i>, <span><a href=../author/junan-xie/>Junan Xie</a></span>, <span><a href=../author/emily-kuang/>Emily Kuang</a></span>, <span><a href=../author/baichuan-feng/>Baichuan Feng</a></span>, <span><a href=../author/kaihao-zhang/>Kaihao Zhang</a></span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="corresponding author"></i>, <span><a href=../author/mingming-fan/>Mingming Fan</a></span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="corresponding author"></i></div><span class=article-date>February 2025
</span><span class=middot-divider></span>
<span class=pub-publication>CHI 2025</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://dl.acm.org/doi/full/10.1145/3706598.3713677 target=_blank rel=noopener>PDF</a></div></div><div class=ml-3><a href=../publication/chi25-massage-instruction/><img src=../publication/chi25-massage-instruction/featured_hu255141a1da8e0cb922ab162250ac90b8_62049_150x0_resize_q75_h2_lanczos_3.webp height=46 width=150 alt="Designing LLM-Powered Multimodal Instructions to Support Rich Hands-on Skills Remote Learning: A Case Study with Massage Instructors and Learners" loading=lazy></a></div></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-3 year-2025"><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=../publication/chi25-multi-agent-reminiscene/>Chorus of the Past: Toward Designing a Multi-agent Conversational Reminiscence System with Digital Artifacts for Older Adults</a></div><div class="stream-meta article-metadata"><div class=article-metadata><div><span><a href=../author/jingwei-sun/>Jingwei Sun</a></span>, <span><a href=../author/zhongyue-zhang/>Zhongyue Zhang</a></span>, <span><a href=../author/mengyang-wang/>Mengyang Wang</a></span>, <span><a href=../author/nianlong-li/>Nianlong Li</a></span>, <span><a href=../author/yan-xiang/>Yan Xiang</a></span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="corresponding author"></i>, <span><a href=../author/liuxin-zhang/>Liuxin Zhang</a></span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="corresponding author"></i>, <span><a href=../author/yu-zhang/>Yu Zhang</a></span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="corresponding author"></i>, <span><a href=../author/qianying-wang/>Qianying Wang</a></span>, <span><a href=../author/mingming-fan/>Mingming Fan</a></span></div><span class=article-date>February 2025
</span><span class=middot-divider></span>
<span class=pub-publication>CHI 2025</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://dl.acm.org/doi/10.1145/3706598.3713810 target=_blank rel=noopener>PDF</a></div></div><div class=ml-3><a href=../publication/chi25-multi-agent-reminiscene/><img src=../publication/chi25-multi-agent-reminiscene/featured_hu912637da4456d2d58d41a36281c4f4df_103414_150x0_resize_q75_h2_lanczos_3.webp height=60 width=150 alt="Chorus of the Past: Toward Designing a Multi-agent Conversational Reminiscence System with Digital Artifacts for Older Adults" loading=lazy></a></div></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-3 year-2025"><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=../publication/chi25-acknowledge/>ACKnowledge: A Computational Framework for Human Compatible Affordance-based Interaction Planning in Real-world Contexts</a></div><div class="stream-meta article-metadata"><div class=article-metadata><div><span><a href=../author/ziqi-pan/>Ziqi Pan</a></span>, <span><a href=../author/xiucheng-zhang/>Xiucheng Zhang</a></span>, <span><a href=../author/zisu-li/>Zisu Li</a></span>, <span><a href=../author/zhenhui-peng/>Zhenhui Peng</a></span>, <span><a href=../author/mingming-fan/>Mingming Fan</a></span>, <span><a href=../author/xiaojuan-ma/>Xiaojuan Ma</a></span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="corresponding author"></i></div><span class=article-date>February 2025
</span><span class=middot-divider></span>
<span class=pub-publication>CHI 2025</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://dl.acm.org/doi/10.1145/3706598.3713791 target=_blank rel=noopener>PDF</a></div></div><div class=ml-3><a href=../publication/chi25-acknowledge/><img src=../publication/chi25-acknowledge/featured_hu13cf0121d5fcea2946e09b0b0e74b601_470632_150x0_resize_q75_h2_lanczos_3.webp height=94 width=150 alt="ACKnowledge: A Computational Framework for Human Compatible Affordance-based Interaction Planning in Real-world Contexts" loading=lazy></a></div></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2025"><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=../publication/chi25-older-practioner-assessment/>"Watch, Smell, Ask, Touch": Practices, Challenges, and Technological Support in Ability Assessment of Older Adults from Practitioners Perspectives in China</a></div><div class="stream-meta article-metadata"><div class=article-metadata><div><span><a href=../author/zhongyue-zhang/>Zhongyue Zhang</a></span>, <span><a href=../author/yuru-huang/>Yuru Huang</a></span>, <span><a href=../author/mengyang-wang/>Mengyang Wang</a></span>, <span><a href=../author/mingming-fan/>Mingming Fan</a></span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="corresponding author"></i></div><span class=article-date>February 2025
</span><span class=middot-divider></span>
<span class=pub-publication>CHI 2025</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://dl.acm.org/doi/10.1145/3706598.3714166 target=_blank rel=noopener>PDF</a></div></div><div class=ml-3><a href=../publication/chi25-older-practioner-assessment/><img src=../publication/chi25-older-practioner-assessment/featured_hu5ed16c30aa4ff0f852e4ab994890a9fd_38700_150x0_resize_q75_h2_lanczos_3.webp height=77 width=150 alt='	"Watch, Smell, Ask, Touch": Practices, Challenges, and Technological Support in Ability Assessment of Older Adults from Practitioners Perspectives in China' loading=lazy></a></div></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-2 year-2025"><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=../publication/tvcg-2025-teamportal/>TeamPortal: Exploring Virtual Reality Collaboration Through Shared and Manipulating Parallel Views</a></div><div class="stream-meta article-metadata"><div class=article-metadata><div><span><a href=../author/xian-wang/>Xian Wang</a></span>, <span><a href=../author/luyao-shen/>Luyao Shen</a></span>, <span><a href=../author/lei-chen/>Lei Chen</a></span>, <span><a href=../author/mingming-fan/>Mingming Fan</a></span>, <span><a href=../author/lik-hang-lee/>Lik-Hang Lee</a></span></div><span class=article-date>January 2025
</span><span class=middot-divider></span>
<span class=pub-publication>TVCG 2025</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/abs/2501.17768 target=_blank rel=noopener>PDF</a></div></div><div class=ml-3><a href=../publication/tvcg-2025-teamportal/><img src=../publication/tvcg-2025-teamportal/featured_hu18b2b57ba2fe3be2bd1f3f08619b4317_371462_150x0_resize_q75_h2_lanczos_3.webp height=64 width=150 alt="TeamPortal: Exploring Virtual Reality Collaboration Through Shared and Manipulating Parallel Views" loading=lazy></a></div></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-2 year-2025"><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=../publication/tvcg-2025-focalselect/>FocalSelect: Improving Occluded Objects Acquisition with Heuristic Selection and Disambiguation in Virtual Reality</a></div><div class="stream-meta article-metadata"><div class=article-metadata><div><span><a href=../author/duotun-wang/>Duotun Wang</a></span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="equal contribution"></i>, <span><a href=../author/linjie-qiu/>Linjie Qiu</a></span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="equal contribution"></i>, <span><a href=../author/boyu-li/>Boyu Li</a></span>, <span><a href=../author/qianxi-liu/>Qianxi Liu</a></span>, <span><a href=../author/xiaoying-wei/>Xiaoying Wei</a></span>, <span><a href=../author/jianhao-chen/>Jianhao Chen</a></span>, <span><a href=../author/zeyu-wang/>Zeyu Wang</a></span>, <span><a href=../author/mingming-fan/>Mingming Fan</a></span></div><span class=article-date>January 2025
</span><span class=middot-divider></span>
<span class=pub-publication>TVCG 2025</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://www.mingmingfan.com/papers/TVCG2025__FocalSelect.pdf target=_blank rel=noopener>PDF</a></div></div><div class=ml-3><a href=../publication/tvcg-2025-focalselect/><img src=../publication/tvcg-2025-focalselect/featured_hucb2583ad336e9dff7192e3f4eb78d0e5_8571966_150x0_resize_q75_h2_lanczos_3.webp height=96 width=150 alt="FocalSelect: Improving Occluded Objects Acquisition with Heuristic Selection and Disambiguation in Virtual Reality" loading=lazy></a></div></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-3 year-2025"><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=../publication/ijhcs-contractmind/>ContractMind: Trust-calibration interaction design for AI contract review tools</a></div><div class="stream-meta article-metadata"><div class=article-metadata><div><span><a href=../author/jian-zeng/>Jian Zeng</a></span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="equal contribution"></i>, <span><a href=../author/kaixin-chen/>Kaixin Chen</a></span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="equal contribution"></i>, <span><a href=../author/ruiqi-wang/>Ruiqi Wang</a></span>, <span><a href=../author/yilong-li/>Yilong Li</a></span>, <span><a href=../author/mingming-fan/>Mingming Fan</a></span>, <span><a href=../author/kaishun-wu/>Kaishun Wu</a></span>, <span><a href=../author/xiaoke-qi/>Xiaoke Qi</a></span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Corresponding author"></i>, <span><a href=../author/lu-wang/>Lu Wang</a></span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Corresponding author"></i></div><span class=article-date>January 2025
</span><span class=middot-divider></span>
<span class=pub-publication>IJHCS</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://linkinghub.elsevier.com/retrieve/pii/S1071581924001940 target=_blank rel=noopener>PDF</a></div></div><div class=ml-3><a href=../publication/ijhcs-contractmind/><img src=../publication/ijhcs-contractmind/featured_hu3c83cad97e7d96fc7b1f16292ee42e4f_260331_150x0_resize_q75_h2_lanczos_3.webp height=84 width=150 alt="ContractMind: Trust-calibration interaction design for AI contract review tools" loading=lazy></a></div></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2025"><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=../publication/cscw25-photo-reminiscene/>Understanding and Co-designing Photo-based Reminiscence with Older Adults</a></div><div class="stream-meta article-metadata"><div class=article-metadata><div><span><a href=../author/zhongyue-zhang/>Zhongyue Zhang</a></span>, <span><a href=../author/lina-xu/>Lina Xu</a></span>, <span><a href=../author/xingkai-wang/>Xingkai Wang</a></span>, <span><a href=../author/xu-zhang/>Xu Zhang</a></span>, <span><a href=../author/mingming-fan/>Mingming Fan</a></span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="corresponding author"></i></div><span class=article-date>January 2025
</span><span class=middot-divider></span>
<span class=pub-publication>CSCW 2025</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/abs/2411.00351 target=_blank rel=noopener>PDF</a></div></div><div class=ml-3><a href=../publication/cscw25-photo-reminiscene/><img src=../publication/cscw25-photo-reminiscene/featured_hua281b1c8831e7ecda14ba88bf9ba0f1b_3040778_150x0_resize_q75_h2_lanczos_3.webp height=113 width=150 alt="Understanding and Co-designing Photo-based Reminiscence with Older Adults" loading=lazy></a></div></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-2 year-2025"><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=../publication/cscw25-social-vr-review/>Systematic Literature Review of Using Virtual Reality as a Social Platform in HCI Community</a></div><div class="stream-meta article-metadata"><div class=article-metadata><div><span><a href=../author/xiaoying-wei/>Xiaoying Wei</a></span>, <span><a href=../author/xiaofu-jin/>Xiaofu Jin</a></span>, <span><a href=../author/ge-lin-kan/>Ge Lin Kan</a></span>, <span><a href=../author/yukang-yan/>Yukang Yan</a></span>, <span><a href=../author/mingming-fan/>Mingming Fan</a></span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="corresponding author"></i></div><span class=article-date>January 2025
</span><span class=middot-divider></span>
<span class=pub-publication>CSCW 2025</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/abs/2410.11869 target=_blank rel=noopener>PDF</a></div></div><div class=ml-3><a href=../publication/cscw25-social-vr-review/><img src=../publication/cscw25-social-vr-review/featured_hudbb6d5986a9881859ee67cfa0d0be8c3_177341_150x0_resize_q75_h2_lanczos_3.webp height=48 width=150 alt="Systematic Literature Review of Using Virtual Reality as a Social Platform in HCI Community" loading=lazy></a></div></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2025"><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=../publication/cscw25-dhh-dating/>Practices and Challenges of Online Love-seeking Among Deaf or Hard of Hearing People: A Case Study in China</a></div><div class="stream-meta article-metadata"><div class=article-metadata><div><span><a href=../author/beiyan-cao/>Beiyan Cao</a></span>, <span><a href=../author/jingling-zhang/>Jingling Zhang</a></span>, <span><a href=../author/changyang-he/>Changyang He</a></span>, <span><a href=../author/yuru-huang/>Yuru Huang</a></span>, <span><a href=../author/muzhi-zhou/>Muzhi Zhou</a></span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="corresponding author"></i>, <span><a href=../author/mingming-fan/>Mingming Fan</a></span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="corresponding author"></i></div><span class=article-date>January 2025
</span><span class=middot-divider></span>
<span class=pub-publication>CSCW 2025</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/pdf/2410.11810 target=_blank rel=noopener>PDF</a></div></div><div class=ml-3><a href=../publication/cscw25-dhh-dating/><img src=../publication/cscw25-dhh-dating/featured_hu3814a636fd5a4a1322f09dfd560d00ef_917571_150x0_resize_q75_h2_lanczos_3.webp height=107 width=150 alt="Practices and Challenges of Online Love-seeking Among Deaf or Hard of Hearing People: A Case Study in China" loading=lazy></a></div></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2025"><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=../publication/cscw25-companion-robot/>Challenges in Adopting Companion Robots: An Exploratory Study of Robotic Companionship Conducted with Chinese Retirees</a></div><div class="stream-meta article-metadata"><div class=article-metadata><div><span><a href=../author/mengyang-wang/>Mengyang Wang</a></span>, <span><a href=../author/keye-yu/>Keye Yu</a></span>, <span><a href=../author/yukai-zhang/>Yukai Zhang</a></span>, <span><a href=../author/mingming-fan/>Mingming Fan</a></span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="corresponding author"></i></div><span class=article-date>January 2025
</span><span class=middot-divider></span>
<span class=pub-publication>CSCW 2025</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/abs/2410.12205 target=_blank rel=noopener>PDF</a></div></div><div class=ml-3><a href=../publication/cscw25-companion-robot/><img src=../publication/cscw25-companion-robot/featured_hu17d647f4f7ce14cc9049d40092d26eb6_2807329_150x0_resize_q75_h2_lanczos_3.webp height=68 width=150 alt="Challenges in Adopting Companion Robots: An Exploratory Study of Robotic Companionship Conducted with Chinese Retirees" loading=lazy></a></div></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2025"><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=../publication/cscw25-ai-support-igc/>AI as a Bridge Across Ages: Exploring The Opportunities of Artificial Intelligence in Supporting Inter-Generational Communication in Virtual Reality</a></div><div class="stream-meta article-metadata"><div class=article-metadata><div><span><a href=../author/qiuxin-du/>Qiuxin Du</a></span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="equal contribution"></i>, <span><a href=../author/xiaoying-wei/>Xiaoying Wei</a></span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="equal contribution"></i>, <span><a href=../author/jiawei-li/>Jiawei Li</a></span>, <span><a href=../author/emily-kuang/>Emily Kuang</a></span>, <span><a href=../author/jie-hao/>Jie Hao</a></span>, <span><a href=../author/dongdong-weng/>Dongdong Weng</a></span>, <span><a href=../author/mingming-fan/>Mingming Fan</a></span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="corresponding author"></i></div><span class=article-date>January 2025
</span><span class=middot-divider></span>
<span class=pub-publication>CSCW 2025</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/abs/2410.17909 target=_blank rel=noopener>PDF</a></div></div><div class=ml-3><a href=../publication/cscw25-ai-support-igc/><img src=../publication/cscw25-ai-support-igc/featured_huc8e6da1d9191aac5a501827c368037ac_5877186_150x0_resize_q75_h2_lanczos_3.webp height=73 width=150 alt="AI as a Bridge Across Ages: Exploring The Opportunities of Artificial Intelligence in Supporting Inter-Generational Communication in Virtual Reality" loading=lazy></a></div></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-2 year-2024"><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=../publication/vrst24-searchvr/>Toward Facilitating Search in VR With the Assistance of Vision Large Language Models</a></div><div class="stream-meta article-metadata"><div class=article-metadata><div><span><a href=../author/chao-liu/>Chao Liu</a></span>, <span><a href=../author/clarence-chi-san-cheung/>Clarence Chi San Cheung</a></span>, <span><a href=../author/mingqing-xu/>Mingqing Xu</a></span>, <span><a href=../author/zhongyue-zhang/>Zhongyue Zhang</a></span>, <span><a href=../author/mingyang-su/>Mingyang Su</a></span>, <span><a href=../author/mingming-fan/>Mingming Fan</a></span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Corresponding author"></i></div><span class=article-date>October 2024
</span><span class=middot-divider></span>
<span class=pub-publication>VRST 2024</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://www.mingmingfan.com/papers/VRST24_VR_Search_Framework.pdf target=_blank rel=noopener>PDF</a></div></div><div class=ml-3><a href=../publication/vrst24-searchvr/><img src=../publication/vrst24-searchvr/featured_hued1f0784c4d70f09a8b0515d1adb6bbd_731792_150x0_resize_q75_h2_lanczos_3.webp height=45 width=150 alt="Toward Facilitating Search in VR With the Assistance of Vision Large Language Models" loading=lazy></a></div></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-2 year-2024"><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=../publication/vinci24-hapticperception/>Investigating Size Congruency Between the Visual Perception of a VR Object and the Haptic Perception of Its Physical World Agent</a></div><div class="stream-meta article-metadata"><div class=article-metadata><div><span><a href=../author/wenqi-zheng/>Wenqi Zheng</a></span>, <span><a href=../author/dawei-xiong/>Dawei Xiong</a></span>, <span><a href=../author/cekai-weng/>Cekai Weng</a></span>, <span><a href=../author/jiajun-jiang/>Jiajun Jiang</a></span>, <span><a href=../author/junwei-li/>Junwei Li</a></span>, <span><a href=../author/jinni-zhou/>Jinni ZHOU</a></span>, <span><a href=../author/mingming-fan/>Mingming Fan</a></span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Corresponding author"></i></div><span class=article-date>October 2024
</span><span class=middot-divider></span>
<span class=pub-publication>VINCI 2024</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://www.mingmingfan.com/papers/VINCI24_VR_Size_Congruency.pdf target=_blank rel=noopener>PDF</a></div></div><div class=ml-3><a href=../publication/vinci24-hapticperception/><img src=../publication/vinci24-hapticperception/featured_hu8922a23f94c5489d3b323499bc50bc7a_343069_150x0_resize_q75_h2_lanczos_3.webp height=63 width=150 alt="Investigating Size Congruency Between the Visual Perception of a VR Object and the Haptic Perception of Its Physical World Agent" loading=lazy></a></div></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-2 year-2024"><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=../publication/cscw24-harassers-vr/>Avatar Appearance and Behavior of Potential Harassers Affect Users' Perceptions and Response Strategies in Social Virtual Reality (VR): A Mixed-Methods Study</a></div><div class="stream-meta article-metadata"><div class=article-metadata><div><span><a href=../author/xuetong-wang/>Xuetong Wang</a></span>, <span><a href=../author/ziyan-wang/>Ziyan Wang</a></span>, <span><a href=../author/mingmin-zhang/>Mingmin Zhang</a></span>, <span><a href=../author/kangyou-yu/>Kangyou Yu</a></span>, <span><a href=../author/pan-hui/>Pan Hui</a></span>, <span><a href=../author/mingming-fan/>Mingming Fan</a></span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="corresponding author"></i></div><span class=article-date>October 2024
</span><span class=middot-divider></span>
<span class=pub-publication>CSCW 2024</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://www.mingmingfan.com/papers/CSCW23_VR_Harassment.pdf target=_blank rel=noopener>PDF</a></div></div><div class=ml-3><a href=../publication/cscw24-harassers-vr/><img src=../publication/cscw24-harassers-vr/featured_hu64187ace12e4cb66ed5f2574ab9b2563_443632_150x0_resize_q75_h2_lanczos.webp height=36 width=150 alt="Avatar Appearance and Behavior of Potential Harassers Affect Users' Perceptions and Response Strategies in Social Virtual Reality (VR): A Mixed-Methods Study" loading=lazy></a></div></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-2 year-2024"><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=../publication/vinci24-augmentedlibrary/>Augmented Library: Toward Enriching Physical Library Experience Using HMD-Based Augmented Reality</a></div><div class="stream-meta article-metadata"><div class=article-metadata><div><span><a href=../author/qianjie-wei/>Qianjie Wei</a></span>, <span><a href=../author/jingling-zhang/>Jingling Zhang</a></span>, <span><a href=../author/pengqi-wang/>Pengqi Wang</a></span>, <span><a href=../author/xiaofu-jin/>Xiaofu Jin</a></span>, <span><a href=../author/mingming-fan/>Mingming Fan</a></span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Corresponding author"></i></div><span class=article-date>October 2024
</span><span class=middot-divider></span>
<span class=pub-publication>VINCI 2024</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://www.mingmingfan.com/papers/VINCI24_AR_Library.pdf target=_blank rel=noopener>PDF</a></div></div><div class=ml-3><a href=../publication/vinci24-augmentedlibrary/><img src=../publication/vinci24-augmentedlibrary/featured_hub4d4f9a8cd5861387cb54724d5924f9a_21946608_150x0_resize_q75_h2_lanczos_3.webp height=67 width=150 alt="Augmented Library: Toward Enriching Physical Library Experience Using HMD-Based Augmented Reality" loading=lazy></a></div></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-2 year-2024"><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=../publication/imwut2024-silvercycling/>SilverCycling: Exploring the Impact of Bike-Based Locomotion on Spatial Orientation for Older Adults in VR</a></div><div class="stream-meta article-metadata"><div class=article-metadata><div><span><a href=../author/qiongyan-chen/>Qiongyan Chen</a></span>, <span><a href=../author/zhiqing-wu/>Zhiqing Wu</a></span>, <span><a href=../author/yucheng-liu/>Yucheng Liu</a></span>, <span><a href=../author/lei-han/>Lei Han</a></span>, <span><a href=../author/zisu-li/>Zisu Li</a></span>, <span><a href=../author/ge-lin-kan/>Ge Lin Kan</a></span>, <span><a href=../author/mingming-fan/>Mingming Fan</a></span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Corresponding author"></i></div><span class=article-date>August 2024
</span><span class=middot-divider></span>
<span class=pub-publication>IMWUT 2024</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://www.mingmingfan.com/papers/IMWUT24-SilverCycling.pdf target=_blank rel=noopener>PDF</a></div></div><div class=ml-3><a href=../publication/imwut2024-silvercycling/><img src=../publication/imwut2024-silvercycling/featured_hu39d4ac45cd0ff5474a1b54b503611ade_2510115_150x0_resize_q75_h2_lanczos_3.webp height=82 width=150 alt="SilverCycling: Exploring the Impact of Bike-Based Locomotion on Spatial Orientation for Older Adults in VR" loading=lazy></a></div></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2024"><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=../publication/assets24-beadwork/>Beadwork Bridge: Understanding and Exploring the Opportunities of Beadwork in Enriching School Education for Blind and Low Vision (BLV) People</a></div><div class="stream-meta article-metadata"><div class=article-metadata><div><span><a href=../author/shumeng-zhang/>Shumeng Zhang</a></span>, <span><a href=../author/weiyue-lin/>Weiyue Lin</a></span>, <span><a href=../author/zisu-li/>Zisu Li</a></span>, <span><a href=../author/ruiqi-jiang/>Ruiqi Jiang</a></span>, <span><a href=../author/chen-liang/>Chen Liang</a></span>, <span><a href=../author/mingming-fan/>Mingming Fan</a></span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Corresponding author"></i>, <span><a href=../author/raul-masu/>Raul Masu</a></span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Corresponding author"></i></div><span class=article-date>July 2024
</span><span class=middot-divider></span>
<span class=pub-publication>ASSETS 2024</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://www.mingmingfan.com/papers/assets24-beadwork.pdf target=_blank rel=noopener>PDF</a></div></div><div class=ml-3><a href=../publication/assets24-beadwork/><img src=../publication/assets24-beadwork/featured_hu56ac9b6b87e6b31ad07bd0505b202863_430306_150x0_resize_q75_h2_lanczos_3.webp height=82 width=150 alt="Beadwork Bridge: Understanding and Exploring the Opportunities of Beadwork in Enriching School Education for Blind and Low Vision (BLV) People" loading=lazy></a></div></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-3 year-2024"><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=../publication/dis24-aigc-ux/>Exploring the Impact of Artificial Intelligence-Generated Content (AIGC) Tools on Social Dynamics in UX Collaboration</a></div><div class="stream-meta article-metadata"><div class=article-metadata><div><span><a href=../author/ziyan-wang/>Ziyan Wang</a></span>, <span><a href=../author/luyao-shen/>Luyao Shen</a></span>, <span><a href=../author/emily-kuang/>Emily Kuang</a></span>, <span><a href=../author/shumeng-zhang/>Shumeng Zhang</a></span>, <span><a href=../author/mingming-fan/>Mingming Fan</a></span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Corresponding author"></i></div><span class=article-date>May 2024
</span><span class=middot-divider></span>
<span class=pub-publication>DIS 2024</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://www.mingmingfan.com/papers/DIS24_AIGC_Social_Dynamics_UX.pdf target=_blank rel=noopener>PDF</a></div></div><div class=ml-3><a href=../publication/dis24-aigc-ux/><img src=../publication/dis24-aigc-ux/featured_hu4a915a46500f8397db873e0f53f5f0a3_100772_150x0_resize_q75_h2_lanczos.webp height=35 width=150 alt="Exploring the Impact of Artificial Intelligence-Generated Content (AIGC) Tools on Social Dynamics in UX Collaboration" loading=lazy></a></div></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-3 year-2024"><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=../publication/splattingavatar/>SplattingAvatar: Realistic Real-Time Human Avatars with Mesh-Embedded Gaussian Splatting</a></div><div class="stream-meta article-metadata"><div class=article-metadata><div><span><a href=../author/zhijing-shao/>Zhijing Shao</a></span>, <span><a href=../author/zhaolong-wang/>Zhaolong Wang</a></span>, <span><a href=../author/zhuang-li/>Zhuang Li</a></span>, <span><a href=../author/duotun-wang/>Duotun Wang</a></span>, <span><a href=../author/xiangru-li/>Xiangru Li</a></span>, <span><a href=../author/yu-zhang/>Yu Zhang</a></span>, <span><a href=../author/mingming-fan/>Mingming Fan</a></span>, <span><a href=../author/zeyu-wang/>Zeyu Wang</a></span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="corresponding author"></i></div><span class=article-date>March 2024
</span><span class=middot-divider></span>
<span class=pub-publication>CVPR 2024</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/abs/2403.05087 target=_blank rel=noopener>PDF</a></div></div><div class=ml-3><a href=../publication/splattingavatar/><img src=../publication/splattingavatar/featured_hu74ddd7a2e9308bbedbfc0bb9ccb6ce64_54041_150x0_resize_q75_h2_lanczos.webp height=170 width=150 alt="SplattingAvatar: Realistic Real-Time Human Avatars with Mesh-Embedded Gaussian Splatting" loading=lazy></a></div></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-2 year-2024"><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=../publication/chi24-older-vr/>Toward Making Virtual Reality (VR) More Inclusive for Older Adults: Investigating Aging Effects on Object Selection and Manipulation in VR</a></div><div class="stream-meta article-metadata"><div class=article-metadata><div><span><a href=../author/zhiqing-wu/>Zhiqing Wu</a></span>, <span><a href=../author/duotun-wang/>Duotun Wang</a></span>, <span><a href=../author/shumeng-zhang/>Shumeng Zhang</a></span>, <span><a href=../author/yuru-huang/>Yuru Huang</a></span>, <span><a href=../author/zeyu-wang/>Zeyu Wang</a></span>, <span><a href=../author/mingming-fan/>Mingming Fan</a></span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="corresponding author"></i></div><span class=article-date>February 2024
</span><span class=middot-divider></span>
<span class=pub-publication>CHI 2024</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://www.mingmingfan.com/papers/CHI24_OlderAdults_VR_Accessibility_Modeling.pdf target=_blank rel=noopener>PDF</a></div></div><div class=ml-3><a href=../publication/chi24-older-vr/><img src=../publication/chi24-older-vr/featured_hud39d9f4125ca11b3e9bcc77a80042ef7_1435094_150x0_resize_q75_h2_lanczos.webp height=104 width=150 alt="Toward Making Virtual Reality (VR) More Inclusive for Older Adults: Investigating Aging Effects on Object Selection and Manipulation in VR" loading=lazy></a></div></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2024"><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=../publication/chi24-vr-rehabilitation-motivation/>To Reach the Unreachable: Exploring the Potential of VR Hand Redirection for Upper Limb Rehabilitation</a></div><div class="stream-meta article-metadata"><div class=article-metadata><div><span><a href=../author/peixuan-xiong/>Peixuan Xiong</a></span>, <span><a href=../author/yukai-zhang/>Yukai Zhang</a></span>, <span><a href=../author/nandi-zhang/>Nandi Zhang</a></span>, <span><a href=../author/shihan-fu/>Shihan Fu</a></span>, <span><a href=../author/xin-li/>Xin Li</a></span>, <span><a href=../author/yadan-zheng/>Yadan Zheng</a></span>, <span><a href=../author/jinni-zhou/>Jinni ZHOU</a></span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="corresponding author"></i>, <span><a href=../author/xiquan-hu/>Xiquan Hu</a></span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="corresponding author"></i>, <span><a href=../author/mingming-fan/>Mingming Fan</a></span></div><span class=article-date>February 2024
</span><span class=middot-divider></span>
<span class=pub-publication>CHI 2024</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://www.mingmingfan.com/papers/CHI24_VR_Visual_Offset_Stoke_Patiences_Rehabilitation.pdf target=_blank rel=noopener>PDF</a></div></div><div class=ml-3><a href=../publication/chi24-vr-rehabilitation-motivation/><img src=../publication/chi24-vr-rehabilitation-motivation/featured_hu89a6c222d965a847645213585ccdb82c_222591_150x0_resize_q75_h2_lanczos.webp height=103 width=150 alt="To Reach the Unreachable: Exploring the Potential of VR Hand Redirection for Upper Limb Rehabilitation" loading=lazy></a></div></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2024"><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=../publication/chi24-lightsword/>LightSword: A Customized Virtual Reality Exergame for Long-Term Cognitive Inhibition Training in Older Adults</a></div><div class="stream-meta article-metadata"><div class=article-metadata><div><span><a href=../author/qiuxin-du/>Qiuxin Du</a></span>, <span><a href=../author/zhen-song/>Zhen Song</a></span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="corresponding author"></i>, <span><a href=../author/haiyan-jiang/>Haiyan Jiang</a></span>, <span><a href=../author/xiaoying-wei/>Xiaoying Wei</a></span>, <span><a href=../author/dongdong-weng/>Dongdong Weng</a></span>, <span><a href=../author/mingming-fan/>Mingming Fan</a></span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="corresponding author"></i></div><span class=article-date>February 2024
</span><span class=middot-divider></span>
<span class=pub-publication>CHI 2024</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://mingmingfan.com/papers/CHI_2024__Cognitive_Training.pdf target=_blank rel=noopener>PDF</a></div></div><div class=ml-3></div></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2024"><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=../publication/chi24-fetchaid/>FetchAid: Making Parcel Lockers More Accessible to Blind and Low Vision People With Deep-learning Enhanced Touchscreen Guidance, Error-Recovery Mechanism, and AR-based Search Support</a></div><div class="stream-meta article-metadata"><div class=article-metadata><div><span><a href=../author/klara-zhitong-guan/>Klara Zhitong Guan</a></span>, <span><a href=../author/zeyu-xiong/>Zeyu Xiong</a></span>, <span><a href=../author/mingming-fan/>Mingming Fan</a></span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="corresponding author"></i></div><span class=article-date>February 2024
</span><span class=middot-divider></span>
<span class=pub-publication>CHI 2024</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://www.mingmingfan.com/papers/CHI24-FetchAid.pdf target=_blank rel=noopener>PDF</a></div></div><div class=ml-3><a href=../publication/chi24-fetchaid/><img src=../publication/chi24-fetchaid/featured_hu36747f81c2e76bcacf6bc010ab2e155a_178351_150x0_resize_q75_h2_lanczos.webp height=92 width=150 alt="FetchAid: Making Parcel Lockers More Accessible to Blind and Low Vision People With Deep-learning Enhanced Touchscreen Guidance, Error-Recovery Mechanism, and AR-based Search Support" loading=lazy></a></div></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-4 year-2024"><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=../publication/chi24-ux-collaboration-ai/>Enhancing UX Evaluation Through Collaboration with Conversational AI Assistants: Effects of Proactive Dialogue and Timing</a></div><div class="stream-meta article-metadata"><div class=article-metadata><div><span><a href=../author/emily-kuang/>Emily Kuang</a></span>, <span><a href=../author/minghao-li/>Minghao Li</a></span>, <span><a href=../author/mingming-fan/>Mingming Fan</a></span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="corresponding author"></i>, <span><a href=../author/kristen-shinohara/>Kristen Shinohara</a></span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="corresponding author"></i></div><span class=article-date>February 2024
</span><span class=middot-divider></span>
<span class=pub-publication>CHI 2024</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://www.mingmingfan.com/papers/CHI24_UX_Proactive_Assistant.pdf target=_blank rel=noopener>PDF</a></div></div><div class=ml-3><a href=../publication/chi24-ux-collaboration-ai/><img src=../publication/chi24-ux-collaboration-ai/featured_hud09f17e46f2267d5311c704ffe1b6f25_146365_150x0_resize_q75_h2_lanczos.webp height=58 width=150 alt="Enhancing UX Evaluation Through Collaboration with Conversational AI Assistants: Effects of Proactive Dialogue and Timing" loading=lazy></a></div></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2024"><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=../publication/chi24-vr-limb-disability/>Designing Upper-Body Gesture Interaction with and for People with Spinal Muscular Atrophy in VR</a></div><div class="stream-meta article-metadata"><div class=article-metadata><div><span><a href=../author/jingze-tian/>Jingze Tian</a></span>, <span><a href=../author/yingna-wang/>Yingna Wang</a></span>, <span><a href=../author/keye-yu/>Keye Yu</a></span>, <span><a href=../author/liyi-xu/>Liyi Xu</a></span>, <span><a href=../author/junan-xie/>Junan Xie</a></span>, <span><a href=../author/franklin-mingzhe-li/>Franklin Mingzhe Li</a></span>, <span><a href=../author/yafeng-niu/>Yafeng Niu</a></span>, <span><a href=../author/mingming-fan/>Mingming Fan</a></span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="corresponding author"></i></div><span class=article-date>February 2024
</span><span class=middot-divider></span>
<span class=pub-publication>CHI 2024</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://mingmingfan.com/papers/CHI24-user-defined-gesture-VR-people-atrophy.pdf target=_blank rel=noopener>PDF</a></div></div><div class=ml-3><a href=../publication/chi24-vr-limb-disability/><img src=../publication/chi24-vr-limb-disability/featured_huc8081818376a797acc1de41bd5165dd4_166815_150x0_resize_q75_h2_lanczos.webp height=88 width=150 alt="Designing Upper-Body Gesture Interaction with and for People with Spinal Muscular Atrophy in VR" loading=lazy></a></div></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2024"><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=../publication/chi24-blv-electro-chart/>Designing Unobtrusive Modulated Electrotactile Feedback on Fingertip Edge to Assist Blind and Low Vision (BLV) People in Comprehending Charts</a></div><div class="stream-meta article-metadata"><div class=article-metadata><div><span><a href=../author/chutian-jiang/>Chutian Jiang</a></span>, <span><a href=../author/yinan-fan/>Yinan Fan</a></span>, <span><a href=../author/junan-xie/>Junan Xie</a></span>, <span><a href=../author/emily-kuang/>Emily Kuang</a></span>, <span><a href=../author/kaihao-zhang/>Kaihao Zhang</a></span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="corresponding author"></i>, <span><a href=../author/mingming-fan/>Mingming Fan</a></span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="corresponding author"></i></div><span class=article-date>February 2024
</span><span class=middot-divider></span>
<span class=pub-publication>CHI 2024</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://www.mingmingfan.com/papers/CHI24_Haptic_Guidance_InfoGraphics_PVI.pdf target=_blank rel=noopener>PDF</a></div></div><div class=ml-3><a href=../publication/chi24-blv-electro-chart/><img src=../publication/chi24-blv-electro-chart/featured_hu5c08d43769c0ad9065c45998c18a4b22_130781_150x0_resize_q75_h2_lanczos.webp height=122 width=150 alt="Designing Unobtrusive Modulated Electrotactile Feedback on Fingertip Edge to Assist Blind and Low Vision (BLV) People in Comprehending Charts" loading=lazy></a></div></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-3 year-2024"><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=../publication/chi24-coprompt/>CoPrompt: Supporting Prompt Sharing and Referring in Collaborative Natural Language Programming</a></div><div class="stream-meta article-metadata"><div class=article-metadata><div><span><a href=../author/li-feng/>Li Feng</a></span>, <span><a href=../author/ryan-yen/>Ryan Yen</a></span>, <span><a href=../author/yuzhe-you/>Yuzhe You</a></span>, <span><a href=../author/mingming-fan/>Mingming Fan</a></span>, <span><a href=../author/jian-zhao/>Jian Zhao</a></span>, <span><a href=../author/zhicong-lu/>Zhicong Lu</a></span></div><span class=article-date>February 2024
</span><span class=middot-divider></span>
<span class=pub-publication>CHI 2024</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://www.mingmingfan.com/papers/CHI24_CoPrompt.pdf target=_blank rel=noopener>PDF</a></div></div><div class=ml-3><a href=../publication/chi24-coprompt/><img src=../publication/chi24-coprompt/featured_hua6ea0d395e743e68c26607ee0389556e_162913_150x0_resize_q75_h2_lanczos.webp height=76 width=150 alt="CoPrompt: Supporting Prompt Sharing and Referring in Collaborative Natural Language Programming" loading=lazy></a></div></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2024"><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=../publication/chi24-literacy-gap/>Bridging the Literacy Gap for Adults: Streaming and Engaging in Adult Literacy Education through Livestreaming</a></div><div class="stream-meta article-metadata"><div class=article-metadata><div><span><a href=../author/shihan-fu/>Shihan Fu</a></span>, <span><a href=../author/jianhao-chen/>Jianhao Chen</a></span>, <span><a href=../author/emily-kuang/>Emily Kuang</a></span>, <span><a href=../author/mingming-fan/>Mingming Fan</a></span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="corresponding author"></i></div><span class=article-date>February 2024
</span><span class=middot-divider></span>
<span class=pub-publication>CHI 2024</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://www.mingmingfan.com/papers/CHI24_Adult_Literacy_Livestreaming.pdf target=_blank rel=noopener>PDF</a></div></div><div class=ml-3><a href=../publication/chi24-literacy-gap/><img src=../publication/chi24-literacy-gap/featured_hue6c3b3b5b5e1c558972989bf804f34f2_201530_150x0_resize_q75_h2_lanczos.webp height=70 width=150 alt="Bridging the Literacy Gap for Adults: Streaming and Engaging in Adult Literacy Education through Livestreaming" loading=lazy></a></div></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-0 year-2024"><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=../publication/chi24-opera-makeup/>“It is hard to remove from my eye': Design Makeup Residue Visualization System for Chinese Traditional Opera (Xiqu) Performers</a></div><div class="stream-meta article-metadata"><div class=article-metadata><div><span><a href=../author/zeyu-xiong/>Zeyu Xiong</a></span>, <span><a href=../author/shihan-fu/>Shihan Fu</a></span>, <span><a href=../author/yanying-zhu/>Yanying Zhu</a></span>, <span><a href=../author/chenqing-zhu/>Chenqing Zhu</a></span>, <span><a href=../author/xiaojuan-ma/>Xiaojuan Ma</a></span>, <span><a href=../author/mingming-fan/>Mingming Fan</a></span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="corresponding author"></i></div><span class=article-date>February 2024
</span><span class=middot-divider></span>
<span class=pub-publication>CHI 2024</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://www.mingmingfan.com/papers/CHI24-Xiqu.pdf target=_blank rel=noopener>PDF</a></div></div><div class=ml-3><a href=../publication/chi24-opera-makeup/><img src=../publication/chi24-opera-makeup/featured_huf855a0950fd841816950f7b1156b7139_150886_150x0_resize_q75_h2_lanczos.webp height=48 width=150 alt="“It is hard to remove from my eye': Design Makeup Residue Visualization System for Chinese Traditional Opera (Xiqu) Performers" loading=lazy></a></div></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2023"><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=../publication/imwut-ubicomp2023-ar-photo-storytelling/>Exploring The Opportunities of AR for Enriching Storytelling With Family Photos Among Grandparents and Grandchildren</a></div><div class="stream-meta article-metadata"><div class=article-metadata><div><span><a href=../author/zisu-li/>Zisu Li</a></span>, <span><a href=../author/li-feng/>Li Feng</a></span>, <span><a href=../author/chen-liang/>Chen Liang</a></span>, <span><a href=../author/yuru-huang/>Yuru Huang</a></span>, <span><a href=../author/mingming-fan/>Mingming Fan</a></span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Corresponding author"></i></div><span class=article-date>September 2023
</span><span class=middot-divider></span>
<span class=pub-publication>IMWUT 2023</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://dl.acm.org/doi/pdf/10.1145/3610903 target=_blank rel=noopener>PDF</a></div></div><div class=ml-3><a href=../publication/imwut-ubicomp2023-ar-photo-storytelling/><img src=../publication/imwut-ubicomp2023-ar-photo-storytelling/featured_hu2735f2f9596fa664256865ed2b5e6d79_2697338_150x0_resize_q75_h2_lanczos_3.webp height=65 width=150 alt="Exploring The Opportunities of AR for Enriching Storytelling With Family Photos Among Grandparents and Grandchildren" loading=lazy></a></div></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2023"><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=../publication/assets23-dda/>Understanding Strategies and Challenges of Conducting Daily Data Analysis (DDA) Among Blind and Low Vision People</a></div><div class="stream-meta article-metadata"><div class=article-metadata><div><span><a href=../author/chutian-jiang/>Chutian Jiang</a></span>, <span><a href=../author/wentao-lei/>Wentao Lei</a></span>, <span><a href=../author/emily-kuang/>Emily Kuang</a></span>, <span><a href=../author/teng-han/>Teng Han</a></span>, <span><a href=../author/mingming-fan/>Mingming Fan</a></span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Corresponding author"></i></div><span class=article-date>September 2023
</span><span class=middot-divider></span>
<span class=pub-publication>ASSETS 2023</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://www.mingmingfan.com/papers/ASSETS23_PVI_Data_exploratory_analysis.pdf target=_blank rel=noopener>PDF</a></div></div><div class=ml-3><a href=../publication/assets23-dda/><img src=../publication/assets23-dda/featured_hu8186dc08cbaf644e8bdc5d38d714f1c9_1585049_150x0_resize_q75_h2_lanczos_3.webp height=96 width=150 alt="Understanding Strategies and Challenges of Conducting Daily Data Analysis (DDA) Among Blind and Low Vision People" loading=lazy></a></div></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2023"><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=../publication/assets23-curators-museum-accessibility/>Understanding Curators' Practices and Challenges of Making Exhibitions More Accessible for Blind and Low Vision People</a></div><div class="stream-meta article-metadata"><div class=article-metadata><div><span><a href=../author/yuru-huang/>Yuru Huang</a></span>, <span><a href=../author/jingling-zhang/>Jingling Zhang</a></span>, <span><a href=../author/xiaofu-jin/>Xiaofu Jin</a></span>, <span><a href=../author/mingming-fan/>Mingming Fan</a></span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Corresponding author"></i></div><span class=article-date>September 2023
</span><span class=middot-divider></span>
<span class=pub-publication>ASSETS 2023</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://www.mingmingfan.com/papers/ASSETS23_Museum_Accessibility.pdf target=_blank rel=noopener>PDF</a></div></div><div class=ml-3><a href=../publication/assets23-curators-museum-accessibility/><img src=../publication/assets23-curators-museum-accessibility/featured_hu6e34eae41830e59d5061139171c22c78_571883_150x0_resize_q75_h2_lanczos_3.webp height=53 width=150 alt="Understanding Curators' Practices and Challenges of Making Exhibitions More Accessible for Blind and Low Vision People" loading=lazy></a></div></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2023"><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=../publication/vinci-2023-odor/>OdorV-Art: An Initial Exploration of An Olfactory Intervention for Appreciating Style Information of Artworks in Virtual Museum</a></div><div class="stream-meta article-metadata"><div class=article-metadata><div><span><a href=../author/shumeng-zhang/>Shumeng Zhang</a></span>, <span><a href=../author/ziyan-wang/>Ziyan Wang</a></span>, <span><a href=../author/you-zhou/>You Zhou</a></span>, <span><a href=../author/hao-cui/>Hao Cui</a></span>, <span><a href=../author/shihan-fu/>Shihan Fu</a></span>, <span><a href=../author/zeyu-wang/>Zeyu Wang</a></span>, <span><a href=../author/mingming-fan/>Mingming Fan</a></span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Corresponding author"></i></div><span class=article-date>August 2023
</span><span class=middot-divider></span>
<span class=pub-publication>VINCI’23</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://www.mingmingfan.com/papers/OdorV-Art-VINCI.pdf target=_blank rel=noopener>PDF</a></div></div><div class=ml-3><a href=../publication/vinci-2023-odor/><img src=../publication/vinci-2023-odor/featured_hu2f1e02e515e351c311ec896043222feb_129691_150x0_resize_q75_h2_lanczos.webp height=40 width=150 alt="OdorV-Art: An Initial Exploration of An Olfactory Intervention for Appreciating Style Information of Artworks in Virtual Museum" loading=lazy></a></div></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-4 year-2023"><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=../publication/tvcg-2023-uxsense/>uxSense: Supporting User Experience Analysis with Visualization and Computer Vision</a></div><div class="stream-meta article-metadata"><div class=article-metadata><div><span><a href=../author/andrea-batch/>Andrea Batch</a></span>, <span><a href=../author/yipeng-ji/>Yipeng Ji</a></span>, <span><a href=../author/mingming-fan/>Mingming Fan</a></span>, <span><a href=../author/jian-zhao/>Jian Zhao</a></span>, <span><a href=../author/niklas-elmqvist/>Niklas Elmqvist</a></span></div><span class=article-date>March 2023
</span><span class=middot-divider></span>
<span class=pub-publication>TVCG 2023</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://www.mingmingfan.com/papers/TVCG_uxSense.pdf target=_blank rel=noopener>PDF
</a><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/10.1109/TVCG.2023.3241581 target=_blank rel=noopener>DOI</a></div></div><div class=ml-3><a href=../publication/tvcg-2023-uxsense/><img src=../publication/tvcg-2023-uxsense/featured_hu73197f1111d73663912e0ff0526d2f03_1208447_150x0_resize_q75_h2_lanczos_3.webp height=82 width=150 alt="uxSense: Supporting User Experience Analysis with Visualization and Computer Vision" loading=lazy></a></div></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-4 year-2023"><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=../publication/taccess-haptic-blv-review/>uxSense: Supporting User Experience Analysis with Visualization and Computer Vision</a></div><div class="stream-meta article-metadata"><div class=article-metadata><div><span><a href=../author/andrea-batch/>Andrea Batch</a></span>, <span><a href=../author/yipeng-ji/>Yipeng Ji</a></span>, <span><a href=../author/mingming-fan/>Mingming Fan</a></span>, <span><a href=../author/jian-zhao/>Jian Zhao</a></span>, <span><a href=../author/niklas-elmqvist/>Niklas Elmqvist</a></span></div><span class=article-date>March 2023
</span><span class=middot-divider></span>
<span class=pub-publication>TVCG 2023</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://www.mingmingfan.com/papers/TVCG_uxSense.pdf target=_blank rel=noopener>PDF
</a><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/10.1109/TVCG.2023.3241581 target=_blank rel=noopener>DOI</a></div></div><div class=ml-3><a href=../publication/taccess-haptic-blv-review/><img src=../publication/taccess-haptic-blv-review/featured_hu72836e3ad933df1470daae31ec358a14_12351054_150x0_resize_q75_h2_lanczos_3.webp height=123 width=150 alt="uxSense: Supporting User Experience Analysis with Visualization and Computer Vision" loading=lazy></a></div></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2023"><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=../publication/chi23-copractter/>CoPracTter: Toward Integrating Personalized Practice Scenarios, Timely Feedback and Social Support into An Online Support Tool for Coping with Stuttering in China</a></div><div class="stream-meta article-metadata"><div class=article-metadata><div><span><a href=../author/li-feng/>Li Feng</a></span>, <span><a href=../author/zeyu-xiong/>Zeyu Xiong</a></span>, <span><a href=../author/xinyi-li/>Xinyi Li</a></span>, <span><a href=../author/mingming-fan/>Mingming Fan</a></span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Corresponding author"></i></div><span class=article-date>February 2023
</span><span class=middot-divider></span>
<span class=pub-publication>CHI 2023</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://www.mingmingfan.com/papers/CHI23-CoPractTer.pdf target=_blank rel=noopener>PDF</a></div></div><div class=ml-3><a href=../publication/chi23-copractter/><img src=../publication/chi23-copractter/featured_hu63eaf26345d82d4e3c76c08f565ac722_6442030_150x0_resize_q75_h2_lanczos.webp height=61 width=150 alt="CoPracTter: Toward Integrating Personalized Practice Scenarios, Timely Feedback and Social Support into An Online Support Tool for Coping with Stuttering in China" loading=lazy></a></div></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-2 year-2023"><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=../publication/chi23-vr-intergenerational-gap/>Bridging the Generational Gap : Exploring How Virtual Reality Supports Remote Communication Between Grandparents and Grandchildren</a></div><div class="stream-meta article-metadata"><div class=article-metadata><div><span><a href=../author/xiaoying-wei/>Xiaoying Wei</a></span>, <span><a href=../author/yizheng-gu/>Yizheng Gu</a></span>, <span><a href=../author/emily-kuang/>Emily Kuang</a></span>, <span><a href=../author/xian-wang/>Xian Wang</a></span>, <span><a href=../author/beiyan-cao/>Beiyan Cao</a></span>, <span><a href=../author/xiaofu-jin/>Xiaofu Jin</a></span>, <span><a href=../author/mingming-fan/>Mingming Fan</a></span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Corresponding author"></i></div><span class=article-date>February 2023
</span><span class=middot-divider></span>
<span class=pub-publication>CHI 2023</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://www.mingmingfan.com/papers/CHI23-bridge-gap.pdf target=_blank rel=noopener>PDF
</a><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/chi23-vr-intergenerational-gap/cite.bib>Cite</a></div></div><div class=ml-3><a href=../publication/chi23-vr-intergenerational-gap/><img src=../publication/chi23-vr-intergenerational-gap/featured_hu8e3acd7b452b1579ef464d2de134b32b_1925300_150x0_resize_q75_h2_lanczos.webp height=80 width=150 alt="Bridging the Generational Gap : Exploring How Virtual Reality Supports Remote Communication Between Grandparents and Grandchildren" loading=lazy></a></div></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2023"><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=../publication/chi23-oa-typing/>Enhancing Older Adults’ Gesture Typing Experience Using the T9 Keyboard on Small Touchscreen Devices</a></div><div class="stream-meta article-metadata"><div class=article-metadata><div><span><a href=../author/emily-kuang/>Emily Kuang</a></span>, <span><a href=../author/ruihuan-chen/>Ruihuan Chen</a></span>, <span><a href=../author/mingming-fan/>Mingming Fan</a></span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Corresponding author"></i></div><span class=article-date>February 2023
</span><span class=middot-divider></span>
<span class=pub-publication>CHI 2023</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://www.mingmingfan.com/papers/CHI23_OA_Gesture_Typing_T9.pdf target=_blank rel=noopener>PDF
</a><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/chi23-oa-typing/cite.bib>Cite
</a><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/10.1145/3544548.3581105 target=_blank rel=noopener>DOI</a></div></div><div class=ml-3><a href=../publication/chi23-oa-typing/><img src=../publication/chi23-oa-typing/featured_hu281c0878955817726fab0276a47d20cc_2022324_150x0_resize_q75_h2_lanczos_3.webp height=49 width=150 alt="Enhancing Older Adults’ Gesture Typing Experience Using the T9 Keyboard on Small Touchscreen Devices" loading=lazy></a></div></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-3 year-2023"><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=../publication/chi23-ux-conversational-ai-assistant/>Collaboration with Conversational AI Assistants for UX Evaluation: Questions and How to Ask them (Voice vs. Text)</a></div><div class="stream-meta article-metadata"><div class=article-metadata><div><span><a href=../author/emily-kuang/>Emily Kuang</a></span>, <span><a href=../author/ehsan-jahangirzadeh-soure/>Ehsan Jahangirzadeh Soure</a></span>, <span><a href=../author/mingming-fan/>Mingming Fan</a></span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Corresponding author"></i>, <span><a href=../author/jian-zhao/>Jian Zhao</a></span>, <span><a href=../author/kristen-shinohara/>Kristen Shinohara</a></span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Corresponding author"></i></div><span class=article-date>February 2023
</span><span class=middot-divider></span>
<span class=pub-publication>CHI 2023</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://www.mingmingfan.com/papers/CHI23_UX_Design_Probe.pdf target=_blank rel=noopener>PDF
</a><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/chi23-ux-conversational-ai-assistant/cite.bib>Cite
</a><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/10.1145/3544548.3581247 target=_blank rel=noopener>DOI</a></div></div><div class=ml-3><a href=../publication/chi23-ux-conversational-ai-assistant/><img src=../publication/chi23-ux-conversational-ai-assistant/featured_hu2ded00ce85048d122b739fa6c894a05a_871191_150x0_resize_q75_h2_lanczos_3.webp height=68 width=150 alt="Collaboration with Conversational AI Assistants for UX Evaluation: Questions and How to Ask them (Voice vs. Text)" loading=lazy></a></div></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2023"><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=../publication/chi23-dhh-livestreaming/>Sparkling Silence: Practices and Challenges of Livestreaming Among Deaf or Hard of Hearing Streamers</a></div><div class="stream-meta article-metadata"><div class=article-metadata><div><span><a href=../author/beiyan-cao/>Beiyan Cao</a></span>, <span><a href=../author/changyang-he/>Changyang He</a></span>, <span><a href=../author/muzhi-zhou/>Muzhi Zhou</a></span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Corresponding author"></i>, <span><a href=../author/mingming-fan/>Mingming Fan</a></span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Corresponding author"></i></div><span class=article-date>February 2023
</span><span class=middot-divider></span>
<span class=pub-publication>CHI 2023</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://www.mingmingfan.com/papers/CHI23-DHH-livestreaming.pdf target=_blank rel=noopener>PDF</a></div></div><div class=ml-3><a href=../publication/chi23-dhh-livestreaming/><img src=../publication/chi23-dhh-livestreaming/featured_hu66403fa46f96e7790dcc41af899c893f_7785539_150x0_resize_q75_h2_lanczos.webp height=50 width=150 alt="Sparkling Silence: Practices and Challenges of Livestreaming Among Deaf or Hard of Hearing Streamers" loading=lazy></a></div></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2023"><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=../publication/chi23-blv-robot-navigation/>"I am the follower, also the boss": Exploring Different Levels of Autonomy and Machine Forms of Guiding Robots for the Visually Impaired</a></div><div class="stream-meta article-metadata"><div class=article-metadata><div><span><a href=../author/yan-zhang/>Yan Zhang</a></span>, <span><a href=../author/ziang-li/>Ziang Li</a></span>, <span><a href=../author/haole-guo/>Haole Guo</a></span>, <span><a href=../author/luyao-wang/>Luyao Wang</a></span>, <span><a href=../author/qihe-chen/>Qihe Chen</a></span>, <span><a href=../author/wenjie-jiang/>Wenjie Jiang</a></span>, <span><a href=../author/mingming-fan/>Mingming Fan</a></span>, <span><a href=../author/guyue-zhou/>Guyue Zhou</a></span>, <span><a href=../author/jiangtao-gong/>Jiangtao Gong</a></span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Corresponding author"></i></div><span class=article-date>February 2023
</span><span class=middot-divider></span>
<span class=pub-publication>CHI 2023</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://www.mingmingfan.com/papers/CHI2023_Navigation_Robot_Autonomy.pdf target=_blank rel=noopener>PDF</a></div></div><div class=ml-3><a href=../publication/chi23-blv-robot-navigation/><img src=../publication/chi23-blv-robot-navigation/featured_hu7dd70351c480c5148e6d50933c6c82c5_6142309_150x0_resize_q75_h2_lanczos.webp height=83 width=150 alt='"I am the follower, also the boss": Exploring Different Levels of Autonomy and Machine Forms of Guiding Robots for the Visually Impaired' loading=lazy></a></div></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-6 year-2023"><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=../publication/chi23-voice-hand-face/>Enabling Voice-Accompanying Hand-to-Face Gesture Recognition with Cross-Device Sensing</a></div><div class="stream-meta article-metadata"><div class=article-metadata><div><span><a href=../author/zisu-li/>Zisu Li</a></span>, <span><a href=../author/chen-liang/>Chen Liang</a></span>, <span><a href=../author/yuntao-wang/>Yuntao Wang</a></span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Corresponding author"></i>, <span><a href=../author/yue-qin/>Yue Qin</a></span>, <span><a href=../author/chun-yu/>Chun Yu</a></span>, <span><a href=../author/yukang-yan/>Yukang Yan</a></span>, <span><a href=../author/mingming-fan/>Mingming Fan</a></span>, <span><a href=../author/yuanchun-shi/>Yuanchun Shi</a></span></div><span class=article-date>February 2023
</span><span class=middot-divider></span>
<span class=pub-publication>CHI 2023</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://www.mingmingfan.com/papers/CHI23-voice-hand-gestures.pdf target=_blank rel=noopener>PDF
</a><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/chi23-voice-hand-face/cite.bib>Cite</a></div></div><div class=ml-3><a href=../publication/chi23-voice-hand-face/><img src=../publication/chi23-voice-hand-face/featured_hub707d3d57145ddd61bb61ed9712d439d_375050_150x0_resize_q75_h2_lanczos.webp height=77 width=150 alt="Enabling Voice-Accompanying Hand-to-Face Gesture Recognition with Cross-Device Sensing" loading=lazy></a></div></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2022"><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=../publication/iwc2022-oa-vr-game-ta/>Older Adults’ Concurrent and Retrospective Think-Aloud Verbalizations for Identifying User Experience Problems of VR Games</a></div><div class="stream-meta article-metadata"><div class=article-metadata><div><span><a href=../author/mingming-fan/>Mingming Fan</a></span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Corresponding author"></i>, <span><a href=../author/vinita-tibdewal/>Vinita Tibdewal</a></span>, <span><a href=../author/qiwen-zhao/>Qiwen Zhao</a></span>, <span><a href=../author/lizhou-cao/>Lizhou Cao</a></span>, <span><a href=../author/chao-peng/>Chao Peng</a></span>, <span><a href=../author/runxuan-shu/>Runxuan Shu</a></span>, <span><a href=../author/yujia-shan/>Yujia Shan</a></span></div><span class=article-date>December 2022
</span><span class=middot-divider></span>
<span class=pub-publication>IwC 2022</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href="https://academic.oup.com/iwc/advance-article/doi/10.1093/iwc/iwac039/6967130?login=true" target=_blank rel=noopener>PDF</a></div></div><div class=ml-3><a href=../publication/iwc2022-oa-vr-game-ta/><img src=../publication/iwc2022-oa-vr-game-ta/featured_hu7a04c764ee55d5a23cb29d9fccfa075b_1261702_150x0_resize_q75_h2_lanczos.webp height=112 width=150 alt="Older Adults’ Concurrent and Retrospective Think-Aloud Verbalizations for Identifying User Experience Problems of VR Games" loading=lazy></a></div></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2022"><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=../publication/imwut22-oa-interactive-guidance/>Synapse: Interactive Guidance by Demonstration with Trial-and-Error Support for Older Adults to Use Smartphone Apps</a></div><div class="stream-meta article-metadata"><div class=article-metadata><div><span><a href=../author/xiaofu-jin/>Xiaofu Jin</a></span>, <span><a href=../author/xiaozhu-hu/>Xiaozhu Hu</a></span>, <span><a href=../author/xiaoying-wei/>Xiaoying Wei</a></span>, <span><a href=../author/mingming-fan/>Mingming Fan</a></span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Corresponding author"></i></div><span class=article-date>December 2022
</span><span class=middot-divider></span>
<span class=pub-publication>IMWUT 2022</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://www.mingmingfan.com/papers/IMWUT_OlderAdults_Trial_And_Error.pdf target=_blank rel=noopener>PDF
</a><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://youtu.be/kSv-HOeTIkc target=_blank rel=noopener>Video</a></div></div><div class=ml-3><a href=../publication/imwut22-oa-interactive-guidance/><img src=../publication/imwut22-oa-interactive-guidance/featured_hucab56ae345f1dde599102718887d224d_1943476_150x0_resize_q75_h2_lanczos.webp height=28 width=150 alt="Synapse: Interactive Guidance by Demonstration with Trial-and-Error Support for Older Adults to Use Smartphone Apps" loading=lazy></a></div></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-3 year-2022"><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=../publication/cscw22-hai-ux-evaluation/>Human-AI Collaboration for UX Evaluation: Effects of Explanation and Synchronization</a></div><div class="stream-meta article-metadata"><div class=article-metadata><div><span><a href=../author/mingming-fan/>Mingming Fan</a></span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Corresponding Author"></i>, <span><a href=../author/xianyou-yang/>Xianyou Yang</a></span>, <span><a href=../author/tsz-tung-yu/>Tsz Tung Yu</a></span>, <span><a href=../author/vera-q.-liao/>Vera Q. Liao</a></span>, <span><a href=../author/jian-zhao/>Jian Zhao</a></span></div><span class=article-date>December 2022
</span><span class=middot-divider></span>
<span class=pub-publication>CSCW 2022</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://www.mingmingfan.com/papers/haicollaboration-cscw2022.pdf target=_blank rel=noopener>PDF</a></div></div><div class=ml-3><a href=../publication/cscw22-hai-ux-evaluation/><img src=../publication/cscw22-hai-ux-evaluation/featured_hu0d5b524f76b3552493e940be452e707d_137076_150x0_resize_q75_h2_lanczos.webp height=92 width=150 alt="Human-AI Collaboration for UX Evaluation: Effects of Explanation and Synchronization" loading=lazy></a></div></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-4 year-2022"><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=../publication/cscw22-typist/>Typist Experiment: an Investigation of Human-to-Human Dictation via Role-play to Inform Voice-based Text Authoring</a></div><div class="stream-meta article-metadata"><div class=article-metadata><div><span><a href=../author/can-liu/>Can Liu</a></span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Corresponding author"></i>, <span><a href=../author/siying-hu/>Siying Hu</a></span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="student first author"></i>, <span><a href=../author/li-feng/>Li Feng</a></span>, <span><a href=../author/mingming-fan/>Mingming Fan</a></span></div><span class=article-date>December 2022
</span><span class=middot-divider></span>
<span class=pub-publication>CSCW 2022</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://dl.acm.org/doi/pdf/10.1145/3555758 target=_blank rel=noopener>PDF
</a><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://youtu.be/f9lO9tin4tw target=_blank rel=noopener>Video</a></div></div><div class=ml-3><a href=../publication/cscw22-typist/><img src=../publication/cscw22-typist/featured_hucac4c27c650346a6f5be2228389f5ea1_220965_150x0_resize_q75_h2_lanczos.webp height=74 width=150 alt="Typist Experiment: an Investigation of Human-to-Human Dictation via Role-play to Inform Voice-based Text Authoring" loading=lazy></a></div></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2022"><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=../publication/assets22-oa-banking/>"I Used To Carry A Wallet, Now I Just Need To Carry My Phone": Understanding Current Banking Practices and Challenges Among Older Adults in China</a></div><div class="stream-meta article-metadata"><div class=article-metadata><div><span><a href=../author/xiaofu-jin/>Xiaofu Jin</a></span>, <span><a href=../author/mingming-fan/>Mingming Fan</a></span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Corresponding author"></i></div><span class=article-date>December 2022
</span><span class=middot-divider></span>
<span class=pub-publication>ASSETS 2022</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://www.mingmingfan.com/papers/ASSETS22_Older_Adults_Banking.pdf target=_blank rel=noopener>PDF</a></div></div><div class=ml-3><a href=../publication/assets22-oa-banking/><img src=../publication/assets22-oa-banking/featured_huddae42340f0608af87bddd00dfad519f_581163_150x0_resize_q75_h2_lanczos.webp height=79 width=150 alt='"I Used To Carry A Wallet, Now I Just Need To Carry My Phone": Understanding Current Banking Practices and Challenges Among Older Adults in China' loading=lazy></a></div></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-2 year-2022"><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=../publication/ijhci-poeticar/>PoeticAR: Reviving Traditional Poetry of the Heritage Site of Jichang Garden via Augmented Reality</a></div><div class="stream-meta article-metadata"><div class=article-metadata><div><span><a href=../author/jin-tian/>Jin Tian</a></span>, <span><a href=../author/yifan-cao/>Yifan Cao</a></span>, <span><a href=../author/lingyi-feng/>Lingyi Feng</a></span>, <span><a href=../author/dongting-fu/>Dongting Fu</a></span>, <span><a href=../author/linping-yuan/>Linping Yuan</a></span>, <span><a href=../author/huaming-qu/>Huaming Qu</a></span>, <span><a href=../author/yang-wang/>Yang Wang</a></span>, <span><a href=../author/mingming-fan/>Mingming Fan</a></span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Corresponding author"></i></div><span class=article-date>December 2022
</span><span class=middot-divider></span>
<span class=pub-publication>IJHCI</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href="https://www.tandfonline.com/doi/abs/10.1080/10447318.2023.2176806?journalCode=hihc20" target=_blank rel=noopener>PDF</a></div></div><div class=ml-3><a href=../publication/ijhci-poeticar/><img src=../publication/ijhci-poeticar/featured_hu2ab18d0dc2a0e15fba7201cd82276b96_11602670_150x0_resize_q75_h2_lanczos.webp height=86 width=150 alt="PoeticAR: Reviving Traditional Poetry of the Heritage Site of Jichang Garden via Augmented Reality" loading=lazy></a></div></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2022"><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=../publication/chinesechi2022-olderadults-ai/>Understanding Older Adults' Perceptions and Challenges in Using AI-enabled Everyday Technologies</a></div><div class="stream-meta article-metadata"><div class=article-metadata><div><span><a href=../author/esha-shandilya/>Esha Shandilya</a></span>, <span><a href=../author/mingming-fan/>Mingming Fan</a></span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Corresponding author"></i></div><span class=article-date>December 2022
</span><span class=middot-divider></span>
<span class=pub-publication>Chinese CHI 2022</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://www.mingmingfan.com/papers/ChineseCHI22_OlderAdults_AI.pdf target=_blank rel=noopener>PDF</a></div></div><div class=ml-3><a href=../publication/chinesechi2022-olderadults-ai/><img src=../publication/chinesechi2022-olderadults-ai/featured_hub335d3ea1f75636366816e50c7809c44_195110_150x0_resize_q75_h2_lanczos.webp height=63 width=150 alt="Understanding Older Adults' Perceptions and Challenges in Using AI-enabled Everyday Technologies" loading=lazy></a></div></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-2 year-2022"><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=../publication/chinesechi2022-lit-review-vr-communication/>Communication in Immersive Social Virtual Reality: A Systematic Review of 10 Years' Studies</a></div><div class="stream-meta article-metadata"><div class=article-metadata><div><span><a href=../author/xiaoying-wei/>Xiaoying Wei</a></span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="equal contribution"></i>, <span><a href=../author/xiaofu-jin/>Xiaofu Jin</a></span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="equal contribution"></i>, <span><a href=../author/mingming-fan/>Mingming Fan</a></span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Corresponding author"></i></div><span class=article-date>December 2022
</span><span class=middot-divider></span>
<span class=pub-publication>Chinese CHI 2022</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://www.mingmingfan.com/papers/ChineseCHI22_Literature_Review_Social_VR.pdf target=_blank rel=noopener>PDF</a></div></div><div class=ml-3><a href=../publication/chinesechi2022-lit-review-vr-communication/><img src=../publication/chinesechi2022-lit-review-vr-communication/featured_hu3f1f475507c9da43a9a946b83d6e5d9c_776362_150x0_resize_q75_h2_lanczos.webp height=72 width=150 alt="Communication in Immersive Social Virtual Reality: A Systematic Review of 10 Years' Studies" loading=lazy></a></div></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-2 year-2022"><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=../publication/chinesechi2022-lit-review-vr-meditation/>Reducing Stress and Anxiety in the Metaverse: A Systematic Review of Meditation, Mindfulness and Virtual Reality</a></div><div class="stream-meta article-metadata"><div class=article-metadata><div><span><a href=../author/xian-wang/>Xian Wang</a></span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="equal contribution"></i>, <span><a href=../author/xiaoyu-wang/>Xiaoyu Wang</a></span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="equal contribution"></i>, <span><a href=../author/mingming-fan/>Mingming Fan</a></span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Corresponding Author"></i>, <span><a href=../author/lik-hang-lee/>Lik-Hang Lee</a></span>, <span><a href=../author/bertram-e.-shi/>Bertram E. Shi</a></span>, <span><a href=../author/pan-hui/>Pan Hui</a></span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Corresponding Author"></i></div><span class=article-date>December 2022
</span><span class=middot-divider></span>
<span class=pub-publication>Chinese CHI 2022</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://www.mingmingfan.com/papers/ChineseCHI_22_Meditation_Social_VR.pdf target=_blank rel=noopener>PDF</a></div></div><div class=ml-3><a href=../publication/chinesechi2022-lit-review-vr-meditation/><img src=../publication/chinesechi2022-lit-review-vr-meditation/featured_hu3084f71d5600ef863a5fc58ccbd0c99c_3383139_150x0_resize_q75_h2_lanczos.webp height=103 width=150 alt="Reducing Stress and Anxiety in the Metaverse: A Systematic Review of Meditation, Mindfulness and Virtual Reality" loading=lazy></a></div></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2022"><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=../publication/dis22-blv-livestreaming/>'It Feels Like Being Locked In A Cage': Understanding Blind or Low Vision Streamers' Perceptions of Content Curation Algorithms</a></div><div class="stream-meta article-metadata"><div class=article-metadata><div><span><a href=../author/zhiyi-rong/>Zhiyi Rong</a></span>, <span><a href=../author/mo-zhou/>Mo Zhou</a></span>, <span><a href=../author/zhicong-lu/>Zhicong Lu</a></span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Corresponding author"></i>, <span><a href=../author/mingming-fan/>Mingming Fan</a></span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Corresponding author"></i></div><span class=article-date>December 2022
</span><span class=middot-divider></span>
<span class=pub-publication>DIS 2022</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://www.mingmingfan.com/papers/DIS22_BLV_Livestreaming.pdf target=_blank rel=noopener>PDF</a></div></div><div class=ml-3><a href=../publication/dis22-blv-livestreaming/><img src=../publication/dis22-blv-livestreaming/featured_huef0bf3da992d02c92efa6598554a617f_3082829_150x0_resize_q75_h2_lanczos.webp height=61 width=150 alt=" 'It Feels Like Being Locked In A Cage': Understanding Blind or Low Vision Streamers' Perceptions of Content Curation Algorithms" loading=lazy></a></div></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2022"><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=../publication/ijhci-olderadults-covid-vis/>Understanding How Older Adults Comprehend COVID-19 Interactive Visualizations via Think-Aloud Protocol</a></div><div class="stream-meta article-metadata"><div class=article-metadata><div><span><a href=../author/mingming-fan/>Mingming Fan</a></span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Corresponding author"></i>, <span><a href=../author/yiwen-wang/>Yiwen Wang</a></span>, <span><a href=../author/yuni-xie/>Yuni Xie</a></span>, <span><a href=../author/franklin-li/>Franklin Li</a></span>, <span><a href=../author/chunyang-chen/>Chunyang Chen</a></span></div><span class=article-date>December 2022
</span><span class=middot-divider></span>
<span class=pub-publication>IJHCI 2022</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://www.mingmingfan.com/papers/IJHCI-older-adults-visualizations.pdf target=_blank rel=noopener>PDF
</a><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://www.mingmingfan.com/papers/IJHCI-older-adults-visualizations-appendix.pdf target=_blank rel=noopener>Source Document</a></div></div><div class=ml-3><a href=../publication/ijhci-olderadults-covid-vis/><img src=../publication/ijhci-olderadults-covid-vis/featured_hu65fa16c210d01cc509c7b4a791933e6c_463709_150x0_resize_q75_h2_lanczos.webp height=124 width=150 alt="Understanding How Older Adults Comprehend COVID-19 Interactive Visualizations via Think-Aloud Protocol" loading=lazy></a></div></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2022"><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=../publication/chi22-kuaidigui/>'I Shake The Package To Check If It's Mine': A Study of Package Fetching Practices and Challenges of Blind and Low Vision People in China</a></div><div class="stream-meta article-metadata"><div class=article-metadata><div><span><a href=../author/wentao-lei/>Wentao Lei</a></span>, <span><a href=../author/mingming-fan/>Mingming Fan</a></span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Corresponding author"></i>, <span><a href=../author/juliann-thang/>Juliann Thang</a></span></div><span class=article-date>March 2022
</span><span class=middot-divider></span>
<span class=pub-publication>CHI 2022</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://www.mingmingfan.com/papers/chi22_243_BLV_KuaiDiGui.pdf target=_blank rel=noopener>PDF</a></div></div><div class=ml-3><a href=../publication/chi22-kuaidigui/><img src=../publication/chi22-kuaidigui/featured_hu9189f72fef482c53a3d235abff599ee9_5712045_150x0_resize_q75_h2_lanczos.webp height=123 width=150 alt=" 'I Shake The Package To Check If It's Mine': A Study of Package Fetching Practices and Challenges of Blind and Low Vision People in China" loading=lazy></a></div></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2022"><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=../publication/chi22-userdefinedgestures/>'I Don't Want People to Look At Me Differently': Designing User-Defined Above-the-Neck Gestures for People with Upper Body Motor Impairments</a></div><div class="stream-meta article-metadata"><div class=article-metadata><div><span><a href=../author/xuan-zhao/>Xuan Zhao</a></span>, <span><a href=../author/mingming-fan/>Mingming Fan</a></span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Corresponding author"></i>, <span><a href=../author/teng-han/>Teng Han</a></span></div><span class=article-date>March 2022
</span><span class=middot-divider></span>
<span class=pub-publication>CHI 2022</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://www.mingmingfan.com/papers/chi22_243_BLV_KuaiDiGui.pdf target=_blank rel=noopener>PDF</a></div></div><div class=ml-3><a href=../publication/chi22-userdefinedgestures/><img src=../publication/chi22-userdefinedgestures/featured_hud01615fd1e109ad5e9bfd88ce28472e6_744233_150x0_resize_q75_h2_lanczos.webp height=170 width=150 alt=" 'I Don't Want People to Look At Me Differently': Designing User-Defined Above-the-Neck Gestures for People with Upper Body Motor Impairments" loading=lazy></a></div></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-4 year-2022"><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=../publication/chi22-ux-survey/>"Merging Results Is No Easy Task": An International Survey Study of Collaborative Data Analysis Practices Among UX Practitioners</a></div><div class="stream-meta article-metadata"><div class=article-metadata><div><span><a href=../author/emily-kuang/>Emily Kuang</a></span>, <span><a href=../author/xiaofu-jin/>Xiaofu Jin</a></span>, <span><a href=../author/mingming-fan/>Mingming Fan</a></span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Corresponding author"></i></div><span class=article-date>March 2022
</span><span class=middot-divider></span>
<span class=pub-publication>CHI 2022</span></div></div><div class=btn-links><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/chi22-ux-survey/cite.bib>Cite
</a><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/10.1145/3491102.3517647 target=_blank rel=noopener>DOI</a></div></div><div class=ml-3><a href=../publication/chi22-ux-survey/><img src=../publication/chi22-ux-survey/featured_hu4039d55e0dc39e9ddd6d3b2c7c28dbc3_95433_150x0_resize_q75_h2_lanczos_3.webp height=69 width=150 alt='"Merging Results Is No Easy Task": An International Survey Study of Collaborative Data Analysis Practices Among UX Practitioners' loading=lazy></a></div></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-4 year-2022"><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=../publication/chi22-storytelling/>From WOW to WHY: Guidelines for Creating the Opening of a Data Video with Cinematic Styles</a></div><div class="stream-meta article-metadata"><div class=article-metadata><div><span><a href=../author/xian-xu/>Xian Xu</a></span>, <span><a href=../author/leni-yang/>Leni Yang</a></span>, <span><a href=../author/david-yip/>David Yip</a></span>, <span><a href=../author/mingming-fan/>Mingming Fan</a></span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Corresponding author"></i>, <span><a href=../author/zheng-wei/>Zheng Wei</a></span>, <span><a href=../author/huamin-qu/>Huamin Qu</a></span></div><span class=article-date>March 2022
</span><span class=middot-divider></span>
<span class=pub-publication>CHI 2022</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://www.mingmingfan.com/papers/CHI22_Storytelling_Film_openings.pdf target=_blank rel=noopener>PDF</a></div></div><div class=ml-3><a href=../publication/chi22-storytelling/><img src=../publication/chi22-storytelling/featured_hu97801766cc66368490a20555e7e35831_1827583_150x0_resize_q75_h2_lanczos.webp height=197 width=150 alt="From WOW to WHY: Guidelines for Creating the Opening of a Data Video with Cinematic Styles" loading=lazy></a></div></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-5 year-2022"><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=../publication/chi22-non-textual-communication/>'I need to be professional until my new team uses emoji, GIFs, or memes first': New Collaborators’ Perspectives on Using Non-Textual Communication in Virtual Workspaces</a></div><div class="stream-meta article-metadata"><div class=article-metadata><div><span><a href=../author/esha-shandilya/>Esha Shandilya</a></span>, <span><a href=../author/mingming-fan/>Mingming Fan</a></span>, <span><a href=../author/garreth-tigwell/>Garreth Tigwell</a></span></div><span class=article-date>March 2022
</span><span class=middot-divider></span>
<span class=pub-publication>CHI 2022</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://www.mingmingfan.com/papers/CHI_2022_Remote_Work.pdf target=_blank rel=noopener>PDF</a></div></div><div class=ml-3><a href=../publication/chi22-non-textual-communication/><img src=../publication/chi22-non-textual-communication/featured_hua19fe96a3fc9a56519495ff5b0b1b625_3565847_150x0_resize_q75_h2_lanczos.webp height=75 width=150 alt=" 'I need to be professional until my new team uses emoji, GIFs, or memes first': New Collaborators’ Perspectives on Using Non-Textual Communication in Virtual Workspaces" loading=lazy></a></div></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2022"><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=../publication/cacm-eyelidgestures/>Eyelid Gestures for People with Motor Impairments</a></div><div class="stream-meta article-metadata"><div class=article-metadata><div><span><a href=../author/mingming-fan/>Mingming Fan</a></span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Corresponding author"></i>, <span><a href=../author/zhen-li/>Zhen Li</a></span>, <span><a href=../author/franklin-mingzhe-li/>Franklin Mingzhe Li</a></span></div><span class=article-date>March 2022
</span><span class=middot-divider></span>
<span class=pub-publication>CACM 2022</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://cacm.acm.org/magazines/2022/1/257451-eyelid-gestures-for-people-with-motor-impairments/pdf target=_blank rel=noopener>PDF
</a><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://github.com/mingming-fan/EyelidGesturesDetection target=_blank rel=noopener>Code
</a><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://cacm.acm.org/magazines/2022/1/257452-technical-perspective-eyelid-gestures-enhance-mobile-interaction/abstract target=_blank rel=noopener>Project
</a><a class="btn btn-outline-primary btn-page-header btn-sm" href="https://www.youtube.com/watch?v=GgpW4tmvdM0" target=_blank rel=noopener>Video
</a><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://cacm.acm.org/magazines/2022/1/257451-eyelid-gestures-for-people-with-motor-impairments/fulltext target=_blank rel=noopener>Source Document</a></div></div><div class=ml-3><a href=../publication/cacm-eyelidgestures/><img src=../publication/cacm-eyelidgestures/featured_hu37f591076f984926bf945660cf87cb66_153401_150x0_resize_q75_h2_lanczos.webp height=150 width=150 alt="Eyelid Gestures for People with Motor Impairments" loading=lazy></a></div></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-4 year-2021"><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=../publication/verbalizations-chinese/>Think-Aloud Verbalizations for Identifying User Experience Problems: Effects of Language Proficiency with Chinese Non-Native English Speakers</a></div><div class="stream-meta article-metadata"><div class=article-metadata><div><span><a href=../author/mingming-fan/>Mingming Fan</a></span>, <span><a href=../author/lingyun-julie-zhu/>Lingyun (Julie) Zhu</a></span></div><span class=article-date>December 2021
</span><span class=middot-divider></span>
<span class=pub-publication>Chinese CHI 2021</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://www.mingmingfan.com/papers/chinesechi2021-3.pdf target=_blank rel=noopener>PDF</a></div></div><div class=ml-3><a href=../publication/verbalizations-chinese/><img src=../publication/verbalizations-chinese/featured_hud79f0c2d55f9ab0564e15707095df7fc_49443_150x0_resize_q75_h2_lanczos.webp height=80 width=150 alt="Think-Aloud Verbalizations for Identifying User Experience Problems: Effects of Language Proficiency with Chinese Non-Native English Speakers" loading=lazy></a></div></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-3 year-2021"><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=../publication/coux/>CoUX: Collaborative Visual Analysis of Think-Aloud Usability Test Videos for Digital Interfaces</a></div><div class="stream-meta article-metadata"><div class=article-metadata><div><span><a href=../author/ehsan-jahangirzadeh-soure/>Ehsan Jahangirzadeh Soure</a></span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Equal Contribution"></i>, <span><a href=../author/emily-kuang/>Emily Kuang</a></span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Equal Contribution"></i>, <span><a href=../author/mingming-fan/>Mingming Fan</a></span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Corresponding author"></i>, <span><a href=../author/jian-zhao/>Jian Zhao</a></span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Corresponding author"></i></div><span class=article-date>December 2021
</span><span class=middot-divider></span>
<span class=pub-publication>VIS 2021</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://www.mingmingfan.com/papers/coux-vis21.pdf target=_blank rel=noopener>PDF
</a><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/coux/cite.bib>Cite
</a><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://github.com/WatVis/CoUX target=_blank rel=noopener>Code
</a><a class="btn btn-outline-primary btn-page-header btn-sm" href="https://www.youtube.com/watch?v=uaSl0x4c93Y" target=_blank rel=noopener>Video
</a><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/10.1109/TVCG.2021.3114822 target=_blank rel=noopener>DOI</a></div></div><div class=ml-3><a href=../publication/coux/><img src=../publication/coux/featured_hu5aa6ab34d05d6616d78c82fee54391f1_768341_150x0_resize_q75_h2_lanczos_3.webp height=82 width=150 alt="CoUX: Collaborative Visual Analysis of Think-Aloud Usability Test Videos for Digital Interfaces" loading=lazy></a></div></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2021"><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=../publication/vr-pain/>Douleur: Creating Pain Sensation with Chemical Stimulant to Enhance User Experience in Virtual Reality</a></div><div class="stream-meta article-metadata"><div class=article-metadata><div><span><a href=../author/chutian-jiang/>Chutian Jiang</a></span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="equal contribution"></i>, <span><a href=../author/yanjun-chen/>Yanjun Chen</a></span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="equal contribution"></i>, <span><a href=../author/mingming-fan/>Mingming Fan</a></span>, <span><a href=../author/liuping-wang/>Liuping Wang</a></span>, <span><a href=../author/luyao-shen/>Luyao Shen</a></span>, <span><a href=../author/nianlong-li/>Nianlong Li</a></span>, <span><a href=../author/wei-sun/>Wei Sun</a></span>, <span><a href=../author/yu-zhang/>Yu Zhang</a></span>, <span><a href=../author/feng-tian/>Feng Tian</a></span>, <span><a href=../author/teng-han/>Teng Han</a></span></div><span class=article-date>December 2021
</span><span class=middot-divider></span>
<span class=pub-publication>IMWUT (UbiComp 2021)</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://www.mingmingfan.com/papers/Douleur-IMWUT-UbiComp21.pdf target=_blank rel=noopener>PDF</a></div></div><div class=ml-3><a href=../publication/vr-pain/><img src=../publication/vr-pain/featured_hu382c1060a22a012bcd748818925723e9_607554_150x0_resize_q75_h2_lanczos.webp height=60 width=150 alt="Douleur: Creating Pain Sensation with Chemical Stimulant to Enhance User Experience in Virtual Reality" loading=lazy></a></div></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2021"><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=../publication/appaccessibility/>Accessibile or Not? An Empirical Investigation of Android App Accessibility</a></div><div class="stream-meta article-metadata"><div class=article-metadata><div><span><a href=../author/sen-chen/>Sen Chen</a></span>, <span><a href=../author/chunyang-chen/>Chunyang Chen</a></span>, <span><a href=../author/lingling-fan/>Lingling Fan</a></span>, <span><a href=../author/mingming-fan/>Mingming Fan</a></span>, <span><a href=../author/xian-zhan/>Xian Zhan</a></span>, <span><a href=../author/yang-liu/>Yang Liu</a></span></div><span class=article-date>December 2021
</span><span class=middot-divider></span>
<span class=pub-publication>TSE 2021</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://www.mingmingfan.com/papers/AppAccessibility-TSE21.pdf target=_blank rel=noopener>PDF
</a><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://github.com/tjusenchen/Xbot target=_blank rel=noopener>Code
</a><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://sites.google.com/view/mobile-accessibility/ target=_blank rel=noopener>Source Document</a></div></div><div class=ml-3><a href=../publication/appaccessibility/><img src=../publication/appaccessibility/featured_hu2082f94925e6847afaede8b0098ba040_43727_150x0_resize_q75_h2_lanczos.webp height=75 width=150 alt="Accessibile or Not? An Empirical Investigation of Android App Accessibility" loading=lazy></a></div></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2021"><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=../publication/oa-banking-survey/>"Too Old to Bank Digitally?": A Survey of Banking Practices and Challenges among Older Adults in China</a></div><div class="stream-meta article-metadata"><div class=article-metadata><div><span><a href=../author/xiaofu-jin/>Xiaofu Jin</a></span>, <span><a href=../author/emily-kuang/>Emily Kuang</a></span>, <span><a href=../author/mingming-fan/>Mingming Fan</a></span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Corresponding author"></i></div><span class=article-date>December 2021
</span><span class=middot-divider></span>
<span class=pub-publication>DIS 2021</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://www.mingmingfan.com/papers/DIS21_Older_Adults_Banking_Survey.pdf target=_blank rel=noopener>PDF
</a><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://www.mingmingfan.com/papers/DIS21-OlderAdults-Banking-Survey-Questions.pdf target=_blank rel=noopener>Source Document</a></div></div><div class=ml-3><a href=../publication/oa-banking-survey/><img src=../publication/oa-banking-survey/featured_hu2b16f1a148c732fe0b8307a80f45c5eb_108948_150x0_resize_q75_h2_lanczos.webp height=67 width=150 alt='"Too Old to Bank Digitally?": A Survey of Banking Practices and Challenges among Older Adults in China' loading=lazy></a></div></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2021"><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=../publication/oa-ta/>Older Adults' Think-Aloud Verbalizations and Speech Features for Identifying User Experience Problems</a></div><div class="stream-meta article-metadata"><div class=article-metadata><div><span><a href=../author/mingming-fan/>Mingming Fan</a></span>, <span><a href=../author/qiwen-zhao/>Qiwen Zhao</a></span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="first student author"></i>, <span><a href=../author/vinita-tibdewal/>Vinita Tibdewal</a></span></div><span class=article-date>December 2021
</span><span class=middot-divider></span>
<span class=pub-publication>CHI 2021</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://www.mingmingfan.com/papers/CHI21_OlderAdults_ThinkAloud_UXProblems.pdf target=_blank rel=noopener>PDF
</a><a class="btn btn-outline-primary btn-page-header btn-sm" href="https://www.youtube.com/watch?v=OkqStiGulbY" target=_blank rel=noopener>Video</a></div></div><div class=ml-3><a href=../publication/oa-ta/><img src=../publication/oa-ta/featured_huc5b2596daae37e4fdf3fb1aed1ae36ef_1083293_150x0_resize_q75_h2_lanczos.webp height=66 width=150 alt="Older Adults' Think-Aloud Verbalizations and Speech Features for Identifying User Experience Problems" loading=lazy></a></div></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2021"><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=../publication/at-china/>"I Choose Assistive Devices That Save My Face": A Study on Perceptions of Accessibility and Assistive Technology Use Conducted in China</a></div><div class="stream-meta article-metadata"><div class=article-metadata><div><span><a href=../author/franklin-mingzhe-li/>Franklin Mingzhe Li</a></span>, <span><a href=../author/di-laura-chen/>Di Laura Chen</a></span>, <span><a href=../author/mingming-fan/>Mingming Fan</a></span>, <span><a href=../author/khai-n.-truong/>Khai N. Truong</a></span></div><span class=article-date>May 2021
</span><span class=middot-divider></span>
<span class=pub-publication>CHI 2021</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://www.mingmingfan.com/papers/CHI21_China_ATs.pdf target=_blank rel=noopener>PDF</a></div></div><div class=ml-3><a href=../publication/at-china/><img src=../publication/at-china/featured_hu2dffac735c90915625ae7cb1e4c71ee6_517140_150x0_resize_q75_h2_lanczos.webp height=57 width=150 alt='"I Choose Assistive Devices That Save My Face": A Study on Perceptions of Accessibility and Assistive Technology Use Conducted in China' loading=lazy></a></div></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-2 year-2021"><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=../publication/vmirror/>vMirror: Enhancing the Interaction with Occluded or Distant Objects in VR with Virtual Mirrors</a></div><div class="stream-meta article-metadata"><div class=article-metadata><div><span><a href=../author/nianlong-li/>Nianlong Li</a></span>, <span><a href=../author/zhengquan-zhang/>Zhengquan Zhang</a></span>, <span><a href=../author/can-liu/>Can Liu</a></span>, <span><a href=../author/zengyao-yang/>Zengyao Yang</a></span>, <span><a href=../author/yinan-fu/>Yinan Fu</a></span>, <span><a href=../author/feng-tian/>Feng Tian</a></span>, <span><a href=../author/teng-han/>Teng Han</a></span>, <span><a href=../author/mingming-fan/>Mingming Fan</a></span></div><span class=article-date>May 2021
</span><span class=middot-divider></span>
<span class=pub-publication>CHI 2021</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://www.mingmingfan.com/papers/CHI21_vMirror_VR.pdf target=_blank rel=noopener>PDF
</a><a class="btn btn-outline-primary btn-page-header btn-sm" href="https://www.youtube.com/watch?v=6BSx3x2KhrQ" target=_blank rel=noopener>Video</a></div></div><div class=ml-3><a href=../publication/vmirror/><img src=../publication/vmirror/featured_hu4c2f12fa3df9ad19bce04a99d940389e_801576_150x0_resize_q75_h2_lanczos.webp height=41 width=150 alt="vMirror: Enhancing the Interaction with Occluded or Distant Objects in VR with Virtual Mirrors" loading=lazy></a></div></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-3 year-2021"><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=../publication/chartseer/>ChartSeer: Interactive Steering Exploratory Visual Analysis with Machine Intelligence</a></div><div class="stream-meta article-metadata"><div class=article-metadata><div><span><a href=../author/jian-zhao/>Jian Zhao</a></span>, <span><a href=../author/mingming-fan/>Mingming Fan</a></span>, <span><a href=../author/mi-feng/>Mi Feng</a></span></div><span class=article-date>May 2021
</span><span class=middot-divider></span>
<span class=pub-publication>TVCG (VIS 2021)</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://www.mingmingfan.com/papers/chartseer_tvcg.pdf target=_blank rel=noopener>PDF
</a><a class="btn btn-outline-primary btn-page-header btn-sm" href="https://www.youtube.com/watch?v=w5K1U6f1Oro" target=_blank rel=noopener>Video
</a><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/10.1109/TVCG.2020.3018724 target=_blank rel=noopener>DOI</a></div></div><div class=ml-3><a href=../publication/chartseer/><img src=../publication/chartseer/featured_hu51898de3fd3f3dcd85fe409605814c5e_1708443_150x0_resize_q75_h2_lanczos.webp height=93 width=150 alt="ChartSeer: Interactive Steering Exploratory Visual Analysis with Machine Intelligence" loading=lazy></a></div></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2020"><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=../publication/eyelidgestures4assets/>Eyelid Gestures on Mobile Devices for People with Motor Impairments</a></div><div class="stream-meta article-metadata"><div class=article-metadata><div><span><a href=../author/mingming-fan/>Mingming Fan</a></span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="equal contribution"></i>, <span><a href=../author/zhen-li/>Zhen Li</a></span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="equal contribution"></i>, <span><a href=../author/franklin-mingzhe-li/>Franklin Mingzhe Li</a></span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="equal contribution"></i></div><span class=article-date>November 2020
</span><span class=middot-divider></span>
<span class=pub-publication>ASSETS 2020</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://www.mingmingfan.com/papers/ASSETS_2020_Fan.pdf target=_blank rel=noopener>PDF
</a><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://github.com/mingming-fan/EyelidGesturesDetection target=_blank rel=noopener>Code
</a><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://cacm.acm.org/magazines/2022/1/257452-technical-perspective-eyelid-gestures-enhance-mobile-interaction/abstract target=_blank rel=noopener>Project
</a><a class="btn btn-outline-primary btn-page-header btn-sm" href="https://www.youtube.com/watch?v=hocUdFlgX5A" target=_blank rel=noopener>Video
</a><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://github.com/mingming-fan/EyelidGesturesDetection/releases target=_blank rel=noopener>Source Document</a></div></div><div class=ml-3><a href=../publication/eyelidgestures4assets/><img src=../publication/eyelidgestures4assets/featured_hu16917b679f115b07e8fd438e20cd64cf_105865_150x0_resize_q75_h2_lanczos.webp height=61 width=150 alt="Eyelid Gestures on Mobile Devices for People with Motor Impairments" loading=lazy></a></div></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-2 year-2020"><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=../publication/mouille/>Mouillé: Exploring Wetness Illusion on Fingertips to Enhance Immersive Experience in VR</a></div><div class="stream-meta article-metadata"><div class=article-metadata><div><span><a href=../author/teng-han/>Teng Han</a></span>, <span><a href=../author/sirui-wang/>Sirui Wang</a></span>, <span><a href=../author/sijia-wang/>Sijia Wang</a></span>, <span><a href=../author/xiangmin-fan/>Xiangmin Fan</a></span>, <span><a href=../author/jie-liu/>Jie Liu</a></span>, <span><a href=../author/feng-tian/>Feng Tian</a></span>, <span><a href=../author/and-mingming-fan/>and Mingming Fan</a></span></div><span class=article-date>May 2020
</span><span class=middot-divider></span>
<span class=pub-publication>CHI 2020</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://www.mingmingfan.com/papers/Mouille-CHI20.pdf target=_blank rel=noopener>PDF
</a><a class="btn btn-outline-primary btn-page-header btn-sm" href="https://www.youtube.com/watch?v=Fx0yt0Albe4" target=_blank rel=noopener>Video</a></div></div><div class=ml-3><a href=../publication/mouille/><img src=../publication/mouille/featured_hu1d9fbbbee08ed8cafbd0a22b8d272ed8_835072_150x0_resize_q75_h2_lanczos.webp height=75 width=150 alt="Mouillé: Exploring Wetness Illusion on Fingertips to Enhance Immersive Experience in VR" loading=lazy></a></div></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-3 year-2020"><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=../publication/uxproblemsdetection/>Automatic Detection of Usability Problem Encounters in Think-Aloud Sessions</a></div><div class="stream-meta article-metadata"><div class=article-metadata><div><span><a href=../author/mingming-fan/>Mingming Fan</a></span>, <span><a href=../author/yue-li/>Yue Li</a></span>, <span><a href=../author/khai-n.-truong/>Khai N. Truong</a></span></div><span class=article-date>May 2020
</span><span class=middot-divider></span>
<span class=pub-publication>TiiS 2020</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://www.mingmingfan.com/papers/TiiS2020-Fan.pdf target=_blank rel=noopener>PDF
</a><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/10.1145/3491102.3517647 target=_blank rel=noopener>DOI</a></div></div><div class=ml-3><a href=../publication/uxproblemsdetection/><img src=../publication/uxproblemsdetection/featured_hub8d52aa5372c63e77d4d9bef65ae253e_16155_150x0_resize_q75_h2_lanczos.webp height=63 width=150 alt="Automatic Detection of Usability Problem Encounters in Think-Aloud Sessions" loading=lazy></a></div></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-4 year-2020"><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=../publication/ta-industry/>Practices and Challenges of Using Think-aloud Protocols in Industry: An International Survey</a></div><div class="stream-meta article-metadata"><div class=article-metadata><div><span><a href=../author/mingming-fan/>Mingming Fan</a></span>, <span><a href=../author/serina-shi/>Serina Shi</a></span>, <span><a href=../author/khai-n.-truong/>Khai N. Truong</a></span></div><span class=article-date>January 2020
</span><span class=middot-divider></span>
<span class=pub-publication>JUS</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://www.mingmingfan.com/papers/JUS_Fan_Feb2020.pdf target=_blank rel=noopener>PDF
</a><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://www.mingmingfan.com/doc/ThinkAloudSurvey-FAN-Mingming.pdf target=_blank rel=noopener>Source Document</a></div></div><div class=ml-3><a href=../publication/ta-industry/><img src=../publication/ta-industry/featured_hue77dde376bee4d7780c96efa0a7a2449_99714_150x0_resize_q75_h2_lanczos.webp height=64 width=150 alt="Practices and Challenges of Using Think-aloud Protocols in Industry: An International Survey" loading=lazy></a></div></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-3 year-2019"><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=../publication/vista/>VisTA: Integrating Machine Intelligence with Visualization to Support the Investigation of Think-Aloud Sessions</a></div><div class="stream-meta article-metadata"><div class=article-metadata><div><span><a href=../author/mingming-fan/>Mingming Fan</a></span>, <span><a href=../author/ke-wu/>Ke Wu</a></span>, <span><a href=../author/jian-zhao/>Jian Zhao</a></span>, <span><a href=../author/yue-li/>Yue Li</a></span>, <span><a href=../author/winter-wei/>Winter Wei</a></span>, <span><a href=../author/khai-n.-truong/>Khai N. Truong</a></span></div><span class=article-date>April 2019
</span><span class=middot-divider></span>
<span class=pub-publication>TVCG (VIS 2019)</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://www.mingmingfan.com/papers/VisTA.pdf target=_blank rel=noopener>PDF
</a><a class="btn btn-outline-primary btn-page-header btn-sm" href="https://www.youtube.com/watch?v=YHpfBqm1Aaw" target=_blank rel=noopener>Video
</a><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/10.1109/TVCG.2019.2934797 target=_blank rel=noopener>DOI</a></div></div><div class=ml-3><a href=../publication/vista/><img src=../publication/vista/featured_hu94650d6457ed202eb28fa4c216d9947d_1136558_150x0_resize_q75_h2_lanczos.webp height=68 width=150 alt="VisTA: Integrating Machine Intelligence with Visualization to Support the Investigation of Think-Aloud Sessions" loading=lazy></a></div></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-4 year-2019"><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=../publication/verbalizationproblems/>Concurrent Think-Aloud Verbalizations and Usability Problems</a></div><div class="stream-meta article-metadata"><div class=article-metadata><div><span><a href=../author/mingming-fan/>Mingming Fan</a></span>, <span><a href=../author/jinglan-lin/>Jinglan Lin</a></span>, <span><a href=../author/christina-chung/>Christina Chung</a></span>, <span><a href=../author/khai-n.-truong/>Khai N. Truong</a></span></div><span class=article-date>April 2019
</span><span class=middot-divider></span>
<span class=pub-publication>TOCHI (CHI 2020)</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://www.mingmingfan.com/papers/TOCHI-2019-Fan.pdf target=_blank rel=noopener>PDF</a></div></div><div class=ml-3><a href=../publication/verbalizationproblems/><img src=../publication/verbalizationproblems/featured_hu14f77f312ddab55683bfd91fb282d54e_524706_150x0_resize_q75_h2_lanczos.webp height=59 width=150 alt="Concurrent Think-Aloud Verbalizations and Usability Problems" loading=lazy></a></div></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2019"><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=../publication/fmt/>FMT: A Wearable Camera-Based Object Tracking Memory Aid for Older Adults</a></div><div class="stream-meta article-metadata"><div class=article-metadata><div><span><a href=../author/franklin-mingzhe-li/>Franklin Mingzhe Li</a></span>, <span><a href=../author/di-laura-chen/>Di Laura Chen</a></span>, <span><a href=../author/mingming-fan/>Mingming Fan</a></span>, <span><a href=../author/khai-n.-truong/>Khai N. Truong</a></span></div><span class=article-date>April 2019
</span><span class=middot-divider></span>
<span class=pub-publication>IMWUT (UbiComp 2019)</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://www.mingmingfan.com/papers/FMT-IMWUT-2019.pdf target=_blank rel=noopener>PDF
</a><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://dl.acm.org/doi/10.1145/3351253 target=_blank rel=noopener>Source Document</a></div></div><div class=ml-3><a href=../publication/fmt/><img src=../publication/fmt/featured_hucd12e8de957195bb9fc463f3071003cc_535685_150x0_resize_q75_h2_lanczos.webp height=96 width=150 alt="FMT: A Wearable Camera-Based Object Tracking Memory Aid for Older Adults" loading=lazy></a></div></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-5 year-2019"><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=../publication/chi19-ich/>"I feel it is my responsibility to stream”: Streaming and Engaging with Intangible Cultural Heritage through Livestreaming</a></div><div class="stream-meta article-metadata"><div class=article-metadata><div><span><a href=../author/zhicong-lu/>Zhicong Lu</a></span>, <span><a href=../author/michelle-annett/>Michelle Annett</a></span>, <span><a href=../author/mingming-fan/>Mingming Fan</a></span>, <span><a href=../author/daniel-wigdor/>Daniel Wigdor</a></span></div><span class=article-date>April 2019
</span><span class=middot-divider></span>
<span class=pub-publication>CHI 2019</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://www.mingmingfan.com/papers/live-streaming-chi2019.pdf target=_blank rel=noopener>PDF</a></div></div><div class=ml-3><a href=../publication/chi19-ich/><img src=../publication/chi19-ich/featured_hu1ffff6a513bcabc629b2b47e2161713c_98463_150x0_resize_q75_h2_lanczos.webp height=100 width=150 alt='"I feel it is my responsibility to stream”: Streaming and Engaging with Intangible Cultural Heritage through Livestreaming' loading=lazy></a></div></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-6 year-2019"><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=../publication/pinchlist/>PinchList: Leveraging Pinch Gestures for Hierarchical List Navigation on Smartphones</a></div><div class="stream-meta article-metadata"><div class=article-metadata><div><span><a href=../author/teng-han/>Teng Han</a></span>, <span><a href=../author/jie-liu/>Jie Liu</a></span>, <span><a href=../author/khalad-hasan/>Khalad Hasan</a></span>, <span><a href=../author/mingming-fan/>Mingming Fan</a></span>, <span><a href=../author/junhyeok-kim/>Junhyeok Kim</a></span>, <span><a href=../author/jiannan-li/>Jiannan Li</a></span>, <span><a href=../author/xiangmin-fan/>Xiangmin Fan</a></span>, <span><a href=../author/feng-tian/>Feng Tian</a></span>, <span><a href=../author/edward-lank/>Edward Lank</a></span>, <span><a href=../author/pourang-irani/>Pourang Irani</a></span></div><span class=article-date>April 2019
</span><span class=middot-divider></span>
<span class=pub-publication>CHI 2019</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://www.mingmingfan.com/papers/pinchlist-chi2019.pdf target=_blank rel=noopener>PDF
</a><a class="btn btn-outline-primary btn-page-header btn-sm" href="https://www.youtube.com/watch?v=e5-_L90TB1E" target=_blank rel=noopener>Video</a></div></div><div class=ml-3><a href=../publication/pinchlist/><img src=../publication/pinchlist/featured_hua7887df763b33c6dc50fedf28fad7446_32071_150x0_resize_q75_h2_lanczos.webp height=131 width=150 alt="PinchList: Leveraging Pinch Gestures for Hierarchical List Navigation on Smartphones" loading=lazy></a></div></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-6 year-2019"><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=../publication/3d-finger-tracking/>Projected Visible Light for 3D Finger Tracking and Device Augmentation on Everyday Objects</a></div><div class="stream-meta article-metadata"><div class=article-metadata><div><span><a href=../author/shang-ma/>Shang Ma</a></span>, <span><a href=../author/qiong-liu/>Qiong Liu</a></span>, <span><a href=../author/mingming-fan/>Mingming Fan</a></span>, <span><a href=../author/phillip-sheu/>Phillip Sheu</a></span></div><span class=article-date>April 2019
</span><span class=middot-divider></span>
<span class=pub-publication>Internet of Things</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://www.mingmingfan.com/papers/InternetOfThings-2019-Ma.pdf target=_blank rel=noopener>PDF</a></div></div><div class=ml-3><a href=../publication/3d-finger-tracking/><img src=../publication/3d-finger-tracking/featured_hucd5912b10ccd2d3eb25559d9b2c88ec8_614288_150x0_resize_q75_h2_lanczos.webp height=48 width=150 alt="Projected Visible Light for 3D Finger Tracking and Device Augmentation on Everyday Objects" loading=lazy></a></div></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-3 year-2018"><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=../publication/inkplanner/>InkPlanner: Supporting Prewriting via Intelligent Visual Diagramming</a></div><div class="stream-meta article-metadata"><div class=article-metadata><div><span><a href=../author/zhicong-lu/>Zhicong Lu</a></span>, <span><a href=../author/mingming-fan/>Mingming Fan</a></span>, <span><a href=../author/yun-wang/>Yun Wang</a></span>, <span><a href=../author/jian-zhao/>Jian Zhao</a></span>, <span><a href=../author/michelle-annett/>Michelle Annett</a></span>, <span><a href=../author/daniel-wigdor/>Daniel Wigdor</a></span></div><span class=article-date>December 2018
</span><span class=middot-divider></span>
<span class=pub-publication>TVCG (VIS 2018)</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://www.mingmingfan.com/papers/InkPlanner_TVCG.pdf target=_blank rel=noopener>PDF
</a><a class="btn btn-outline-primary btn-page-header btn-sm" href="https://www.youtube.com/watch?v=vzVOdtCP5LM" target=_blank rel=noopener>Video
</a><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/10.1145/3491102.3517647 target=_blank rel=noopener>DOI</a></div></div><div class=ml-3><a href=../publication/inkplanner/><img src=../publication/inkplanner/featured_hudbd6c8bfdf01566ce54eaf958a0b0978_506254_150x0_resize_q75_h2_lanczos.webp height=94 width=150 alt="InkPlanner: Supporting Prewriting via Intelligent Visual Diagramming" loading=lazy></a></div></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2018"><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=../publication/seniorguidelines/>Guidelines for Creating Senior-Friendly Product Instructions</a></div><div class="stream-meta article-metadata"><div class=article-metadata><div><span><a href=../author/mingming-fan/>Mingming Fan</a></span>, <span><a href=../author/khai-n.-truong/>Khai N. Truong</a></span></div><span class=article-date>April 2018
</span><span class=middot-divider></span>
<span class=pub-publication>TACCESS (ASSETS 2018)</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://www.mingmingfan.com/papers/TACCESS-2018-Fan.pdf target=_blank rel=noopener>PDF</a></div></div><div class=ml-3><a href=../publication/seniorguidelines/><img src=../publication/seniorguidelines/featured_huf46122220513721019857947253c18bc_3142509_150x0_resize_q75_h2_lanczos.webp height=93 width=150 alt="Guidelines for Creating Senior-Friendly Product Instructions" loading=lazy></a></div></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2017"><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=../publication/braillesketch/>BrailleSketch: A Gesture-based Text Entry Method for People with Visual Impairments</a></div><div class="stream-meta article-metadata"><div class=article-metadata><div><span><a href=../author/franklin-li/>Franklin Li</a></span>, <span><a href=../author/mingming-fan/>Mingming Fan</a></span>, <span><a href=../author/khai-n.-truong/>Khai N. Truong</a></span></div><span class=article-date>August 2017
</span><span class=middot-divider></span>
<span class=pub-publication>ASSETS 2017</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://www.mingmingfan.com/papers/BrailleSketch-ASSETS-2017.pdf target=_blank rel=noopener>PDF</a></div></div><div class=ml-3><a href=../publication/braillesketch/><img src=../publication/braillesketch/featured_hu9f3a4d2f43c4f68e2627d7f2e1d6912a_278010_150x0_resize_q75_h2_lanczos.webp height=128 width=150 alt="BrailleSketch: A Gesture-based Text Entry Method for People with Visual Impairments" loading=lazy></a></div></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-6 year-2017"><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=../publication/mobileauthentication/>An Empirical Study of Touch-based Authentication Methods on Smartwatches</a></div><div class="stream-meta article-metadata"><div class=article-metadata><div><span><a href=../author/yue-zhao/>Yue Zhao</a></span>, <span><a href=../author/zhongtian-qiu/>Zhongtian Qiu</a></span>, <span><a href=../author/yiqing-yang/>Yiqing Yang</a></span>, <span><a href=../author/weiwei-li/>Weiwei Li</a></span>, <span><a href=../author/mingming-fan/>Mingming Fan</a></span></div><span class=article-date>August 2017
</span><span class=middot-divider></span>
<span class=pub-publication>ISWC 2017</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://www.mingmingfan.com/papers/ISWC17-foot-gesture.pdf target=_blank rel=noopener>PDF</a></div></div><div class=ml-3><a href=../publication/mobileauthentication/><img src=../publication/mobileauthentication/featured_hud4ccdff08c164888fe08dbc7c35b856b_1455321_150x0_resize_q75_h2_lanczos.webp height=91 width=150 alt="An Empirical Study of Touch-based Authentication Methods on Smartwatches" loading=lazy></a></div></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-6 year-2017"><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=../publication/foot/>An Empirical Study of Foot Gestures for Hands-Occupied Mobile Interaction</a></div><div class="stream-meta article-metadata"><div class=article-metadata><div><span><a href=../author/yue-zhao/>Yue Zhao</a></span>, <span><a href=../author/zhongtian-qiu/>Zhongtian Qiu</a></span>, <span><a href=../author/yiqing-yang/>Yiqing Yang</a></span>, <span><a href=../author/weiwei-li/>Weiwei Li</a></span>, <span><a href=../author/mingming-fan/>Mingming Fan</a></span></div><span class=article-date>July 2017
</span><span class=middot-divider></span>
<span class=pub-publication>ISWC 2017</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://www.mingmingfan.com/papers/ISWC17-foot-gesture.pdf target=_blank rel=noopener>PDF</a></div></div><div class=ml-3><a href=../publication/foot/><img src=../publication/foot/featured_hufb1aa3e6adaecbe08c5c9cc1bf1f683e_1574353_150x0_resize_q75_h2_lanczos.webp height=85 width=150 alt="An Empirical Study of Foot Gestures for Hands-Occupied Mobile Interaction" loading=lazy></a></div></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-6 year-2017"><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=../publication/liquid/>Exploring the Use of Capacitive Sensing to Externally Measure Liquid Level in Fluid Containers</a></div><div class="stream-meta article-metadata"><div class=article-metadata><div><span><a href=../author/mingming-fan/>Mingming Fan</a></span>, <span><a href=../author/khai-n.-truong/>Khai N. Truong</a></span>, <span><a href=../author/abhishek-ranjan/>Abhishek Ranjan</a></span></div><span class=article-date>February 2017
</span><span class=middot-divider></span>
<span class=pub-publication>Knowledge Media Design Institute Technical Report, University of Toronto, 2016</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://www.mingmingfan.com/papers/Exploring_Capacitive_Sensing_2016_1.pdf target=_blank rel=noopener>PDF
</a><a class="btn btn-outline-primary btn-page-header btn-sm" href="https://www.youtube.com/watch?v=_oGq8O_4fNk" target=_blank rel=noopener>Video</a></div></div><div class=ml-3><a href=../publication/liquid/><img src=../publication/liquid/featured_hudbb7e463e9b1a7842c78c13598373f70_406611_150x0_resize_q75_h2_lanczos.webp height=66 width=150 alt="Exploring the Use of Capacitive Sensing to Externally Measure Liquid Level in Fluid Containers" loading=lazy></a></div></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-6 year-2015"><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=../publication/soqr/>SoQr: Sonically Quantifying the Content Level inside Containers</a></div><div class="stream-meta article-metadata"><div class=article-metadata><div><span><a href=../author/mingming-fan/>Mingming Fan</a></span>, <span><a href=../author/khai-n.-truong/>Khai N. Truong</a></span></div><span class=article-date>February 2015
</span><span class=middot-divider></span>
<span class=pub-publication>UbiComp 2015</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://www.mingmingfan.com/papers/SoQr-UbiComp2015-Fan.pdf target=_blank rel=noopener>PDF
</a><a class="btn btn-outline-primary btn-page-header btn-sm" href="https://www.youtube.com/watch?v=el6vkFdT9Wc" target=_blank rel=noopener>Video</a></div></div><div class=ml-3><a href=../publication/soqr/><img src=../publication/soqr/featured_hua52d136f261db8fdbf79289558912617_339510_150x0_resize_q75_h2_lanczos.webp height=99 width=150 alt="SoQr: Sonically Quantifying the Content Level inside Containers" loading=lazy></a></div></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-6 year-2014"><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=../publication/restroom/>Public Restroom Detection on Mobile Phone via Active Probing</a></div><div class="stream-meta article-metadata"><div class=article-metadata><div><span><a href=../author/mingming-fan/>Mingming Fan</a></span>, <span><a href=../author/alexander-t.-adams/>Alexander T. Adams</a></span>, <span><a href=../author/khai-n.-truong/>Khai N. Truong</a></span></div><span class=article-date>February 2014
</span><span class=middot-divider></span>
<span class=pub-publication>ISWC 2014</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://www.mingmingfan.com/papers/ISWC14-restroom-detection.pdf target=_blank rel=noopener>PDF
</a><a class="btn btn-outline-primary btn-page-header btn-sm" href="https://www.youtube.com/watch?v=WLWRAHnDjec" target=_blank rel=noopener>Video</a></div></div><div class=ml-3><a href=../publication/restroom/><img src=../publication/restroom/featured_huf1de1059042dc301365a0e900bce3563_924238_150x0_resize_q75_h2_lanczos.webp height=72 width=150 alt="Public Restroom Detection on Mobile Phone via Active Probing" loading=lazy></a></div></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-6 year-2014"><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=../publication/hifi/>HiFi: Hide and Find Digital Content Associated with Physical Objects via Coded Light</a></div><div class="stream-meta article-metadata"><div class=article-metadata><div><span><a href=../author/mingming-fan/>Mingming Fan</a></span>, <span><a href=../author/qiong-liu/>Qiong Liu</a></span>, <span><a href=../author/hao-tang/>Hao Tang</a></span>, <span><a href=../author/patrick-chiu/>Patrick Chiu</a></span></div><span class=article-date>February 2014
</span><span class=middot-divider></span>
<span class=pub-publication>HotMobile 2014</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://www.mingmingfan.com/papers/HotMobile14-HiFi.pdf target=_blank rel=noopener>PDF</a></div></div><div class=ml-3><a href=../publication/hifi/><img src=../publication/hifi/featured_hu5079c9d63a933364afe33d8154429c6c_1402179_150x0_resize_q75_h2_lanczos.webp height=70 width=150 alt="HiFi: Hide and Find Digital Content Associated with Physical Objects via Coded Light" loading=lazy></a></div></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-6 year-2012"><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=../publication/erlang-cox/>Augmenting Gesture Recognition with Erlang-Cox Models to Identify Neurological Disorders in Premature Babies</a></div><div class="stream-meta article-metadata"><div class=article-metadata><div><span><a href=../author/mingming-fan/>Mingming Fan</a></span>, <span><a href=../author/dana-gravem/>Dana Gravem</a></span>, <span><a href=../author/dan-cooper/>Dan Cooper</a></span>, <span><a href=../author/donald-j-patterson/>Donald J Patterson</a></span></div><span class=article-date>February 2012
</span><span class=middot-divider></span>
<span class=pub-publication>UbiComp 2012</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://www.mingmingfan.com/papers/Ubicomp12_MingmingFan.pdf target=_blank rel=noopener>PDF
</a><a class="btn btn-outline-primary btn-page-header btn-sm" href="https://www.youtube.com/watch?v=WpW_zPow_zM" target=_blank rel=noopener>Video</a></div></div><div class=ml-3><a href=../publication/erlang-cox/><img src=../publication/erlang-cox/featured_hub2468cc38435e8d219dace7989a8d9e5_18104_150x0_resize_q75_h2_lanczos.webp height=105 width=150 alt="Augmenting Gesture Recognition with Erlang-Cox Models to Identify Neurological Disorders in Premature Babies" loading=lazy></a></div></div></div></div></div></div></div></div><div class=page-footer><div class=container><footer class=site-footer><p class="powered-by copyright-license-text">© 2025 APEX Group. This work is licensed under <a href=https://creativecommons.org/licenses/by-nc-nd/4.0 rel="noopener noreferrer" target=_blank>CC BY NC ND 4.0</a></p><p class="powered-by footer-license-icons"><a href=https://creativecommons.org/licenses/by-nc-nd/4.0 rel="noopener noreferrer" target=_blank aria-label="Creative Commons"><i class="fab fa-creative-commons fa-2x" aria-hidden=true></i>
<i class="fab fa-creative-commons-by fa-2x" aria-hidden=true></i>
<i class="fab fa-creative-commons-nc fa-2x" aria-hidden=true></i>
<i class="fab fa-creative-commons-nd fa-2x" aria-hidden=true></i></a></p><p class=powered-by>Published with <a href="https://wowchemy.com/?utm_campaign=poweredby" target=_blank rel=noopener>Wowchemy</a> — the free, <a href=https://github.com/wowchemy/wowchemy-hugo-themes target=_blank rel=noopener>open source</a> website builder that empowers creators.</p></footer></div></div><script src=../js/vendor-bundle.min.b4708d4364577c16ab7001b265a063a4.js></script><script src=https://cdn.jsdelivr.net/gh/desandro/imagesloaded@v4.1.4/imagesloaded.pkgd.min.js integrity="sha512-S5PZ9GxJZO16tT9r3WJp/Safn31eu8uWrzglMahDT4dsmgqWonRY9grk3j+3tfuPr9WJNsfooOR7Gi7HL5W2jw==" crossorigin=anonymous></script><script src=https://cdn.jsdelivr.net/gh/metafizzy/isotope@v3.0.6/dist/isotope.pkgd.min.js integrity="sha512-Zq2BOxyhvnRFXu0+WE6ojpZLOU2jdnqbrM1hmVdGzyeCa1DgM3X5Q4A/Is9xA1IkbUeDd7755dNNI/PzSf2Pew==" crossorigin=anonymous></script><script src=https://cdn.jsdelivr.net/npm/leaflet@1.7.1/dist/leaflet.min.js integrity crossorigin=anonymous></script><script id=search-hit-fuse-template type=text/x-template>
    <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
    </div>
  </script><script src=https://cdn.jsdelivr.net/gh/krisk/Fuse@v3.2.1/dist/fuse.min.js integrity="sha512-o38bmzBGX+hD3JHWUFCDA09btWaqrNmoJ3RXLlrysA7PP01Kgs4UlE4MhelE1v5dJR3+cxlR4qQlotsW7jKsnw==" crossorigin=anonymous></script><script src=https://cdn.jsdelivr.net/gh/julmot/mark.js@8.11.1/dist/jquery.mark.min.js integrity="sha512-mhbv5DqBMgrWL+32MmsDOt/OAvqr/cHimk6B8y/bx/xS88MVkYGPiVv2ixKVrkywF2qHplNRUvFsAHUdxZ3Krg==" crossorigin=anonymous></script><script id=page-data type=application/json>{"use_headroom":true}</script><script src=../js/wowchemy-headroom.db4755770454eb63685f8de785c0a172.js type=module></script><script src=../en/js/wowchemy.min.6ab16275cbca742a586c1726e3d94093.js></script><script src=../js/wowchemy-map.a26e9d2f7238ba5b868384f1c5bc6477.js type=module></script><div id=modal class="modal fade" role=dialog><div class=modal-dialog><div class=modal-content><div class=modal-header><h5 class=modal-title>Cite</h5><button type=button class=close data-dismiss=modal aria-label=Close>
<span aria-hidden=true>&#215;</span></button></div><div class=modal-body><pre><code></code></pre></div><div class=modal-footer><a class="btn btn-outline-primary my-1 js-copy-cite" href=# target=_blank><i class="fas fa-copy"></i> Copy
</a><a class="btn btn-outline-primary my-1 js-download-cite" href=# target=_blank><i class="fas fa-download"></i> Download</a><div id=modal-error></div></div></div></div></div><script src=../js/wowchemy-publication.68f8d7090562ca65fc6d3cb3f8f2d2cb.js type=module></script></body></html>
<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=generator content="Wowchemy 5.7.0 for Hugo"><link rel=stylesheet href=./css/vendor-bundle.min.16f785cdb553c8c4431db6775122af35.css media=print onload='this.media="all"'><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/academicons@1.9.2/css/academicons.min.css integrity="sha512-KlJCpRsLf+KKu2VQa5vmRuClRFjxc5lXO03ixZt82HZUk41+1I0bD8KBSA0fY290ayMfWYI9udIqeOWSu1/uZg==" crossorigin=anonymous media=print onload='this.media="all"'><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/leaflet@1.7.1/dist/leaflet.min.css integrity crossorigin=anonymous media=print onload='this.media="all"'><link rel=stylesheet href=./css/wowchemy.b9231f75dcc97a371ce6141b4d2aadaf.css><link rel=stylesheet href=./css/libs/chroma/github-light.min.css title=hl-light media=print onload='this.media="all"'><link rel=stylesheet href=./css/libs/chroma/dracula.min.css title=hl-dark media=print onload='this.media="all"' disabled><meta name=author content="Mingming Fan"><meta name=description content="A highly-customizable Hugo research group theme powered by Wowchemy website builder."><link rel=alternate hreflang=zh href=./zh/><link rel=alternate hreflang=en-us href=./><link rel=canonical href=./><link rel=manifest href=./manifest.webmanifest><link rel=icon type=image/png href=./media/icon_hu9a55cf1972a19f5af2e1cfae94af68a2_11598_32x32_fill_lanczos_center_3.png><link rel=apple-touch-icon type=image/png href=./media/icon_hu9a55cf1972a19f5af2e1cfae94af68a2_11598_180x180_fill_lanczos_center_3.png><meta name=theme-color content="#1565c0"><meta property="twitter:card" content="summary"><meta property="twitter:site" content="@wowchemy"><meta property="twitter:creator" content="@wowchemy"><meta property="twitter:image" content="/media/logo_hub0d66fecfb75d04d26dd973adb7e184d_88470_300x300_fit_lanczos_3.png"><meta property="og:site_name" content="APEX, HKUST(GZ) & HKUST"><meta property="og:url" content="/"><meta property="og:title" content="APEX, HKUST(GZ) & HKUST"><meta property="og:description" content="A highly-customizable Hugo research group theme powered by Wowchemy website builder."><meta property="og:image" content="/media/logo_hub0d66fecfb75d04d26dd973adb7e184d_88470_300x300_fit_lanczos_3.png"><meta property="og:locale" content="en-us"><meta property="og:updated_time" content="2025-02-22T00:00:00+00:00"><script type=application/ld+json>{"@context":"https://schema.org","@type":"WebSite","potentialAction":{"@type":"SearchAction","target":"/?q={search_term_string}","query-input":"required name=search_term_string"},"url":"/"}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"Organization","@id":"/","name":"APEX, HKUST(GZ) \u0026 HKUST","logo":"/media/logo_hub0d66fecfb75d04d26dd973adb7e184d_88470_192x192_fit_lanczos_3.png","url":"/"}</script><script src=https://identity.netlify.com/v1/netlify-identity-widget.js></script><link rel=alternate href=./index.xml type=application/rss+xml title="APEX, HKUST(GZ) & HKUST"><title>APEX, HKUST(GZ) & HKUST</title></head><body id=top data-spy=scroll data-offset=70 data-target=#TableOfContents class=page-wrapper><script src=./js/wowchemy-init.min.fe8634e7d00f14d07fb33caf14cc8e55.js></script><aside class=search-modal id=search><div class=container><section class=search-header><div class="row no-gutters justify-content-between mb-3"><div class=col-6><h1>Search</h1></div><div class="col-6 col-search-close"><a class=js-search href=# aria-label=Close><i class="fas fa-times-circle text-muted" aria-hidden=true></i></a></div></div><div id=search-box><input name=q id=search-query placeholder=Search... autocapitalize=off autocomplete=off autocorrect=off spellcheck=false type=search class=form-control aria-label=Search...></div></section><section class=section-search-results><div id=search-hits></div></section></div></aside><div class="page-header header--fixed"><header><nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id=navbar-main><div class=container-xl><div class="d-none d-lg-inline-flex"><a class=navbar-brand href=./><img src=./media/logo_hub0d66fecfb75d04d26dd973adb7e184d_88470_0x70_resize_lanczos_3.png alt="APEX, HKUST(GZ) & HKUST"></a></div><button type=button class=navbar-toggler data-toggle=collapse data-target=#navbar-content aria-controls=navbar-content aria-expanded=false aria-label="Toggle navigation">
<span><i class="fas fa-bars"></i></span></button><div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none"><a class=navbar-brand href=./><img src=./media/logo_hub0d66fecfb75d04d26dd973adb7e184d_88470_0x70_resize_lanczos_3.png alt="APEX, HKUST(GZ) & HKUST"></a></div><div class="navbar-collapse main-menu-item collapse justify-content-end" id=navbar-content><ul class="navbar-nav d-md-inline-flex"><li class=nav-item><a class=nav-link href=./><span>Home</span></a></li><li class=nav-item><a class=nav-link href=./people><span>People</span></a></li><li class=nav-item><a class=nav-link href=./publication><span>Publications</span></a></li></ul></div><ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2"><li class=nav-item><a class="nav-link js-search" href=# aria-label=Search><i class="fas fa-search" aria-hidden=true></i></a></li><li class="nav-item dropdown i18n-dropdown"><a href=# class="nav-link dropdown-toggle" data-toggle=dropdown aria-haspopup=true aria-label=Languages><i class="fas fa-globe mr-1" aria-hidden=true></i><span class="d-none d-lg-inline">English</span></a><div class=dropdown-menu><div class="dropdown-item dropdown-item-active"><span>English</span></div><a class=dropdown-item href=./zh/ data-target=/zh/><span>中文 (简体)</span></a></div></li></ul></div></nav></header></div><div class=page-body><span class="js-widget-page d-none"></span><section id=welcome class="home-section wg-blank"><div class=home-section-bg></div><div class=container><div class="row justify-content-center"><div class="section-heading col-12 mb-3 text-center"><h1 class=mb-0><strong>A</strong>ugmenting & <strong>P</strong>eople by <strong>E</strong>mpowering <strong>X</strong><br>(APEX) Group</h1></div><div class=col-12><br><div style=text-align:center><strong>Augmenting People by Empowering X (perception, mobility, cognition, social and emotional needs)</strong></div><br><p>AI and computing technologies have been changing people’s lives at unprecedented speed. It is important to “get the right design” and “get the design right” when designing AI- and computing-powered systems that humans ultimately use so that everyone can benefit from computing.</p><p>Led by <a href=https://www.mingmingfan.com/ target=_blank rel=noopener>Prof. Mingming Fan</a>, APEX group conducts research at the intersection of Human-Computer Interaction (HCI), AI and VR/AR/MR with three foci:</p><ul><li><p><strong>Assistive Technology (AT):</strong> to empower older adults and people with disabilities</p></li><li><p><strong>Human-AI Collaboration:</strong> to understand human-AI collaboration dynamics via scientific studies and to create novel human-AI collaboration systems.</p></li><li><p><strong>VR/AR/MR:</strong> to create novel VR/AR/MR interaction techniques and applications, for aging, accessibility and learning/education purposes.</p></li></ul><p>Our group’s work has been published at top-tier venues of human-computer interaction (HCI). We have won Best Paper Honorable Mention Awards (4 times) and Best Paper Award from ACM CHI, Best Paper Honorable Mention Awards from UbiComp, Best Artifact Awad from ACM ASSETS, and Best Paper Honorable Mention Awards from Chinese CHI (2 times).</p><img src=./home/QR.jpeg width=50% style=margin-left:auto;margin-right:auto;align:center><div style=display:flex;justify-content:center><div style=margin:20px><ul class=cta-group><li><a href=./people/ class="btn btn-primary px-3 py-3">Meet the team →</a></li></ul></div></div></div></div></div></section><section id=news class="home-section wg-pages"><div class=home-section-bg></div><div class=container><div class="row justify-content-center"><div class="section-heading col-12 mb-3 text-center"><h1 class=mb-0>Latest News</h1></div><div class=col-12><div class="card-simple view-card"><div class=article-metadata><span class=article-date>Jan 14, 2023
</span><span class=middot-divider></span>
<span class=article-reading-time>1 min read</span></div><a href=./post/chi2023/><div class=img-hover-zoom><img src=./post/chi2023/featured_hu795383ad64dc15f176bbc1cdd22b5a51_252461_808x455_fill_q75_h2_lanczos_smart1_3.webp height=455 width=808 class=article-banner alt="Seven full papers accepted to CHI 2023." loading=lazy></div></a><div class="section-subheading article-title mb-1 mt-3"><a href=./post/chi2023/>Seven full papers accepted to CHI 2023.</a></div><a href=./post/chi2023/ class=summary-link><div class=article-style><p><p>Seven full papers accepted to <a href=https://chi2023.acm.org/ target=_blank rel=noopener>ACM CHI 2023</a>, the flagship conference of the field of Human-Computer Interaction (HCI)</p></p></div></a></div></div></div></div></section><section id=featured class="home-section wg-featured"><div class=home-section-bg></div><div class=container><div class=row><div class="section-heading col-12 col-lg-4 mb-3 mb-lg-0 d-flex flex-column align-items-center align-items-lg-start"><h1 class=mb-0>Featured Publications</h1></div><div class="col-12 col-lg-8"><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=./publication/chi25-older-practioner-assessment/>"Watch, Smell, Ask, Touch": Practices, Challenges, and Technological Support in Ability Assessment of Older Adults from Practitioners Perspectives in China</a></div><div class="stream-meta article-metadata"><div class=article-metadata><div><span><a href=./author/zhongyue-zhang/>Zhongyue Zhang</a></span>, <span><a href=./author/yuru-huang/>Yuru Huang</a></span>, <span><a href=./author/mengyang-wang/>Mengyang Wang</a></span>, <span><a href=./author/mingming-fan/>Mingming Fan</a></span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="corresponding author"></i></div><span class=article-date>February 2025
</span><span class=middot-divider></span>
<span class=pub-publication>CHI 2025</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://dl.acm.org/doi/10.1145/3706598.3714166 target=_blank rel=noopener>PDF</a></div></div><div class=ml-3><a href=./publication/chi25-older-practioner-assessment/><img src=./publication/chi25-older-practioner-assessment/featured_hu5ed16c30aa4ff0f852e4ab994890a9fd_38700_150x0_resize_q75_h2_lanczos_3.webp height=77 width=150 alt='	"Watch, Smell, Ask, Touch": Practices, Challenges, and Technological Support in Ability Assessment of Older Adults from Practitioners Perspectives in China' loading=lazy></a></div></div><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=./publication/chi25-acknowledge/>ACKnowledge: A Computational Framework for Human Compatible Affordance-based Interaction Planning in Real-world Contexts</a></div><div class="stream-meta article-metadata"><div class=article-metadata><div><span><a href=./author/ziqi-pan/>Ziqi Pan</a></span>, <span><a href=./author/xiucheng-zhang/>Xiucheng Zhang</a></span>, <span><a href=./author/zisu-li/>Zisu Li</a></span>, <span><a href=./author/zhenhui-peng/>Zhenhui Peng</a></span>, <span><a href=./author/mingming-fan/>Mingming Fan</a></span>, <span><a href=./author/xiaojuan-ma/>Xiaojuan Ma</a></span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="corresponding author"></i></div><span class=article-date>February 2025
</span><span class=middot-divider></span>
<span class=pub-publication>CHI 2025</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://dl.acm.org/doi/10.1145/3706598.3713791 target=_blank rel=noopener>PDF</a></div></div><div class=ml-3><a href=./publication/chi25-acknowledge/><img src=./publication/chi25-acknowledge/featured_hu13cf0121d5fcea2946e09b0b0e74b601_470632_150x0_resize_q75_h2_lanczos_3.webp height=94 width=150 alt="ACKnowledge: A Computational Framework for Human Compatible Affordance-based Interaction Planning in Real-world Contexts" loading=lazy></a></div></div><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=./publication/chi25-multi-agent-reminiscene/>Chorus of the Past: Toward Designing a Multi-agent Conversational Reminiscence System with Digital Artifacts for Older Adults</a></div><div class="stream-meta article-metadata"><div class=article-metadata><div><span><a href=./author/jingwei-sun/>Jingwei Sun</a></span>, <span><a href=./author/zhongyue-zhang/>Zhongyue Zhang</a></span>, <span><a href=./author/mengyang-wang/>Mengyang Wang</a></span>, <span><a href=./author/nianlong-li/>Nianlong Li</a></span>, <span><a href=./author/yan-xiang/>Yan Xiang</a></span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="corresponding author"></i>, <span><a href=./author/liuxin-zhang/>Liuxin Zhang</a></span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="corresponding author"></i>, <span><a href=./author/yu-zhang/>Yu Zhang</a></span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="corresponding author"></i>, <span><a href=./author/qianying-wang/>Qianying Wang</a></span>, <span><a href=./author/mingming-fan/>Mingming Fan</a></span></div><span class=article-date>February 2025
</span><span class=middot-divider></span>
<span class=pub-publication>CHI 2025</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://dl.acm.org/doi/10.1145/3706598.3713810 target=_blank rel=noopener>PDF</a></div></div><div class=ml-3><a href=./publication/chi25-multi-agent-reminiscene/><img src=./publication/chi25-multi-agent-reminiscene/featured_hu912637da4456d2d58d41a36281c4f4df_103414_150x0_resize_q75_h2_lanczos_3.webp height=60 width=150 alt="Chorus of the Past: Toward Designing a Multi-agent Conversational Reminiscence System with Digital Artifacts for Older Adults" loading=lazy></a></div></div><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=./publication/chi25-massage-instruction/>Designing LLM-Powered Multimodal Instructions to Support Rich Hands-on Skills Remote Learning: A Case Study with Massage Instructors and Learners</a></div><div class="stream-meta article-metadata"><div class=article-metadata><div><span><a href=./author/chutian-jiang/>Chutian Jiang</a></span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="equal contribution"></i>, <span><a href=./author/yinan-fan/>Yinan Fan</a></span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="equal contribution"></i>, <span><a href=./author/junan-xie/>Junan Xie</a></span>, <span><a href=./author/emily-kuang/>Emily Kuang</a></span>, <span><a href=./author/baichuan-feng/>Baichuan Feng</a></span>, <span><a href=./author/kaihao-zhang/>Kaihao Zhang</a></span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="corresponding author"></i>, <span><a href=./author/mingming-fan/>Mingming Fan</a></span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="corresponding author"></i></div><span class=article-date>February 2025
</span><span class=middot-divider></span>
<span class=pub-publication>CHI 2025</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://dl.acm.org/doi/full/10.1145/3706598.3713677 target=_blank rel=noopener>PDF</a></div></div><div class=ml-3><a href=./publication/chi25-massage-instruction/><img src=./publication/chi25-massage-instruction/featured_hu255141a1da8e0cb922ab162250ac90b8_62049_150x0_resize_q75_h2_lanczos_3.webp height=46 width=150 alt="Designing LLM-Powered Multimodal Instructions to Support Rich Hands-on Skills Remote Learning: A Case Study with Massage Instructors and Learners" loading=lazy></a></div></div><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=./publication/chi25-flower-arrangement/>Facilitating Daily Practice in Intangible Cultural Heritage through Virtual Reality: A Case Study of Traditional Chinese Flower Arrangement</a></div><div class="stream-meta article-metadata"><div class=article-metadata><div><span><a href=./author/yingna-wang/>Yingna Wang</a></span>, <span><a href=./author/qingqin-liu/>Qingqin Liu</a></span>, <span><a href=./author/xiaoying-wei/>Xiaoying Wei</a></span>, <span><a href=./author/mingming-fan/>Mingming Fan</a></span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="corresponding author"></i></div><span class=article-date>February 2025
</span><span class=middot-divider></span>
<span class=pub-publication>CHI 2025</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/abs/2503.06122 target=_blank rel=noopener>PDF</a></div></div><div class=ml-3><a href=./publication/chi25-flower-arrangement/><img src=./publication/chi25-flower-arrangement/featured_hu26ec7528e83ed42e255b247ad8bc73f5_617940_150x0_resize_q75_h2_lanczos_3.webp height=59 width=150 alt="Facilitating Daily Practice in Intangible Cultural Heritage through Virtual Reality: A Case Study of Traditional Chinese Flower Arrangement" loading=lazy></a></div></div><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=./publication/chi25-interecon/>InteRecon: Towards Reconstructing Interactivity of Personal Memorable Items in Mixed Reality</a></div><div class="stream-meta article-metadata"><div class=article-metadata><div><span><a href=./author/zisu-li/>Zisu Li</a></span>, <span><a href=./author/jiawei-li/>Jiawei Li</a></span>, <span><a href=./author/zeyu-xiong/>Zeyu Xiong</a></span>, <span><a href=./author/shumeng-zhang/>Shumeng Zhang</a></span>, <span><a href=./author/faraz-faruqi/>Faraz Faruqi</a></span>, <span><a href=./author/stefanie-mueller/>Stefanie Mueller</a></span>, <span><a href=./author/chen-liang/>Chen Liang</a></span>, <span><a href=./author/xiaojuan-ma/>Xiaojuan Ma</a></span>, <span><a href=./author/mingming-fan/>Mingming Fan</a></span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="corresponding author"></i></div><span class=article-date>February 2025
</span><span class=middot-divider></span>
<span class=pub-publication>CHI 2025</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/abs/2502.09973 target=_blank rel=noopener>PDF</a></div></div><div class=ml-3><a href=./publication/chi25-interecon/><img src=./publication/chi25-interecon/featured_hue15a0749bc383756d696b1191b69ea43_399053_150x0_resize_q75_h2_lanczos_3.webp height=31 width=150 alt="InteRecon: Towards Reconstructing Interactivity of Personal Memorable Items in Mixed Reality" loading=lazy></a></div></div><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=./publication/chi25-journalalde/>JournalAIde: Empowering Older Adults in Digital Journal Writing</a></div><div class="stream-meta article-metadata"><div class=article-metadata><div><span><a href=./author/shixu-zhou/>Shixu Zhou</a></span>, <span><a href=./author/weiyue-lin/>Weiyue Lin</a></span>, <span><a href=./author/zuyu-xu/>Zuyu Xu</a></span>, <span><a href=./author/xiaoying-wei/>Xiaoying Wei</a></span>, <span><a href=./author/raoyi-huang/>Raoyi Huang</a></span>, <span><a href=./author/xiaojuan-ma/>Xiaojuan Ma</a></span>, <span><a href=./author/mingming-fan/>Mingming Fan</a></span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="corresponding author"></i></div><span class=article-date>February 2025
</span><span class=middot-divider></span>
<span class=pub-publication>CHI 2025</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://dl.acm.org/doi/10.1145/3706598.3713339 target=_blank rel=noopener>PDF</a></div></div><div class=ml-3><a href=./publication/chi25-journalalde/><img src=./publication/chi25-journalalde/featured_hu487ff789adde4aaf7507b29bc9a993df_568033_150x0_resize_q75_h2_lanczos_3.webp height=63 width=150 alt="JournalAIde: Empowering Older Adults in Digital Journal Writing" loading=lazy></a></div></div><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=./publication/chi25-remotechess/>RemoteChess: Enhancing Older Adults’ Social Connectedness via Designing a Virtual Reality Chinese Chess (Xiangqi) Community</a></div><div class="stream-meta article-metadata"><div class=article-metadata><div><span><a href=./author/qianjie-wei/>Qianjie Wei</a></span>, <span><a href=./author/xiaoying-wei/>Xiaoying Wei</a></span>, <span><a href=./author/yiqi-liang/>Yiqi Liang</a></span>, <span><a href=./author/fan-lin/>Fan Lin</a></span>, <span><a href=./author/nuonan-si/>Nuonan Si</a></span>, <span><a href=./author/mingming-fan/>Mingming Fan</a></span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="corresponding author"></i></div><span class=article-date>February 2025
</span><span class=middot-divider></span>
<span class=pub-publication>CHI 2025</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/abs/2502.11627 target=_blank rel=noopener>PDF</a></div></div><div class=ml-3><a href=./publication/chi25-remotechess/><img src=./publication/chi25-remotechess/featured_hu2eb9836bc6ccfbe21b3fa96c072f76eb_149616_150x0_resize_q75_h2_lanczos_3.webp height=39 width=150 alt="RemoteChess: Enhancing Older Adults’ Social Connectedness via Designing a Virtual Reality Chinese Chess (Xiangqi) Community" loading=lazy></a></div></div><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=./publication/chi25-nature-conversation/>Toward Enabling Natural Conversation with Older Adults via the Design of LLM-Powered Voice Agents that Support Interruptions and Backchannels</a></div><div class="stream-meta article-metadata"><div class=article-metadata><div><span><a href=./author/chao-liu/>Chao Liu</a></span>, <span><a href=./author/mingyang-su/>Mingyang Su</a></span>, <span><a href=./author/yan-xiang/>Yan Xiang</a></span>, <span><a href=./author/yuru-huang/>Yuru Huang</a></span>, <span><a href=./author/yiqian-yang/>Yiqian Yang</a></span>, <span><a href=./author/kang-zhang/>Kang Zhang</a></span>, <span><a href=./author/mingming-fan/>Mingming Fan</a></span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="corresponding author"></i></div><span class=article-date>February 2025
</span><span class=middot-divider></span>
<span class=pub-publication>CHI 2025</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://dl.acm.org/doi/full/10.1145/3706598.3714228 target=_blank rel=noopener>PDF</a></div></div><div class=ml-3><a href=./publication/chi25-nature-conversation/><img src=./publication/chi25-nature-conversation/featured_hu415a5d359b417b1a2b2d7327fdde8e65_244684_150x0_resize_q75_h2_lanczos_3.webp height=46 width=150 alt="Toward Enabling Natural Conversation with Older Adults via the Design of LLM-Powered Voice Agents that Support Interruptions and Backchannels" loading=lazy></a></div></div><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=./publication/chi25-ai-story-writing/>Toward Personalizable AI Node Graph Creative Writing Support: Insights on Preferences for Generative AI Features and Information Presentation Across Story Writing Processes</a></div><div class="stream-meta article-metadata"><div class=article-metadata><div><span><a href=./author/hua-xuan-qin/>Hua Xuan Qin</a></span>, <span><a href=./author/guangzhi-zhu/>Guangzhi Zhu</a></span>, <span><a href=./author/mingming-fan/>Mingming Fan</a></span>, <span><a href=./author/pan-hui/>Pan Hui</a></span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="corresponding author"></i></div><span class=article-date>February 2025
</span><span class=middot-divider></span>
<span class=pub-publication>CHI 2025</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://www.mingmingfan.com/papers/CHI25-Writing.pdf target=_blank rel=noopener>PDF</a></div></div><div class=ml-3><a href=./publication/chi25-ai-story-writing/><img src=./publication/chi25-ai-story-writing/featured_hub6addb2505a978145aceacbb2b49ed5d_451176_150x0_resize_q75_h2_lanczos_3.webp height=123 width=150 alt="Toward Personalizable AI Node Graph Creative Writing Support: Insights on Preferences for Generative AI Features and Information Presentation Across Story Writing Processes" loading=lazy></a></div></div><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=./publication/tvcg-2025-focalselect/>FocalSelect: Improving Occluded Objects Acquisition with Heuristic Selection and Disambiguation in Virtual Reality</a></div><div class="stream-meta article-metadata"><div class=article-metadata><div><span><a href=./author/duotun-wang/>Duotun Wang</a></span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="equal contribution"></i>, <span><a href=./author/linjie-qiu/>Linjie Qiu</a></span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="equal contribution"></i>, <span><a href=./author/boyu-li/>Boyu Li</a></span>, <span><a href=./author/qianxi-liu/>Qianxi Liu</a></span>, <span><a href=./author/xiaoying-wei/>Xiaoying Wei</a></span>, <span><a href=./author/jianhao-chen/>Jianhao Chen</a></span>, <span><a href=./author/zeyu-wang/>Zeyu Wang</a></span>, <span><a href=./author/mingming-fan/>Mingming Fan</a></span></div><span class=article-date>January 2025
</span><span class=middot-divider></span>
<span class=pub-publication>TVCG 2025</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://www.mingmingfan.com/papers/TVCG2025__FocalSelect.pdf target=_blank rel=noopener>PDF</a></div></div><div class=ml-3><a href=./publication/tvcg-2025-focalselect/><img src=./publication/tvcg-2025-focalselect/featured_hucb2583ad336e9dff7192e3f4eb78d0e5_8571966_150x0_resize_q75_h2_lanczos_3.webp height=96 width=150 alt="FocalSelect: Improving Occluded Objects Acquisition with Heuristic Selection and Disambiguation in Virtual Reality" loading=lazy></a></div></div><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=./publication/tvcg-2025-teamportal/>TeamPortal: Exploring Virtual Reality Collaboration Through Shared and Manipulating Parallel Views</a></div><div class="stream-meta article-metadata"><div class=article-metadata><div><span><a href=./author/xian-wang/>Xian Wang</a></span>, <span><a href=./author/luyao-shen/>Luyao Shen</a></span>, <span><a href=./author/lei-chen/>Lei Chen</a></span>, <span><a href=./author/mingming-fan/>Mingming Fan</a></span>, <span><a href=./author/lik-hang-lee/>Lik-Hang Lee</a></span></div><span class=article-date>January 2025
</span><span class=middot-divider></span>
<span class=pub-publication>TVCG 2025</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/abs/2501.17768 target=_blank rel=noopener>PDF</a></div></div><div class=ml-3><a href=./publication/tvcg-2025-teamportal/><img src=./publication/tvcg-2025-teamportal/featured_hu18b2b57ba2fe3be2bd1f3f08619b4317_371462_150x0_resize_q75_h2_lanczos_3.webp height=64 width=150 alt="TeamPortal: Exploring Virtual Reality Collaboration Through Shared and Manipulating Parallel Views" loading=lazy></a></div></div><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=./publication/cscw25-ai-support-igc/>AI as a Bridge Across Ages: Exploring The Opportunities of Artificial Intelligence in Supporting Inter-Generational Communication in Virtual Reality</a></div><div class="stream-meta article-metadata"><div class=article-metadata><div><span><a href=./author/qiuxin-du/>Qiuxin Du</a></span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="equal contribution"></i>, <span><a href=./author/xiaoying-wei/>Xiaoying Wei</a></span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="equal contribution"></i>, <span><a href=./author/jiawei-li/>Jiawei Li</a></span>, <span><a href=./author/emily-kuang/>Emily Kuang</a></span>, <span><a href=./author/jie-hao/>Jie Hao</a></span>, <span><a href=./author/dongdong-weng/>Dongdong Weng</a></span>, <span><a href=./author/mingming-fan/>Mingming Fan</a></span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="corresponding author"></i></div><span class=article-date>January 2025
</span><span class=middot-divider></span>
<span class=pub-publication>CSCW 2025</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/abs/2410.17909 target=_blank rel=noopener>PDF</a></div></div><div class=ml-3><a href=./publication/cscw25-ai-support-igc/><img src=./publication/cscw25-ai-support-igc/featured_huc8e6da1d9191aac5a501827c368037ac_5877186_150x0_resize_q75_h2_lanczos_3.webp height=73 width=150 alt="AI as a Bridge Across Ages: Exploring The Opportunities of Artificial Intelligence in Supporting Inter-Generational Communication in Virtual Reality" loading=lazy></a></div></div><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=./publication/cscw25-companion-robot/>Challenges in Adopting Companion Robots: An Exploratory Study of Robotic Companionship Conducted with Chinese Retirees</a></div><div class="stream-meta article-metadata"><div class=article-metadata><div><span><a href=./author/mengyang-wang/>Mengyang Wang</a></span>, <span><a href=./author/keye-yu/>Keye Yu</a></span>, <span><a href=./author/yukai-zhang/>Yukai Zhang</a></span>, <span><a href=./author/mingming-fan/>Mingming Fan</a></span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="corresponding author"></i></div><span class=article-date>January 2025
</span><span class=middot-divider></span>
<span class=pub-publication>CSCW 2025</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/abs/2410.12205 target=_blank rel=noopener>PDF</a></div></div><div class=ml-3><a href=./publication/cscw25-companion-robot/><img src=./publication/cscw25-companion-robot/featured_hu17d647f4f7ce14cc9049d40092d26eb6_2807329_150x0_resize_q75_h2_lanczos_3.webp height=68 width=150 alt="Challenges in Adopting Companion Robots: An Exploratory Study of Robotic Companionship Conducted with Chinese Retirees" loading=lazy></a></div></div><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=./publication/cscw25-dhh-dating/>Practices and Challenges of Online Love-seeking Among Deaf or Hard of Hearing People: A Case Study in China</a></div><div class="stream-meta article-metadata"><div class=article-metadata><div><span><a href=./author/beiyan-cao/>Beiyan Cao</a></span>, <span><a href=./author/jingling-zhang/>Jingling Zhang</a></span>, <span><a href=./author/changyang-he/>Changyang He</a></span>, <span><a href=./author/yuru-huang/>Yuru Huang</a></span>, <span><a href=./author/muzhi-zhou/>Muzhi Zhou</a></span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="corresponding author"></i>, <span><a href=./author/mingming-fan/>Mingming Fan</a></span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="corresponding author"></i></div><span class=article-date>January 2025
</span><span class=middot-divider></span>
<span class=pub-publication>CSCW 2025</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/pdf/2410.11810 target=_blank rel=noopener>PDF</a></div></div><div class=ml-3><a href=./publication/cscw25-dhh-dating/><img src=./publication/cscw25-dhh-dating/featured_hu3814a636fd5a4a1322f09dfd560d00ef_917571_150x0_resize_q75_h2_lanczos_3.webp height=107 width=150 alt="Practices and Challenges of Online Love-seeking Among Deaf or Hard of Hearing People: A Case Study in China" loading=lazy></a></div></div><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=./publication/cscw25-social-vr-review/>Systematic Literature Review of Using Virtual Reality as a Social Platform in HCI Community</a></div><div class="stream-meta article-metadata"><div class=article-metadata><div><span><a href=./author/xiaoying-wei/>Xiaoying Wei</a></span>, <span><a href=./author/xiaofu-jin/>Xiaofu Jin</a></span>, <span><a href=./author/ge-lin-kan/>Ge Lin Kan</a></span>, <span><a href=./author/yukang-yan/>Yukang Yan</a></span>, <span><a href=./author/mingming-fan/>Mingming Fan</a></span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="corresponding author"></i></div><span class=article-date>January 2025
</span><span class=middot-divider></span>
<span class=pub-publication>CSCW 2025</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/abs/2410.11869 target=_blank rel=noopener>PDF</a></div></div><div class=ml-3><a href=./publication/cscw25-social-vr-review/><img src=./publication/cscw25-social-vr-review/featured_hudbb6d5986a9881859ee67cfa0d0be8c3_177341_150x0_resize_q75_h2_lanczos_3.webp height=48 width=150 alt="Systematic Literature Review of Using Virtual Reality as a Social Platform in HCI Community" loading=lazy></a></div></div><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=./publication/cscw25-photo-reminiscene/>Understanding and Co-designing Photo-based Reminiscence with Older Adults</a></div><div class="stream-meta article-metadata"><div class=article-metadata><div><span><a href=./author/zhongyue-zhang/>Zhongyue Zhang</a></span>, <span><a href=./author/lina-xu/>Lina Xu</a></span>, <span><a href=./author/xingkai-wang/>Xingkai Wang</a></span>, <span><a href=./author/xu-zhang/>Xu Zhang</a></span>, <span><a href=./author/mingming-fan/>Mingming Fan</a></span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="corresponding author"></i></div><span class=article-date>January 2025
</span><span class=middot-divider></span>
<span class=pub-publication>CSCW 2025</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/abs/2411.00351 target=_blank rel=noopener>PDF</a></div></div><div class=ml-3><a href=./publication/cscw25-photo-reminiscene/><img src=./publication/cscw25-photo-reminiscene/featured_hua281b1c8831e7ecda14ba88bf9ba0f1b_3040778_150x0_resize_q75_h2_lanczos_3.webp height=113 width=150 alt="Understanding and Co-designing Photo-based Reminiscence with Older Adults" loading=lazy></a></div></div><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=./publication/vrst24-searchvr/>Toward Facilitating Search in VR With the Assistance of Vision Large Language Models</a></div><div class="stream-meta article-metadata"><div class=article-metadata><div><span><a href=./author/chao-liu/>Chao Liu</a></span>, <span><a href=./author/clarence-chi-san-cheung/>Clarence Chi San Cheung</a></span>, <span><a href=./author/mingqing-xu/>Mingqing Xu</a></span>, <span><a href=./author/zhongyue-zhang/>Zhongyue Zhang</a></span>, <span><a href=./author/mingyang-su/>Mingyang Su</a></span>, <span><a href=./author/mingming-fan/>Mingming Fan</a></span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Corresponding author"></i></div><span class=article-date>October 2024
</span><span class=middot-divider></span>
<span class=pub-publication>VRST 2024</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://www.mingmingfan.com/papers/VRST24_VR_Search_Framework.pdf target=_blank rel=noopener>PDF</a></div></div><div class=ml-3><a href=./publication/vrst24-searchvr/><img src=./publication/vrst24-searchvr/featured_hued1f0784c4d70f09a8b0515d1adb6bbd_731792_150x0_resize_q75_h2_lanczos_3.webp height=45 width=150 alt="Toward Facilitating Search in VR With the Assistance of Vision Large Language Models" loading=lazy></a></div></div><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=./publication/vinci24-augmentedlibrary/>Augmented Library: Toward Enriching Physical Library Experience Using HMD-Based Augmented Reality</a></div><div class="stream-meta article-metadata"><div class=article-metadata><div><span><a href=./author/qianjie-wei/>Qianjie Wei</a></span>, <span><a href=./author/jingling-zhang/>Jingling Zhang</a></span>, <span><a href=./author/pengqi-wang/>Pengqi Wang</a></span>, <span><a href=./author/xiaofu-jin/>Xiaofu Jin</a></span>, <span><a href=./author/mingming-fan/>Mingming Fan</a></span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Corresponding author"></i></div><span class=article-date>October 2024
</span><span class=middot-divider></span>
<span class=pub-publication>VINCI 2024</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://www.mingmingfan.com/papers/VINCI24_AR_Library.pdf target=_blank rel=noopener>PDF</a></div></div><div class=ml-3><a href=./publication/vinci24-augmentedlibrary/><img src=./publication/vinci24-augmentedlibrary/featured_hub4d4f9a8cd5861387cb54724d5924f9a_21946608_150x0_resize_q75_h2_lanczos_3.webp height=67 width=150 alt="Augmented Library: Toward Enriching Physical Library Experience Using HMD-Based Augmented Reality" loading=lazy></a></div></div><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=./publication/cscw24-harassers-vr/>Avatar Appearance and Behavior of Potential Harassers Affect Users' Perceptions and Response Strategies in Social Virtual Reality (VR): A Mixed-Methods Study</a></div><div class="stream-meta article-metadata"><div class=article-metadata><div><span><a href=./author/xuetong-wang/>Xuetong Wang</a></span>, <span><a href=./author/ziyan-wang/>Ziyan Wang</a></span>, <span><a href=./author/mingmin-zhang/>Mingmin Zhang</a></span>, <span><a href=./author/kangyou-yu/>Kangyou Yu</a></span>, <span><a href=./author/pan-hui/>Pan Hui</a></span>, <span><a href=./author/mingming-fan/>Mingming Fan</a></span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="corresponding author"></i></div><span class=article-date>October 2024
</span><span class=middot-divider></span>
<span class=pub-publication>CSCW 2024</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://www.mingmingfan.com/papers/CSCW23_VR_Harassment.pdf target=_blank rel=noopener>PDF</a></div></div><div class=ml-3><a href=./publication/cscw24-harassers-vr/><img src=./publication/cscw24-harassers-vr/featured_hu64187ace12e4cb66ed5f2574ab9b2563_443632_150x0_resize_q75_h2_lanczos.webp height=36 width=150 alt="Avatar Appearance and Behavior of Potential Harassers Affect Users' Perceptions and Response Strategies in Social Virtual Reality (VR): A Mixed-Methods Study" loading=lazy></a></div></div><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=./publication/vinci24-hapticperception/>Investigating Size Congruency Between the Visual Perception of a VR Object and the Haptic Perception of Its Physical World Agent</a></div><div class="stream-meta article-metadata"><div class=article-metadata><div><span><a href=./author/wenqi-zheng/>Wenqi Zheng</a></span>, <span><a href=./author/dawei-xiong/>Dawei Xiong</a></span>, <span><a href=./author/cekai-weng/>Cekai Weng</a></span>, <span><a href=./author/jiajun-jiang/>Jiajun Jiang</a></span>, <span><a href=./author/junwei-li/>Junwei Li</a></span>, <span><a href=./author/jinni-zhou/>Jinni ZHOU</a></span>, <span><a href=./author/mingming-fan/>Mingming Fan</a></span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Corresponding author"></i></div><span class=article-date>October 2024
</span><span class=middot-divider></span>
<span class=pub-publication>VINCI 2024</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://www.mingmingfan.com/papers/VINCI24_VR_Size_Congruency.pdf target=_blank rel=noopener>PDF</a></div></div><div class=ml-3><a href=./publication/vinci24-hapticperception/><img src=./publication/vinci24-hapticperception/featured_hu8922a23f94c5489d3b323499bc50bc7a_343069_150x0_resize_q75_h2_lanczos_3.webp height=63 width=150 alt="Investigating Size Congruency Between the Visual Perception of a VR Object and the Haptic Perception of Its Physical World Agent" loading=lazy></a></div></div><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=./publication/imwut2024-silvercycling/>SilverCycling: Exploring the Impact of Bike-Based Locomotion on Spatial Orientation for Older Adults in VR</a></div><div class="stream-meta article-metadata"><div class=article-metadata><div><span><a href=./author/qiongyan-chen/>Qiongyan Chen</a></span>, <span><a href=./author/zhiqing-wu/>Zhiqing Wu</a></span>, <span><a href=./author/yucheng-liu/>Yucheng Liu</a></span>, <span><a href=./author/lei-han/>Lei Han</a></span>, <span><a href=./author/zisu-li/>Zisu Li</a></span>, <span><a href=./author/ge-lin-kan/>Ge Lin Kan</a></span>, <span><a href=./author/mingming-fan/>Mingming Fan</a></span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Corresponding author"></i></div><span class=article-date>August 2024
</span><span class=middot-divider></span>
<span class=pub-publication>IMWUT 2024</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://www.mingmingfan.com/papers/IMWUT24-SilverCycling.pdf target=_blank rel=noopener>PDF</a></div></div><div class=ml-3><a href=./publication/imwut2024-silvercycling/><img src=./publication/imwut2024-silvercycling/featured_hu39d4ac45cd0ff5474a1b54b503611ade_2510115_150x0_resize_q75_h2_lanczos_3.webp height=82 width=150 alt="SilverCycling: Exploring the Impact of Bike-Based Locomotion on Spatial Orientation for Older Adults in VR" loading=lazy></a></div></div><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=./publication/assets24-beadwork/>Beadwork Bridge: Understanding and Exploring the Opportunities of Beadwork in Enriching School Education for Blind and Low Vision (BLV) People</a></div><div class="stream-meta article-metadata"><div class=article-metadata><div><span><a href=./author/shumeng-zhang/>Shumeng Zhang</a></span>, <span><a href=./author/weiyue-lin/>Weiyue Lin</a></span>, <span><a href=./author/zisu-li/>Zisu Li</a></span>, <span><a href=./author/ruiqi-jiang/>Ruiqi Jiang</a></span>, <span><a href=./author/chen-liang/>Chen Liang</a></span>, <span><a href=./author/mingming-fan/>Mingming Fan</a></span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Corresponding author"></i>, <span><a href=./author/raul-masu/>Raul Masu</a></span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Corresponding author"></i></div><span class=article-date>July 2024
</span><span class=middot-divider></span>
<span class=pub-publication>ASSETS 2024</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://www.mingmingfan.com/papers/assets24-beadwork.pdf target=_blank rel=noopener>PDF</a></div></div><div class=ml-3><a href=./publication/assets24-beadwork/><img src=./publication/assets24-beadwork/featured_hu56ac9b6b87e6b31ad07bd0505b202863_430306_150x0_resize_q75_h2_lanczos_3.webp height=82 width=150 alt="Beadwork Bridge: Understanding and Exploring the Opportunities of Beadwork in Enriching School Education for Blind and Low Vision (BLV) People" loading=lazy></a></div></div><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=./publication/chi24-opera-makeup/>“It is hard to remove from my eye': Design Makeup Residue Visualization System for Chinese Traditional Opera (Xiqu) Performers</a></div><div class="stream-meta article-metadata"><div class=article-metadata><div><span><a href=./author/zeyu-xiong/>Zeyu Xiong</a></span>, <span><a href=./author/shihan-fu/>Shihan Fu</a></span>, <span><a href=./author/yanying-zhu/>Yanying Zhu</a></span>, <span><a href=./author/chenqing-zhu/>Chenqing Zhu</a></span>, <span><a href=./author/xiaojuan-ma/>Xiaojuan Ma</a></span>, <span><a href=./author/mingming-fan/>Mingming Fan</a></span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="corresponding author"></i></div><span class=article-date>February 2024
</span><span class=middot-divider></span>
<span class=pub-publication>CHI 2024</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://www.mingmingfan.com/papers/CHI24-Xiqu.pdf target=_blank rel=noopener>PDF</a></div></div><div class=ml-3><a href=./publication/chi24-opera-makeup/><img src=./publication/chi24-opera-makeup/featured_huf855a0950fd841816950f7b1156b7139_150886_150x0_resize_q75_h2_lanczos.webp height=48 width=150 alt="“It is hard to remove from my eye': Design Makeup Residue Visualization System for Chinese Traditional Opera (Xiqu) Performers" loading=lazy></a></div></div><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=./publication/chi24-literacy-gap/>Bridging the Literacy Gap for Adults: Streaming and Engaging in Adult Literacy Education through Livestreaming</a></div><div class="stream-meta article-metadata"><div class=article-metadata><div><span><a href=./author/shihan-fu/>Shihan Fu</a></span>, <span><a href=./author/jianhao-chen/>Jianhao Chen</a></span>, <span><a href=./author/emily-kuang/>Emily Kuang</a></span>, <span><a href=./author/mingming-fan/>Mingming Fan</a></span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="corresponding author"></i></div><span class=article-date>February 2024
</span><span class=middot-divider></span>
<span class=pub-publication>CHI 2024</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://www.mingmingfan.com/papers/CHI24_Adult_Literacy_Livestreaming.pdf target=_blank rel=noopener>PDF</a></div></div><div class=ml-3><a href=./publication/chi24-literacy-gap/><img src=./publication/chi24-literacy-gap/featured_hue6c3b3b5b5e1c558972989bf804f34f2_201530_150x0_resize_q75_h2_lanczos.webp height=70 width=150 alt="Bridging the Literacy Gap for Adults: Streaming and Engaging in Adult Literacy Education through Livestreaming" loading=lazy></a></div></div><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=./publication/chi24-coprompt/>CoPrompt: Supporting Prompt Sharing and Referring in Collaborative Natural Language Programming</a></div><div class="stream-meta article-metadata"><div class=article-metadata><div><span><a href=./author/li-feng/>Li Feng</a></span>, <span><a href=./author/ryan-yen/>Ryan Yen</a></span>, <span><a href=./author/yuzhe-you/>Yuzhe You</a></span>, <span><a href=./author/mingming-fan/>Mingming Fan</a></span>, <span><a href=./author/jian-zhao/>Jian Zhao</a></span>, <span><a href=./author/zhicong-lu/>Zhicong Lu</a></span></div><span class=article-date>February 2024
</span><span class=middot-divider></span>
<span class=pub-publication>CHI 2024</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://www.mingmingfan.com/papers/CHI24_CoPrompt.pdf target=_blank rel=noopener>PDF</a></div></div><div class=ml-3><a href=./publication/chi24-coprompt/><img src=./publication/chi24-coprompt/featured_hua6ea0d395e743e68c26607ee0389556e_162913_150x0_resize_q75_h2_lanczos.webp height=76 width=150 alt="CoPrompt: Supporting Prompt Sharing and Referring in Collaborative Natural Language Programming" loading=lazy></a></div></div><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=./publication/chi24-blv-electro-chart/>Designing Unobtrusive Modulated Electrotactile Feedback on Fingertip Edge to Assist Blind and Low Vision (BLV) People in Comprehending Charts</a></div><div class="stream-meta article-metadata"><div class=article-metadata><div><span><a href=./author/chutian-jiang/>Chutian Jiang</a></span>, <span><a href=./author/yinan-fan/>Yinan Fan</a></span>, <span><a href=./author/junan-xie/>Junan Xie</a></span>, <span><a href=./author/emily-kuang/>Emily Kuang</a></span>, <span><a href=./author/kaihao-zhang/>Kaihao Zhang</a></span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="corresponding author"></i>, <span><a href=./author/mingming-fan/>Mingming Fan</a></span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="corresponding author"></i></div><span class=article-date>February 2024
</span><span class=middot-divider></span>
<span class=pub-publication>CHI 2024</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://www.mingmingfan.com/papers/CHI24_Haptic_Guidance_InfoGraphics_PVI.pdf target=_blank rel=noopener>PDF</a></div></div><div class=ml-3><a href=./publication/chi24-blv-electro-chart/><img src=./publication/chi24-blv-electro-chart/featured_hu5c08d43769c0ad9065c45998c18a4b22_130781_150x0_resize_q75_h2_lanczos.webp height=122 width=150 alt="Designing Unobtrusive Modulated Electrotactile Feedback on Fingertip Edge to Assist Blind and Low Vision (BLV) People in Comprehending Charts" loading=lazy></a></div></div><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=./publication/chi24-vr-limb-disability/>Designing Upper-Body Gesture Interaction with and for People with Spinal Muscular Atrophy in VR</a></div><div class="stream-meta article-metadata"><div class=article-metadata><div><span><a href=./author/jingze-tian/>Jingze Tian</a></span>, <span><a href=./author/yingna-wang/>Yingna Wang</a></span>, <span><a href=./author/keye-yu/>Keye Yu</a></span>, <span><a href=./author/liyi-xu/>Liyi Xu</a></span>, <span><a href=./author/junan-xie/>Junan Xie</a></span>, <span><a href=./author/franklin-mingzhe-li/>Franklin Mingzhe Li</a></span>, <span><a href=./author/yafeng-niu/>Yafeng Niu</a></span>, <span><a href=./author/mingming-fan/>Mingming Fan</a></span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="corresponding author"></i></div><span class=article-date>February 2024
</span><span class=middot-divider></span>
<span class=pub-publication>CHI 2024</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://mingmingfan.com/papers/CHI24-user-defined-gesture-VR-people-atrophy.pdf target=_blank rel=noopener>PDF</a></div></div><div class=ml-3><a href=./publication/chi24-vr-limb-disability/><img src=./publication/chi24-vr-limb-disability/featured_huc8081818376a797acc1de41bd5165dd4_166815_150x0_resize_q75_h2_lanczos.webp height=88 width=150 alt="Designing Upper-Body Gesture Interaction with and for People with Spinal Muscular Atrophy in VR" loading=lazy></a></div></div><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=./publication/chi24-ux-collaboration-ai/>Enhancing UX Evaluation Through Collaboration with Conversational AI Assistants: Effects of Proactive Dialogue and Timing</a></div><div class="stream-meta article-metadata"><div class=article-metadata><div><span><a href=./author/emily-kuang/>Emily Kuang</a></span>, <span><a href=./author/minghao-li/>Minghao Li</a></span>, <span><a href=./author/mingming-fan/>Mingming Fan</a></span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="corresponding author"></i>, <span><a href=./author/kristen-shinohara/>Kristen Shinohara</a></span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="corresponding author"></i></div><span class=article-date>February 2024
</span><span class=middot-divider></span>
<span class=pub-publication>CHI 2024</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://www.mingmingfan.com/papers/CHI24_UX_Proactive_Assistant.pdf target=_blank rel=noopener>PDF</a></div></div><div class=ml-3><a href=./publication/chi24-ux-collaboration-ai/><img src=./publication/chi24-ux-collaboration-ai/featured_hud09f17e46f2267d5311c704ffe1b6f25_146365_150x0_resize_q75_h2_lanczos.webp height=58 width=150 alt="Enhancing UX Evaluation Through Collaboration with Conversational AI Assistants: Effects of Proactive Dialogue and Timing" loading=lazy></a></div></div><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=./publication/chi24-fetchaid/>FetchAid: Making Parcel Lockers More Accessible to Blind and Low Vision People With Deep-learning Enhanced Touchscreen Guidance, Error-Recovery Mechanism, and AR-based Search Support</a></div><div class="stream-meta article-metadata"><div class=article-metadata><div><span><a href=./author/klara-zhitong-guan/>Klara Zhitong Guan</a></span>, <span><a href=./author/zeyu-xiong/>Zeyu Xiong</a></span>, <span><a href=./author/mingming-fan/>Mingming Fan</a></span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="corresponding author"></i></div><span class=article-date>February 2024
</span><span class=middot-divider></span>
<span class=pub-publication>CHI 2024</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://www.mingmingfan.com/papers/CHI24-FetchAid.pdf target=_blank rel=noopener>PDF</a></div></div><div class=ml-3><a href=./publication/chi24-fetchaid/><img src=./publication/chi24-fetchaid/featured_hu36747f81c2e76bcacf6bc010ab2e155a_178351_150x0_resize_q75_h2_lanczos.webp height=92 width=150 alt="FetchAid: Making Parcel Lockers More Accessible to Blind and Low Vision People With Deep-learning Enhanced Touchscreen Guidance, Error-Recovery Mechanism, and AR-based Search Support" loading=lazy></a></div></div><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=./publication/chi24-lightsword/>LightSword: A Customized Virtual Reality Exergame for Long-Term Cognitive Inhibition Training in Older Adults</a></div><div class="stream-meta article-metadata"><div class=article-metadata><div><span><a href=./author/qiuxin-du/>Qiuxin Du</a></span>, <span><a href=./author/zhen-song/>Zhen Song</a></span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="corresponding author"></i>, <span><a href=./author/haiyan-jiang/>Haiyan Jiang</a></span>, <span><a href=./author/xiaoying-wei/>Xiaoying Wei</a></span>, <span><a href=./author/dongdong-weng/>Dongdong Weng</a></span>, <span><a href=./author/mingming-fan/>Mingming Fan</a></span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="corresponding author"></i></div><span class=article-date>February 2024
</span><span class=middot-divider></span>
<span class=pub-publication>CHI 2024</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://mingmingfan.com/papers/CHI_2024__Cognitive_Training.pdf target=_blank rel=noopener>PDF</a></div></div><div class=ml-3></div></div><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=./publication/chi24-vr-rehabilitation-motivation/>To Reach the Unreachable: Exploring the Potential of VR Hand Redirection for Upper Limb Rehabilitation</a></div><div class="stream-meta article-metadata"><div class=article-metadata><div><span><a href=./author/peixuan-xiong/>Peixuan Xiong</a></span>, <span><a href=./author/yukai-zhang/>Yukai Zhang</a></span>, <span><a href=./author/nandi-zhang/>Nandi Zhang</a></span>, <span><a href=./author/shihan-fu/>Shihan Fu</a></span>, <span><a href=./author/xin-li/>Xin Li</a></span>, <span><a href=./author/yadan-zheng/>Yadan Zheng</a></span>, <span><a href=./author/jinni-zhou/>Jinni ZHOU</a></span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="corresponding author"></i>, <span><a href=./author/xiquan-hu/>Xiquan Hu</a></span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="corresponding author"></i>, <span><a href=./author/mingming-fan/>Mingming Fan</a></span></div><span class=article-date>February 2024
</span><span class=middot-divider></span>
<span class=pub-publication>CHI 2024</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://www.mingmingfan.com/papers/CHI24_VR_Visual_Offset_Stoke_Patiences_Rehabilitation.pdf target=_blank rel=noopener>PDF</a></div></div><div class=ml-3><a href=./publication/chi24-vr-rehabilitation-motivation/><img src=./publication/chi24-vr-rehabilitation-motivation/featured_hu89a6c222d965a847645213585ccdb82c_222591_150x0_resize_q75_h2_lanczos.webp height=103 width=150 alt="To Reach the Unreachable: Exploring the Potential of VR Hand Redirection for Upper Limb Rehabilitation" loading=lazy></a></div></div><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=./publication/chi24-older-vr/>Toward Making Virtual Reality (VR) More Inclusive for Older Adults: Investigating Aging Effects on Object Selection and Manipulation in VR</a></div><div class="stream-meta article-metadata"><div class=article-metadata><div><span><a href=./author/zhiqing-wu/>Zhiqing Wu</a></span>, <span><a href=./author/duotun-wang/>Duotun Wang</a></span>, <span><a href=./author/shumeng-zhang/>Shumeng Zhang</a></span>, <span><a href=./author/yuru-huang/>Yuru Huang</a></span>, <span><a href=./author/zeyu-wang/>Zeyu Wang</a></span>, <span><a href=./author/mingming-fan/>Mingming Fan</a></span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="corresponding author"></i></div><span class=article-date>February 2024
</span><span class=middot-divider></span>
<span class=pub-publication>CHI 2024</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://www.mingmingfan.com/papers/CHI24_OlderAdults_VR_Accessibility_Modeling.pdf target=_blank rel=noopener>PDF</a></div></div><div class=ml-3><a href=./publication/chi24-older-vr/><img src=./publication/chi24-older-vr/featured_hud39d9f4125ca11b3e9bcc77a80042ef7_1435094_150x0_resize_q75_h2_lanczos.webp height=104 width=150 alt="Toward Making Virtual Reality (VR) More Inclusive for Older Adults: Investigating Aging Effects on Object Selection and Manipulation in VR" loading=lazy></a></div></div><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=./publication/imwut-ubicomp2023-ar-photo-storytelling/>Exploring The Opportunities of AR for Enriching Storytelling With Family Photos Among Grandparents and Grandchildren</a></div><div class="stream-meta article-metadata"><div class=article-metadata><div><span><a href=./author/zisu-li/>Zisu Li</a></span>, <span><a href=./author/li-feng/>Li Feng</a></span>, <span><a href=./author/chen-liang/>Chen Liang</a></span>, <span><a href=./author/yuru-huang/>Yuru Huang</a></span>, <span><a href=./author/mingming-fan/>Mingming Fan</a></span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Corresponding author"></i></div><span class=article-date>September 2023
</span><span class=middot-divider></span>
<span class=pub-publication>IMWUT 2023</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://dl.acm.org/doi/pdf/10.1145/3610903 target=_blank rel=noopener>PDF</a></div></div><div class=ml-3><a href=./publication/imwut-ubicomp2023-ar-photo-storytelling/><img src=./publication/imwut-ubicomp2023-ar-photo-storytelling/featured_hu2735f2f9596fa664256865ed2b5e6d79_2697338_150x0_resize_q75_h2_lanczos_3.webp height=65 width=150 alt="Exploring The Opportunities of AR for Enriching Storytelling With Family Photos Among Grandparents and Grandchildren" loading=lazy></a></div></div><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=./publication/assets23-dda/>Understanding Strategies and Challenges of Conducting Daily Data Analysis (DDA) Among Blind and Low Vision People</a></div><div class="stream-meta article-metadata"><div class=article-metadata><div><span><a href=./author/chutian-jiang/>Chutian Jiang</a></span>, <span><a href=./author/wentao-lei/>Wentao Lei</a></span>, <span><a href=./author/emily-kuang/>Emily Kuang</a></span>, <span><a href=./author/teng-han/>Teng Han</a></span>, <span><a href=./author/mingming-fan/>Mingming Fan</a></span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Corresponding author"></i></div><span class=article-date>September 2023
</span><span class=middot-divider></span>
<span class=pub-publication>ASSETS 2023</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://www.mingmingfan.com/papers/ASSETS23_PVI_Data_exploratory_analysis.pdf target=_blank rel=noopener>PDF</a></div></div><div class=ml-3><a href=./publication/assets23-dda/><img src=./publication/assets23-dda/featured_hu8186dc08cbaf644e8bdc5d38d714f1c9_1585049_150x0_resize_q75_h2_lanczos_3.webp height=96 width=150 alt="Understanding Strategies and Challenges of Conducting Daily Data Analysis (DDA) Among Blind and Low Vision People" loading=lazy></a></div></div><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=./publication/assets23-curators-museum-accessibility/>Understanding Curators' Practices and Challenges of Making Exhibitions More Accessible for Blind and Low Vision People</a></div><div class="stream-meta article-metadata"><div class=article-metadata><div><span><a href=./author/yuru-huang/>Yuru Huang</a></span>, <span><a href=./author/jingling-zhang/>Jingling Zhang</a></span>, <span><a href=./author/xiaofu-jin/>Xiaofu Jin</a></span>, <span><a href=./author/mingming-fan/>Mingming Fan</a></span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Corresponding author"></i></div><span class=article-date>September 2023
</span><span class=middot-divider></span>
<span class=pub-publication>ASSETS 2023</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://www.mingmingfan.com/papers/ASSETS23_Museum_Accessibility.pdf target=_blank rel=noopener>PDF</a></div></div><div class=ml-3><a href=./publication/assets23-curators-museum-accessibility/><img src=./publication/assets23-curators-museum-accessibility/featured_hu6e34eae41830e59d5061139171c22c78_571883_150x0_resize_q75_h2_lanczos_3.webp height=53 width=150 alt="Understanding Curators' Practices and Challenges of Making Exhibitions More Accessible for Blind and Low Vision People" loading=lazy></a></div></div><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=./publication/vinci-2023-odor/>OdorV-Art: An Initial Exploration of An Olfactory Intervention for Appreciating Style Information of Artworks in Virtual Museum</a></div><div class="stream-meta article-metadata"><div class=article-metadata><div><span><a href=./author/shumeng-zhang/>Shumeng Zhang</a></span>, <span><a href=./author/ziyan-wang/>Ziyan Wang</a></span>, <span><a href=./author/you-zhou/>You Zhou</a></span>, <span><a href=./author/hao-cui/>Hao Cui</a></span>, <span><a href=./author/shihan-fu/>Shihan Fu</a></span>, <span><a href=./author/zeyu-wang/>Zeyu Wang</a></span>, <span><a href=./author/mingming-fan/>Mingming Fan</a></span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Corresponding author"></i></div><span class=article-date>August 2023
</span><span class=middot-divider></span>
<span class=pub-publication>VINCI’23</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://www.mingmingfan.com/papers/OdorV-Art-VINCI.pdf target=_blank rel=noopener>PDF</a></div></div><div class=ml-3><a href=./publication/vinci-2023-odor/><img src=./publication/vinci-2023-odor/featured_hu2f1e02e515e351c311ec896043222feb_129691_150x0_resize_q75_h2_lanczos.webp height=40 width=150 alt="OdorV-Art: An Initial Exploration of An Olfactory Intervention for Appreciating Style Information of Artworks in Virtual Museum" loading=lazy></a></div></div><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=./publication/taccess-haptic-blv-review/>uxSense: Supporting User Experience Analysis with Visualization and Computer Vision</a></div><div class="stream-meta article-metadata"><div class=article-metadata><div><span><a href=./author/andrea-batch/>Andrea Batch</a></span>, <span><a href=./author/yipeng-ji/>Yipeng Ji</a></span>, <span><a href=./author/mingming-fan/>Mingming Fan</a></span>, <span><a href=./author/jian-zhao/>Jian Zhao</a></span>, <span><a href=./author/niklas-elmqvist/>Niklas Elmqvist</a></span></div><span class=article-date>March 2023
</span><span class=middot-divider></span>
<span class=pub-publication>TVCG 2023</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://www.mingmingfan.com/papers/TVCG_uxSense.pdf target=_blank rel=noopener>PDF
</a><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/10.1109/TVCG.2023.3241581 target=_blank rel=noopener>DOI</a></div></div><div class=ml-3><a href=./publication/taccess-haptic-blv-review/><img src=./publication/taccess-haptic-blv-review/featured_hu72836e3ad933df1470daae31ec358a14_12351054_150x0_resize_q75_h2_lanczos_3.webp height=123 width=150 alt="uxSense: Supporting User Experience Analysis with Visualization and Computer Vision" loading=lazy></a></div></div><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=./publication/tvcg-2023-uxsense/>uxSense: Supporting User Experience Analysis with Visualization and Computer Vision</a></div><div class="stream-meta article-metadata"><div class=article-metadata><div><span><a href=./author/andrea-batch/>Andrea Batch</a></span>, <span><a href=./author/yipeng-ji/>Yipeng Ji</a></span>, <span><a href=./author/mingming-fan/>Mingming Fan</a></span>, <span><a href=./author/jian-zhao/>Jian Zhao</a></span>, <span><a href=./author/niklas-elmqvist/>Niklas Elmqvist</a></span></div><span class=article-date>March 2023
</span><span class=middot-divider></span>
<span class=pub-publication>TVCG 2023</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://www.mingmingfan.com/papers/TVCG_uxSense.pdf target=_blank rel=noopener>PDF
</a><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/10.1109/TVCG.2023.3241581 target=_blank rel=noopener>DOI</a></div></div><div class=ml-3><a href=./publication/tvcg-2023-uxsense/><img src=./publication/tvcg-2023-uxsense/featured_hu73197f1111d73663912e0ff0526d2f03_1208447_150x0_resize_q75_h2_lanczos_3.webp height=82 width=150 alt="uxSense: Supporting User Experience Analysis with Visualization and Computer Vision" loading=lazy></a></div></div><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=./publication/chi23-copractter/>CoPracTter: Toward Integrating Personalized Practice Scenarios, Timely Feedback and Social Support into An Online Support Tool for Coping with Stuttering in China</a></div><div class="stream-meta article-metadata"><div class=article-metadata><div><span><a href=./author/li-feng/>Li Feng</a></span>, <span><a href=./author/zeyu-xiong/>Zeyu Xiong</a></span>, <span><a href=./author/xinyi-li/>Xinyi Li</a></span>, <span><a href=./author/mingming-fan/>Mingming Fan</a></span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Corresponding author"></i></div><span class=article-date>February 2023
</span><span class=middot-divider></span>
<span class=pub-publication>CHI 2023</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://www.mingmingfan.com/papers/CHI23-CoPractTer.pdf target=_blank rel=noopener>PDF</a></div></div><div class=ml-3><a href=./publication/chi23-copractter/><img src=./publication/chi23-copractter/featured_hu63eaf26345d82d4e3c76c08f565ac722_6442030_150x0_resize_q75_h2_lanczos.webp height=61 width=150 alt="CoPracTter: Toward Integrating Personalized Practice Scenarios, Timely Feedback and Social Support into An Online Support Tool for Coping with Stuttering in China" loading=lazy></a></div></div><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=./publication/chi23-vr-intergenerational-gap/>Bridging the Generational Gap : Exploring How Virtual Reality Supports Remote Communication Between Grandparents and Grandchildren</a></div><div class="stream-meta article-metadata"><div class=article-metadata><div><span><a href=./author/xiaoying-wei/>Xiaoying Wei</a></span>, <span><a href=./author/yizheng-gu/>Yizheng Gu</a></span>, <span><a href=./author/emily-kuang/>Emily Kuang</a></span>, <span><a href=./author/xian-wang/>Xian Wang</a></span>, <span><a href=./author/beiyan-cao/>Beiyan Cao</a></span>, <span><a href=./author/xiaofu-jin/>Xiaofu Jin</a></span>, <span><a href=./author/mingming-fan/>Mingming Fan</a></span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Corresponding author"></i></div><span class=article-date>February 2023
</span><span class=middot-divider></span>
<span class=pub-publication>CHI 2023</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://www.mingmingfan.com/papers/CHI23-bridge-gap.pdf target=_blank rel=noopener>PDF
</a><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/chi23-vr-intergenerational-gap/cite.bib>Cite</a></div></div><div class=ml-3><a href=./publication/chi23-vr-intergenerational-gap/><img src=./publication/chi23-vr-intergenerational-gap/featured_hu8e3acd7b452b1579ef464d2de134b32b_1925300_150x0_resize_q75_h2_lanczos.webp height=80 width=150 alt="Bridging the Generational Gap : Exploring How Virtual Reality Supports Remote Communication Between Grandparents and Grandchildren" loading=lazy></a></div></div><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=./publication/chi23-oa-typing/>Enhancing Older Adults’ Gesture Typing Experience Using the T9 Keyboard on Small Touchscreen Devices</a></div><div class="stream-meta article-metadata"><div class=article-metadata><div><span><a href=./author/emily-kuang/>Emily Kuang</a></span>, <span><a href=./author/ruihuan-chen/>Ruihuan Chen</a></span>, <span><a href=./author/mingming-fan/>Mingming Fan</a></span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Corresponding author"></i></div><span class=article-date>February 2023
</span><span class=middot-divider></span>
<span class=pub-publication>CHI 2023</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://www.mingmingfan.com/papers/CHI23_OA_Gesture_Typing_T9.pdf target=_blank rel=noopener>PDF
</a><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/chi23-oa-typing/cite.bib>Cite
</a><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/10.1145/3544548.3581105 target=_blank rel=noopener>DOI</a></div></div><div class=ml-3><a href=./publication/chi23-oa-typing/><img src=./publication/chi23-oa-typing/featured_hu281c0878955817726fab0276a47d20cc_2022324_150x0_resize_q75_h2_lanczos_3.webp height=49 width=150 alt="Enhancing Older Adults’ Gesture Typing Experience Using the T9 Keyboard on Small Touchscreen Devices" loading=lazy></a></div></div><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=./publication/chi23-ux-conversational-ai-assistant/>Collaboration with Conversational AI Assistants for UX Evaluation: Questions and How to Ask them (Voice vs. Text)</a></div><div class="stream-meta article-metadata"><div class=article-metadata><div><span><a href=./author/emily-kuang/>Emily Kuang</a></span>, <span><a href=./author/ehsan-jahangirzadeh-soure/>Ehsan Jahangirzadeh Soure</a></span>, <span><a href=./author/mingming-fan/>Mingming Fan</a></span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Corresponding author"></i>, <span><a href=./author/jian-zhao/>Jian Zhao</a></span>, <span><a href=./author/kristen-shinohara/>Kristen Shinohara</a></span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Corresponding author"></i></div><span class=article-date>February 2023
</span><span class=middot-divider></span>
<span class=pub-publication>CHI 2023</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://www.mingmingfan.com/papers/CHI23_UX_Design_Probe.pdf target=_blank rel=noopener>PDF
</a><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/chi23-ux-conversational-ai-assistant/cite.bib>Cite
</a><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/10.1145/3544548.3581247 target=_blank rel=noopener>DOI</a></div></div><div class=ml-3><a href=./publication/chi23-ux-conversational-ai-assistant/><img src=./publication/chi23-ux-conversational-ai-assistant/featured_hu2ded00ce85048d122b739fa6c894a05a_871191_150x0_resize_q75_h2_lanczos_3.webp height=68 width=150 alt="Collaboration with Conversational AI Assistants for UX Evaluation: Questions and How to Ask them (Voice vs. Text)" loading=lazy></a></div></div><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=./publication/chi23-dhh-livestreaming/>Sparkling Silence: Practices and Challenges of Livestreaming Among Deaf or Hard of Hearing Streamers</a></div><div class="stream-meta article-metadata"><div class=article-metadata><div><span><a href=./author/beiyan-cao/>Beiyan Cao</a></span>, <span><a href=./author/changyang-he/>Changyang He</a></span>, <span><a href=./author/muzhi-zhou/>Muzhi Zhou</a></span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Corresponding author"></i>, <span><a href=./author/mingming-fan/>Mingming Fan</a></span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Corresponding author"></i></div><span class=article-date>February 2023
</span><span class=middot-divider></span>
<span class=pub-publication>CHI 2023</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://www.mingmingfan.com/papers/CHI23-DHH-livestreaming.pdf target=_blank rel=noopener>PDF</a></div></div><div class=ml-3><a href=./publication/chi23-dhh-livestreaming/><img src=./publication/chi23-dhh-livestreaming/featured_hu66403fa46f96e7790dcc41af899c893f_7785539_150x0_resize_q75_h2_lanczos.webp height=50 width=150 alt="Sparkling Silence: Practices and Challenges of Livestreaming Among Deaf or Hard of Hearing Streamers" loading=lazy></a></div></div><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=./publication/chi23-blv-robot-navigation/>"I am the follower, also the boss": Exploring Different Levels of Autonomy and Machine Forms of Guiding Robots for the Visually Impaired</a></div><div class="stream-meta article-metadata"><div class=article-metadata><div><span><a href=./author/yan-zhang/>Yan Zhang</a></span>, <span><a href=./author/ziang-li/>Ziang Li</a></span>, <span><a href=./author/haole-guo/>Haole Guo</a></span>, <span><a href=./author/luyao-wang/>Luyao Wang</a></span>, <span><a href=./author/qihe-chen/>Qihe Chen</a></span>, <span><a href=./author/wenjie-jiang/>Wenjie Jiang</a></span>, <span><a href=./author/mingming-fan/>Mingming Fan</a></span>, <span><a href=./author/guyue-zhou/>Guyue Zhou</a></span>, <span><a href=./author/jiangtao-gong/>Jiangtao Gong</a></span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Corresponding author"></i></div><span class=article-date>February 2023
</span><span class=middot-divider></span>
<span class=pub-publication>CHI 2023</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://www.mingmingfan.com/papers/CHI2023_Navigation_Robot_Autonomy.pdf target=_blank rel=noopener>PDF</a></div></div><div class=ml-3><a href=./publication/chi23-blv-robot-navigation/><img src=./publication/chi23-blv-robot-navigation/featured_hu7dd70351c480c5148e6d50933c6c82c5_6142309_150x0_resize_q75_h2_lanczos.webp height=83 width=150 alt='"I am the follower, also the boss": Exploring Different Levels of Autonomy and Machine Forms of Guiding Robots for the Visually Impaired' loading=lazy></a></div></div><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=./publication/chi23-voice-hand-face/>Enabling Voice-Accompanying Hand-to-Face Gesture Recognition with Cross-Device Sensing</a></div><div class="stream-meta article-metadata"><div class=article-metadata><div><span><a href=./author/zisu-li/>Zisu Li</a></span>, <span><a href=./author/chen-liang/>Chen Liang</a></span>, <span><a href=./author/yuntao-wang/>Yuntao Wang</a></span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Corresponding author"></i>, <span><a href=./author/yue-qin/>Yue Qin</a></span>, <span><a href=./author/chun-yu/>Chun Yu</a></span>, <span><a href=./author/yukang-yan/>Yukang Yan</a></span>, <span><a href=./author/mingming-fan/>Mingming Fan</a></span>, <span><a href=./author/yuanchun-shi/>Yuanchun Shi</a></span></div><span class=article-date>February 2023
</span><span class=middot-divider></span>
<span class=pub-publication>CHI 2023</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://www.mingmingfan.com/papers/CHI23-voice-hand-gestures.pdf target=_blank rel=noopener>PDF
</a><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/chi23-voice-hand-face/cite.bib>Cite</a></div></div><div class=ml-3><a href=./publication/chi23-voice-hand-face/><img src=./publication/chi23-voice-hand-face/featured_hub707d3d57145ddd61bb61ed9712d439d_375050_150x0_resize_q75_h2_lanczos.webp height=77 width=150 alt="Enabling Voice-Accompanying Hand-to-Face Gesture Recognition with Cross-Device Sensing" loading=lazy></a></div></div><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=./publication/iwc2022-oa-vr-game-ta/>Older Adults’ Concurrent and Retrospective Think-Aloud Verbalizations for Identifying User Experience Problems of VR Games</a></div><div class="stream-meta article-metadata"><div class=article-metadata><div><span><a href=./author/mingming-fan/>Mingming Fan</a></span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Corresponding author"></i>, <span><a href=./author/vinita-tibdewal/>Vinita Tibdewal</a></span>, <span><a href=./author/qiwen-zhao/>Qiwen Zhao</a></span>, <span><a href=./author/lizhou-cao/>Lizhou Cao</a></span>, <span><a href=./author/chao-peng/>Chao Peng</a></span>, <span><a href=./author/runxuan-shu/>Runxuan Shu</a></span>, <span><a href=./author/yujia-shan/>Yujia Shan</a></span></div><span class=article-date>December 2022
</span><span class=middot-divider></span>
<span class=pub-publication>IwC 2022</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href="https://academic.oup.com/iwc/advance-article/doi/10.1093/iwc/iwac039/6967130?login=true" target=_blank rel=noopener>PDF</a></div></div><div class=ml-3><a href=./publication/iwc2022-oa-vr-game-ta/><img src=./publication/iwc2022-oa-vr-game-ta/featured_hu7a04c764ee55d5a23cb29d9fccfa075b_1261702_150x0_resize_q75_h2_lanczos.webp height=112 width=150 alt="Older Adults’ Concurrent and Retrospective Think-Aloud Verbalizations for Identifying User Experience Problems of VR Games" loading=lazy></a></div></div><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=./publication/imwut22-oa-interactive-guidance/>Synapse: Interactive Guidance by Demonstration with Trial-and-Error Support for Older Adults to Use Smartphone Apps</a></div><div class="stream-meta article-metadata"><div class=article-metadata><div><span><a href=./author/xiaofu-jin/>Xiaofu Jin</a></span>, <span><a href=./author/xiaozhu-hu/>Xiaozhu Hu</a></span>, <span><a href=./author/xiaoying-wei/>Xiaoying Wei</a></span>, <span><a href=./author/mingming-fan/>Mingming Fan</a></span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Corresponding author"></i></div><span class=article-date>December 2022
</span><span class=middot-divider></span>
<span class=pub-publication>IMWUT 2022</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://www.mingmingfan.com/papers/IMWUT_OlderAdults_Trial_And_Error.pdf target=_blank rel=noopener>PDF
</a><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://youtu.be/kSv-HOeTIkc target=_blank rel=noopener>Video</a></div></div><div class=ml-3><a href=./publication/imwut22-oa-interactive-guidance/><img src=./publication/imwut22-oa-interactive-guidance/featured_hucab56ae345f1dde599102718887d224d_1943476_150x0_resize_q75_h2_lanczos.webp height=28 width=150 alt="Synapse: Interactive Guidance by Demonstration with Trial-and-Error Support for Older Adults to Use Smartphone Apps" loading=lazy></a></div></div><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=./publication/cscw22-typist/>Typist Experiment: an Investigation of Human-to-Human Dictation via Role-play to Inform Voice-based Text Authoring</a></div><div class="stream-meta article-metadata"><div class=article-metadata><div><span><a href=./author/can-liu/>Can Liu</a></span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Corresponding author"></i>, <span><a href=./author/siying-hu/>Siying Hu</a></span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="student first author"></i>, <span><a href=./author/li-feng/>Li Feng</a></span>, <span><a href=./author/mingming-fan/>Mingming Fan</a></span></div><span class=article-date>December 2022
</span><span class=middot-divider></span>
<span class=pub-publication>CSCW 2022</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://dl.acm.org/doi/pdf/10.1145/3555758 target=_blank rel=noopener>PDF
</a><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://youtu.be/f9lO9tin4tw target=_blank rel=noopener>Video</a></div></div><div class=ml-3><a href=./publication/cscw22-typist/><img src=./publication/cscw22-typist/featured_hucac4c27c650346a6f5be2228389f5ea1_220965_150x0_resize_q75_h2_lanczos.webp height=74 width=150 alt="Typist Experiment: an Investigation of Human-to-Human Dictation via Role-play to Inform Voice-based Text Authoring" loading=lazy></a></div></div><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=./publication/assets22-oa-banking/>"I Used To Carry A Wallet, Now I Just Need To Carry My Phone": Understanding Current Banking Practices and Challenges Among Older Adults in China</a></div><div class="stream-meta article-metadata"><div class=article-metadata><div><span><a href=./author/xiaofu-jin/>Xiaofu Jin</a></span>, <span><a href=./author/mingming-fan/>Mingming Fan</a></span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Corresponding author"></i></div><span class=article-date>December 2022
</span><span class=middot-divider></span>
<span class=pub-publication>ASSETS 2022</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://www.mingmingfan.com/papers/ASSETS22_Older_Adults_Banking.pdf target=_blank rel=noopener>PDF</a></div></div><div class=ml-3><a href=./publication/assets22-oa-banking/><img src=./publication/assets22-oa-banking/featured_huddae42340f0608af87bddd00dfad519f_581163_150x0_resize_q75_h2_lanczos.webp height=79 width=150 alt='"I Used To Carry A Wallet, Now I Just Need To Carry My Phone": Understanding Current Banking Practices and Challenges Among Older Adults in China' loading=lazy></a></div></div><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=./publication/chinesechi2022-olderadults-ai/>Understanding Older Adults' Perceptions and Challenges in Using AI-enabled Everyday Technologies</a></div><div class="stream-meta article-metadata"><div class=article-metadata><div><span><a href=./author/esha-shandilya/>Esha Shandilya</a></span>, <span><a href=./author/mingming-fan/>Mingming Fan</a></span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Corresponding author"></i></div><span class=article-date>December 2022
</span><span class=middot-divider></span>
<span class=pub-publication>Chinese CHI 2022</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://www.mingmingfan.com/papers/ChineseCHI22_OlderAdults_AI.pdf target=_blank rel=noopener>PDF</a></div></div><div class=ml-3><a href=./publication/chinesechi2022-olderadults-ai/><img src=./publication/chinesechi2022-olderadults-ai/featured_hub335d3ea1f75636366816e50c7809c44_195110_150x0_resize_q75_h2_lanczos.webp height=63 width=150 alt="Understanding Older Adults' Perceptions and Challenges in Using AI-enabled Everyday Technologies" loading=lazy></a></div></div><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=./publication/ijhci-olderadults-covid-vis/>Understanding How Older Adults Comprehend COVID-19 Interactive Visualizations via Think-Aloud Protocol</a></div><div class="stream-meta article-metadata"><div class=article-metadata><div><span><a href=./author/mingming-fan/>Mingming Fan</a></span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Corresponding author"></i>, <span><a href=./author/yiwen-wang/>Yiwen Wang</a></span>, <span><a href=./author/yuni-xie/>Yuni Xie</a></span>, <span><a href=./author/franklin-li/>Franklin Li</a></span>, <span><a href=./author/chunyang-chen/>Chunyang Chen</a></span></div><span class=article-date>December 2022
</span><span class=middot-divider></span>
<span class=pub-publication>IJHCI 2022</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://www.mingmingfan.com/papers/IJHCI-older-adults-visualizations.pdf target=_blank rel=noopener>PDF
</a><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://www.mingmingfan.com/papers/IJHCI-older-adults-visualizations-appendix.pdf target=_blank rel=noopener>Source Document</a></div></div><div class=ml-3><a href=./publication/ijhci-olderadults-covid-vis/><img src=./publication/ijhci-olderadults-covid-vis/featured_hu65fa16c210d01cc509c7b4a791933e6c_463709_150x0_resize_q75_h2_lanczos.webp height=124 width=150 alt="Understanding How Older Adults Comprehend COVID-19 Interactive Visualizations via Think-Aloud Protocol" loading=lazy></a></div></div><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=./publication/chi22-kuaidigui/>'I Shake The Package To Check If It's Mine': A Study of Package Fetching Practices and Challenges of Blind and Low Vision People in China</a></div><div class="stream-meta article-metadata"><div class=article-metadata><div><span><a href=./author/wentao-lei/>Wentao Lei</a></span>, <span><a href=./author/mingming-fan/>Mingming Fan</a></span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Corresponding author"></i>, <span><a href=./author/juliann-thang/>Juliann Thang</a></span></div><span class=article-date>March 2022
</span><span class=middot-divider></span>
<span class=pub-publication>CHI 2022</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://www.mingmingfan.com/papers/chi22_243_BLV_KuaiDiGui.pdf target=_blank rel=noopener>PDF</a></div></div><div class=ml-3><a href=./publication/chi22-kuaidigui/><img src=./publication/chi22-kuaidigui/featured_hu9189f72fef482c53a3d235abff599ee9_5712045_150x0_resize_q75_h2_lanczos.webp height=123 width=150 alt=" 'I Shake The Package To Check If It's Mine': A Study of Package Fetching Practices and Challenges of Blind and Low Vision People in China" loading=lazy></a></div></div><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=./publication/chi22-userdefinedgestures/>'I Don't Want People to Look At Me Differently': Designing User-Defined Above-the-Neck Gestures for People with Upper Body Motor Impairments</a></div><div class="stream-meta article-metadata"><div class=article-metadata><div><span><a href=./author/xuan-zhao/>Xuan Zhao</a></span>, <span><a href=./author/mingming-fan/>Mingming Fan</a></span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Corresponding author"></i>, <span><a href=./author/teng-han/>Teng Han</a></span></div><span class=article-date>March 2022
</span><span class=middot-divider></span>
<span class=pub-publication>CHI 2022</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://www.mingmingfan.com/papers/chi22_243_BLV_KuaiDiGui.pdf target=_blank rel=noopener>PDF</a></div></div><div class=ml-3><a href=./publication/chi22-userdefinedgestures/><img src=./publication/chi22-userdefinedgestures/featured_hud01615fd1e109ad5e9bfd88ce28472e6_744233_150x0_resize_q75_h2_lanczos.webp height=170 width=150 alt=" 'I Don't Want People to Look At Me Differently': Designing User-Defined Above-the-Neck Gestures for People with Upper Body Motor Impairments" loading=lazy></a></div></div><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=./publication/chi22-ux-survey/>"Merging Results Is No Easy Task": An International Survey Study of Collaborative Data Analysis Practices Among UX Practitioners</a></div><div class="stream-meta article-metadata"><div class=article-metadata><div><span><a href=./author/emily-kuang/>Emily Kuang</a></span>, <span><a href=./author/xiaofu-jin/>Xiaofu Jin</a></span>, <span><a href=./author/mingming-fan/>Mingming Fan</a></span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Corresponding author"></i></div><span class=article-date>March 2022
</span><span class=middot-divider></span>
<span class=pub-publication>CHI 2022</span></div></div><div class=btn-links><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/chi22-ux-survey/cite.bib>Cite
</a><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/10.1145/3491102.3517647 target=_blank rel=noopener>DOI</a></div></div><div class=ml-3><a href=./publication/chi22-ux-survey/><img src=./publication/chi22-ux-survey/featured_hu4039d55e0dc39e9ddd6d3b2c7c28dbc3_95433_150x0_resize_q75_h2_lanczos_3.webp height=69 width=150 alt='"Merging Results Is No Easy Task": An International Survey Study of Collaborative Data Analysis Practices Among UX Practitioners' loading=lazy></a></div></div><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=./publication/cacm-eyelidgestures/>Eyelid Gestures for People with Motor Impairments</a></div><div class="stream-meta article-metadata"><div class=article-metadata><div><span><a href=./author/mingming-fan/>Mingming Fan</a></span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Corresponding author"></i>, <span><a href=./author/zhen-li/>Zhen Li</a></span>, <span><a href=./author/franklin-mingzhe-li/>Franklin Mingzhe Li</a></span></div><span class=article-date>March 2022
</span><span class=middot-divider></span>
<span class=pub-publication>CACM 2022</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://cacm.acm.org/magazines/2022/1/257451-eyelid-gestures-for-people-with-motor-impairments/pdf target=_blank rel=noopener>PDF
</a><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://github.com/mingming-fan/EyelidGesturesDetection target=_blank rel=noopener>Code
</a><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://cacm.acm.org/magazines/2022/1/257452-technical-perspective-eyelid-gestures-enhance-mobile-interaction/abstract target=_blank rel=noopener>Project
</a><a class="btn btn-outline-primary btn-page-header btn-sm" href="https://www.youtube.com/watch?v=GgpW4tmvdM0" target=_blank rel=noopener>Video
</a><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://cacm.acm.org/magazines/2022/1/257451-eyelid-gestures-for-people-with-motor-impairments/fulltext target=_blank rel=noopener>Source Document</a></div></div><div class=ml-3><a href=./publication/cacm-eyelidgestures/><img src=./publication/cacm-eyelidgestures/featured_hu37f591076f984926bf945660cf87cb66_153401_150x0_resize_q75_h2_lanczos.webp height=150 width=150 alt="Eyelid Gestures for People with Motor Impairments" loading=lazy></a></div></div><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=./publication/coux/>CoUX: Collaborative Visual Analysis of Think-Aloud Usability Test Videos for Digital Interfaces</a></div><div class="stream-meta article-metadata"><div class=article-metadata><div><span><a href=./author/ehsan-jahangirzadeh-soure/>Ehsan Jahangirzadeh Soure</a></span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Equal Contribution"></i>, <span><a href=./author/emily-kuang/>Emily Kuang</a></span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Equal Contribution"></i>, <span><a href=./author/mingming-fan/>Mingming Fan</a></span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Corresponding author"></i>, <span><a href=./author/jian-zhao/>Jian Zhao</a></span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Corresponding author"></i></div><span class=article-date>December 2021
</span><span class=middot-divider></span>
<span class=pub-publication>VIS 2021</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://www.mingmingfan.com/papers/coux-vis21.pdf target=_blank rel=noopener>PDF
</a><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/coux/cite.bib>Cite
</a><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://github.com/WatVis/CoUX target=_blank rel=noopener>Code
</a><a class="btn btn-outline-primary btn-page-header btn-sm" href="https://www.youtube.com/watch?v=uaSl0x4c93Y" target=_blank rel=noopener>Video
</a><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/10.1109/TVCG.2021.3114822 target=_blank rel=noopener>DOI</a></div></div><div class=ml-3><a href=./publication/coux/><img src=./publication/coux/featured_hu5aa6ab34d05d6616d78c82fee54391f1_768341_150x0_resize_q75_h2_lanczos_3.webp height=82 width=150 alt="CoUX: Collaborative Visual Analysis of Think-Aloud Usability Test Videos for Digital Interfaces" loading=lazy></a></div></div><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=./publication/oa-ta/>Older Adults' Think-Aloud Verbalizations and Speech Features for Identifying User Experience Problems</a></div><div class="stream-meta article-metadata"><div class=article-metadata><div><span><a href=./author/mingming-fan/>Mingming Fan</a></span>, <span><a href=./author/qiwen-zhao/>Qiwen Zhao</a></span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="first student author"></i>, <span><a href=./author/vinita-tibdewal/>Vinita Tibdewal</a></span></div><span class=article-date>December 2021
</span><span class=middot-divider></span>
<span class=pub-publication>CHI 2021</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://www.mingmingfan.com/papers/CHI21_OlderAdults_ThinkAloud_UXProblems.pdf target=_blank rel=noopener>PDF
</a><a class="btn btn-outline-primary btn-page-header btn-sm" href="https://www.youtube.com/watch?v=OkqStiGulbY" target=_blank rel=noopener>Video</a></div></div><div class=ml-3><a href=./publication/oa-ta/><img src=./publication/oa-ta/featured_huc5b2596daae37e4fdf3fb1aed1ae36ef_1083293_150x0_resize_q75_h2_lanczos.webp height=66 width=150 alt="Older Adults' Think-Aloud Verbalizations and Speech Features for Identifying User Experience Problems" loading=lazy></a></div></div><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=./publication/chartseer/>ChartSeer: Interactive Steering Exploratory Visual Analysis with Machine Intelligence</a></div><div class="stream-meta article-metadata"><div class=article-metadata><div><span><a href=./author/jian-zhao/>Jian Zhao</a></span>, <span><a href=./author/mingming-fan/>Mingming Fan</a></span>, <span><a href=./author/mi-feng/>Mi Feng</a></span></div><span class=article-date>May 2021
</span><span class=middot-divider></span>
<span class=pub-publication>TVCG (VIS 2021)</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://www.mingmingfan.com/papers/chartseer_tvcg.pdf target=_blank rel=noopener>PDF
</a><a class="btn btn-outline-primary btn-page-header btn-sm" href="https://www.youtube.com/watch?v=w5K1U6f1Oro" target=_blank rel=noopener>Video
</a><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/10.1109/TVCG.2020.3018724 target=_blank rel=noopener>DOI</a></div></div><div class=ml-3><a href=./publication/chartseer/><img src=./publication/chartseer/featured_hu51898de3fd3f3dcd85fe409605814c5e_1708443_150x0_resize_q75_h2_lanczos.webp height=93 width=150 alt="ChartSeer: Interactive Steering Exploratory Visual Analysis with Machine Intelligence" loading=lazy></a></div></div><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=./publication/eyelidgestures4assets/>Eyelid Gestures on Mobile Devices for People with Motor Impairments</a></div><div class="stream-meta article-metadata"><div class=article-metadata><div><span><a href=./author/mingming-fan/>Mingming Fan</a></span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="equal contribution"></i>, <span><a href=./author/zhen-li/>Zhen Li</a></span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="equal contribution"></i>, <span><a href=./author/franklin-mingzhe-li/>Franklin Mingzhe Li</a></span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="equal contribution"></i></div><span class=article-date>November 2020
</span><span class=middot-divider></span>
<span class=pub-publication>ASSETS 2020</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://www.mingmingfan.com/papers/ASSETS_2020_Fan.pdf target=_blank rel=noopener>PDF
</a><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://github.com/mingming-fan/EyelidGesturesDetection target=_blank rel=noopener>Code
</a><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://cacm.acm.org/magazines/2022/1/257452-technical-perspective-eyelid-gestures-enhance-mobile-interaction/abstract target=_blank rel=noopener>Project
</a><a class="btn btn-outline-primary btn-page-header btn-sm" href="https://www.youtube.com/watch?v=hocUdFlgX5A" target=_blank rel=noopener>Video
</a><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://github.com/mingming-fan/EyelidGesturesDetection/releases target=_blank rel=noopener>Source Document</a></div></div><div class=ml-3><a href=./publication/eyelidgestures4assets/><img src=./publication/eyelidgestures4assets/featured_hu16917b679f115b07e8fd438e20cd64cf_105865_150x0_resize_q75_h2_lanczos.webp height=61 width=150 alt="Eyelid Gestures on Mobile Devices for People with Motor Impairments" loading=lazy></a></div></div><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=./publication/uxproblemsdetection/>Automatic Detection of Usability Problem Encounters in Think-Aloud Sessions</a></div><div class="stream-meta article-metadata"><div class=article-metadata><div><span><a href=./author/mingming-fan/>Mingming Fan</a></span>, <span><a href=./author/yue-li/>Yue Li</a></span>, <span><a href=./author/khai-n.-truong/>Khai N. Truong</a></span></div><span class=article-date>May 2020
</span><span class=middot-divider></span>
<span class=pub-publication>TiiS 2020</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://www.mingmingfan.com/papers/TiiS2020-Fan.pdf target=_blank rel=noopener>PDF
</a><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/10.1145/3491102.3517647 target=_blank rel=noopener>DOI</a></div></div><div class=ml-3><a href=./publication/uxproblemsdetection/><img src=./publication/uxproblemsdetection/featured_hub8d52aa5372c63e77d4d9bef65ae253e_16155_150x0_resize_q75_h2_lanczos.webp height=63 width=150 alt="Automatic Detection of Usability Problem Encounters in Think-Aloud Sessions" loading=lazy></a></div></div><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=./publication/vista/>VisTA: Integrating Machine Intelligence with Visualization to Support the Investigation of Think-Aloud Sessions</a></div><div class="stream-meta article-metadata"><div class=article-metadata><div><span><a href=./author/mingming-fan/>Mingming Fan</a></span>, <span><a href=./author/ke-wu/>Ke Wu</a></span>, <span><a href=./author/jian-zhao/>Jian Zhao</a></span>, <span><a href=./author/yue-li/>Yue Li</a></span>, <span><a href=./author/winter-wei/>Winter Wei</a></span>, <span><a href=./author/khai-n.-truong/>Khai N. Truong</a></span></div><span class=article-date>April 2019
</span><span class=middot-divider></span>
<span class=pub-publication>TVCG (VIS 2019)</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://www.mingmingfan.com/papers/VisTA.pdf target=_blank rel=noopener>PDF
</a><a class="btn btn-outline-primary btn-page-header btn-sm" href="https://www.youtube.com/watch?v=YHpfBqm1Aaw" target=_blank rel=noopener>Video
</a><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/10.1109/TVCG.2019.2934797 target=_blank rel=noopener>DOI</a></div></div><div class=ml-3><a href=./publication/vista/><img src=./publication/vista/featured_hu94650d6457ed202eb28fa4c216d9947d_1136558_150x0_resize_q75_h2_lanczos.webp height=68 width=150 alt="VisTA: Integrating Machine Intelligence with Visualization to Support the Investigation of Think-Aloud Sessions" loading=lazy></a></div></div><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=./publication/inkplanner/>InkPlanner: Supporting Prewriting via Intelligent Visual Diagramming</a></div><div class="stream-meta article-metadata"><div class=article-metadata><div><span><a href=./author/zhicong-lu/>Zhicong Lu</a></span>, <span><a href=./author/mingming-fan/>Mingming Fan</a></span>, <span><a href=./author/yun-wang/>Yun Wang</a></span>, <span><a href=./author/jian-zhao/>Jian Zhao</a></span>, <span><a href=./author/michelle-annett/>Michelle Annett</a></span>, <span><a href=./author/daniel-wigdor/>Daniel Wigdor</a></span></div><span class=article-date>December 2018
</span><span class=middot-divider></span>
<span class=pub-publication>TVCG (VIS 2018)</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://www.mingmingfan.com/papers/InkPlanner_TVCG.pdf target=_blank rel=noopener>PDF
</a><a class="btn btn-outline-primary btn-page-header btn-sm" href="https://www.youtube.com/watch?v=vzVOdtCP5LM" target=_blank rel=noopener>Video
</a><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/10.1145/3491102.3517647 target=_blank rel=noopener>DOI</a></div></div><div class=ml-3><a href=./publication/inkplanner/><img src=./publication/inkplanner/featured_hudbd6c8bfdf01566ce54eaf958a0b0978_506254_150x0_resize_q75_h2_lanczos.webp height=94 width=150 alt="InkPlanner: Supporting Prewriting via Intelligent Visual Diagramming" loading=lazy></a></div></div><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=./publication/seniorguidelines/>Guidelines for Creating Senior-Friendly Product Instructions</a></div><div class="stream-meta article-metadata"><div class=article-metadata><div><span><a href=./author/mingming-fan/>Mingming Fan</a></span>, <span><a href=./author/khai-n.-truong/>Khai N. Truong</a></span></div><span class=article-date>April 2018
</span><span class=middot-divider></span>
<span class=pub-publication>TACCESS (ASSETS 2018)</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://www.mingmingfan.com/papers/TACCESS-2018-Fan.pdf target=_blank rel=noopener>PDF</a></div></div><div class=ml-3><a href=./publication/seniorguidelines/><img src=./publication/seniorguidelines/featured_huf46122220513721019857947253c18bc_3142509_150x0_resize_q75_h2_lanczos.webp height=93 width=150 alt="Guidelines for Creating Senior-Friendly Product Instructions" loading=lazy></a></div></div><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=./publication/braillesketch/>BrailleSketch: A Gesture-based Text Entry Method for People with Visual Impairments</a></div><div class="stream-meta article-metadata"><div class=article-metadata><div><span><a href=./author/franklin-li/>Franklin Li</a></span>, <span><a href=./author/mingming-fan/>Mingming Fan</a></span>, <span><a href=./author/khai-n.-truong/>Khai N. Truong</a></span></div><span class=article-date>August 2017
</span><span class=middot-divider></span>
<span class=pub-publication>ASSETS 2017</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://www.mingmingfan.com/papers/BrailleSketch-ASSETS-2017.pdf target=_blank rel=noopener>PDF</a></div></div><div class=ml-3><a href=./publication/braillesketch/><img src=./publication/braillesketch/featured_hu9f3a4d2f43c4f68e2627d7f2e1d6912a_278010_150x0_resize_q75_h2_lanczos.webp height=128 width=150 alt="BrailleSketch: A Gesture-based Text Entry Method for People with Visual Impairments" loading=lazy></a></div></div></div></div></div></section></div><div class=page-footer><div class=container><footer class=site-footer><p class="powered-by copyright-license-text">© 2025 APEX Group. This work is licensed under <a href=https://creativecommons.org/licenses/by-nc-nd/4.0 rel="noopener noreferrer" target=_blank>CC BY NC ND 4.0</a></p><p class="powered-by footer-license-icons"><a href=https://creativecommons.org/licenses/by-nc-nd/4.0 rel="noopener noreferrer" target=_blank aria-label="Creative Commons"><i class="fab fa-creative-commons fa-2x" aria-hidden=true></i>
<i class="fab fa-creative-commons-by fa-2x" aria-hidden=true></i>
<i class="fab fa-creative-commons-nc fa-2x" aria-hidden=true></i>
<i class="fab fa-creative-commons-nd fa-2x" aria-hidden=true></i></a></p><p class=powered-by>Published with <a href="https://wowchemy.com/?utm_campaign=poweredby" target=_blank rel=noopener>Wowchemy</a> — the free, <a href=https://github.com/wowchemy/wowchemy-hugo-themes target=_blank rel=noopener>open source</a> website builder that empowers creators.</p></footer></div></div><script src=./js/vendor-bundle.min.b4708d4364577c16ab7001b265a063a4.js></script><script src=https://cdn.jsdelivr.net/npm/leaflet@1.7.1/dist/leaflet.min.js integrity crossorigin=anonymous></script><script id=search-hit-fuse-template type=text/x-template>
    <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
    </div>
  </script><script src=https://cdn.jsdelivr.net/gh/krisk/Fuse@v3.2.1/dist/fuse.min.js integrity="sha512-o38bmzBGX+hD3JHWUFCDA09btWaqrNmoJ3RXLlrysA7PP01Kgs4UlE4MhelE1v5dJR3+cxlR4qQlotsW7jKsnw==" crossorigin=anonymous></script><script src=https://cdn.jsdelivr.net/gh/julmot/mark.js@8.11.1/dist/jquery.mark.min.js integrity="sha512-mhbv5DqBMgrWL+32MmsDOt/OAvqr/cHimk6B8y/bx/xS88MVkYGPiVv2ixKVrkywF2qHplNRUvFsAHUdxZ3Krg==" crossorigin=anonymous></script><script id=page-data type=application/json>{"use_headroom":false}</script><script src=./en/js/wowchemy.min.6ab16275cbca742a586c1726e3d94093.js></script><script src=./js/wowchemy-map.a26e9d2f7238ba5b868384f1c5bc6477.js type=module></script><div id=modal class="modal fade" role=dialog><div class=modal-dialog><div class=modal-content><div class=modal-header><h5 class=modal-title>Cite</h5><button type=button class=close data-dismiss=modal aria-label=Close>
<span aria-hidden=true>&#215;</span></button></div><div class=modal-body><pre><code></code></pre></div><div class=modal-footer><a class="btn btn-outline-primary my-1 js-copy-cite" href=# target=_blank><i class="fas fa-copy"></i> Copy
</a><a class="btn btn-outline-primary my-1 js-download-cite" href=# target=_blank><i class="fas fa-download"></i> Download</a><div id=modal-error></div></div></div></div></div><script src=./js/wowchemy-publication.68f8d7090562ca65fc6d3cb3f8f2d2cb.js type=module></script></body></html>
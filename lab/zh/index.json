[{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":-62135596800,"objectID":"635c1c25bf0506e3c7ff4a2d84481072","permalink":"/zh/author/mingming-fan/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/zh/author/mingming-fan/","section":"authors","summary":"","tags":null,"title":"Mingming Fan","type":"authors"},{"authors":["Zhiyi Rong"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":-62135596800,"objectID":"794ac84730168f55492a62efd1514c08","permalink":"/zh/author/zhiyi-rong/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/zh/author/zhiyi-rong/","section":"authors","summary":"","tags":null,"title":"Zhiyi Rong","type":"authors"},{"authors":["Emily Kuang"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":-62135596800,"objectID":"d0cce6aaf4399802f8e57b64547839d3","permalink":"/zh/author/emily-kuang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/zh/author/emily-kuang/","section":"authors","summary":"","tags":null,"title":"Emily Kuang","type":"authors"},{"authors":["Xiaofu Jin"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":-62135596800,"objectID":"362f4fe5a9611440b6b3f4f562b951ed","permalink":"/zh/author/xiaofu-jin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/zh/author/xiaofu-jin/","section":"authors","summary":"","tags":null,"title":"Xiaofu Jin","type":"authors"},{"authors":["Yiwen (Molly) Wang"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":-62135596800,"objectID":"ab0839c5737260cb3856325c3fa4e076","permalink":"/zh/author/yiwen-molly-wang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/zh/author/yiwen-molly-wang/","section":"authors","summary":"","tags":null,"title":"Yiwen (Molly) Wang","type":"authors"},{"authors":["Pengkai Liao"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":-62135596800,"objectID":"d239ecbd9d18e75a86862c246b62b406","permalink":"/zh/author/pengkai-liao/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/zh/author/pengkai-liao/","section":"authors","summary":"","tags":null,"title":"Pengkai Liao","type":"authors"},{"authors":["Xiaoying Wei"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":-62135596800,"objectID":"9ed6e7572f960e0978d2cfe5bbb60423","permalink":"/zh/author/xiaoying-wei/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/zh/author/xiaoying-wei/","section":"authors","summary":"","tags":null,"title":"Xiaoying Wei","type":"authors"},{"authors":["Zisu Li"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":-62135596800,"objectID":"78a51a7ab77f6bd6105ad265d5a85873","permalink":"/zh/author/zisu-li/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/zh/author/zisu-li/","section":"authors","summary":"","tags":null,"title":"Zisu Li","type":"authors"},{"authors":["Chutian Jiang"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":-62135596800,"objectID":"17c75d263c8ef85c59d0f2e27393a49d","permalink":"/zh/author/chutian-jiang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/zh/author/chutian-jiang/","section":"authors","summary":"","tags":null,"title":"Chutian Jiang","type":"authors"},{"authors":["Li (Felicia) Feng"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":-62135596800,"objectID":"b1f048e24fa0dd7bdfd356e38fe88bd0","permalink":"/zh/author/li-felicia-feng/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/zh/author/li-felicia-feng/","section":"authors","summary":"","tags":null,"title":"Li (Felicia) Feng","type":"authors"},{"authors":["Monika Verma"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":-62135596800,"objectID":"37dd629c068158a97e69f89d58187c64","permalink":"/zh/author/monika-verma/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/zh/author/monika-verma/","section":"authors","summary":"","tags":null,"title":"Monika Verma","type":"authors"},{"authors":["Xiaozhu Hu"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":-62135596800,"objectID":"6b299337088786588f5aaa632d657f2d","permalink":"/zh/author/xiaozhu-hu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/zh/author/xiaozhu-hu/","section":"authors","summary":"","tags":null,"title":"Xiaozhu Hu","type":"authors"},{"authors":["Mary Thumma"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":-62135596800,"objectID":"c6313b56398bbddf9836871feee1d4c8","permalink":"/zh/author/mary-thumma/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/zh/author/mary-thumma/","section":"authors","summary":"","tags":null,"title":"Mary Thumma","type":"authors"},{"authors":["Zhiqing Wu"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":-62135596800,"objectID":"7fced6928864fe2de3241983cfbd6814","permalink":"/zh/author/zhiqing-wu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/zh/author/zhiqing-wu/","section":"authors","summary":"","tags":null,"title":"Zhiqing Wu","type":"authors"},{"authors":["Ruihuan Chen"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":-62135596800,"objectID":"83889bfd8e463097d17c07988b1e9263","permalink":"/zh/author/ruihuan-chen/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/zh/author/ruihuan-chen/","section":"authors","summary":"","tags":null,"title":"Ruihuan Chen","type":"authors"},{"authors":["Xian Wang"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":-62135596800,"objectID":"30f50317bddffd89701b9c8fd33703b6","permalink":"/zh/author/xian-wang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/zh/author/xian-wang/","section":"authors","summary":"","tags":null,"title":"Xian Wang","type":"authors"},{"authors":["Beiyan Cao"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":-62135596800,"objectID":"0441963bdbaccdbd2c2a844932f7649b","permalink":"/zh/author/beiyan-cao/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/zh/author/beiyan-cao/","section":"authors","summary":"","tags":null,"title":"Beiyan Cao","type":"authors"},{"authors":["Wentao Lei"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":-62135596800,"objectID":"18392e5e659621a0272c8f67798d3ad5","permalink":"/zh/author/wentao-lei/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/zh/author/wentao-lei/","section":"authors","summary":"","tags":null,"title":"Wentao Lei","type":"authors"},{"authors":["Xiaoyu Mo"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":-62135596800,"objectID":"3e20531f39cba4e842d0d16ba00ae331","permalink":"/zh/author/xiaoyu-mo/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/zh/author/xiaoyu-mo/","section":"authors","summary":"","tags":null,"title":"Xiaoyu Mo","type":"authors"},{"authors":["Luyao Shen"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":-62135596800,"objectID":"b7d9145aa47b6a6739b479b4b93058e4","permalink":"/zh/author/luyao-shen/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/zh/author/luyao-shen/","section":"authors","summary":"","tags":null,"title":"Luyao Shen","type":"authors"},{"authors":["Xuan Zhao"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":-62135596800,"objectID":"75c0c395f96be87409ddba5156dfafa1","permalink":"/zh/author/xuan-zhao/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/zh/author/xuan-zhao/","section":"authors","summary":"","tags":null,"title":"Xuan Zhao","type":"authors"},{"authors":["Yuni Xie"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":-62135596800,"objectID":"271fa96082b38557eafc8fe4ca59de40","permalink":"/zh/author/yuni-xie/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/zh/author/yuni-xie/","section":"authors","summary":"","tags":null,"title":"Yuni Xie","type":"authors"},{"authors":["Zhongyuan Liao"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":-62135596800,"objectID":"f5485d0f381cbbae36f0bff7424db3c7","permalink":"/zh/author/zhongyuan-liao/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/zh/author/zhongyuan-liao/","section":"authors","summary":"","tags":null,"title":"Zhongyuan Liao","type":"authors"},{"authors":["Shimei Qiu"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":-62135596800,"objectID":"1d149b888d539da2388cbb9649f98edb","permalink":"/zh/author/shimei-qiu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/zh/author/shimei-qiu/","section":"authors","summary":"","tags":null,"title":"Shimei Qiu","type":"authors"},{"authors":["Ka Man (Janet) Choi"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":-62135596800,"objectID":"b31739708a90c76f4e6b9e2614beac74","permalink":"/zh/author/ka-man-janet-choi/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/zh/author/ka-man-janet-choi/","section":"authors","summary":"","tags":null,"title":"Ka Man (Janet) Choi","type":"authors"},{"authors":["Peixuan Xiong"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":-62135596800,"objectID":"c984ed7acbcaa1d4745798d53fb8af70","permalink":"/zh/author/peixuan-xiong/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/zh/author/peixuan-xiong/","section":"authors","summary":"","tags":null,"title":"Peixuan Xiong","type":"authors"},{"authors":["Qiongyan Chen"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":-62135596800,"objectID":"c5abcd5cdfa3f33f2b09ce6af91d8375","permalink":"/zh/author/qiongyan-chen/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/zh/author/qiongyan-chen/","section":"authors","summary":"","tags":null,"title":"Qiongyan Chen","type":"authors"},{"authors":["Shihan Fu"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":-62135596800,"objectID":"4b8dc945d42c76c5e55035263e382f2c","permalink":"/zh/author/shihan-fu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/zh/author/shihan-fu/","section":"authors","summary":"","tags":null,"title":"Shihan Fu","type":"authors"},{"authors":["Shixu Zhou"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":-62135596800,"objectID":"83d5cc5eb61f4c3090e76eef23ef8534","permalink":"/zh/author/shixu-zhou/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/zh/author/shixu-zhou/","section":"authors","summary":"","tags":null,"title":"Shixu Zhou","type":"authors"},{"authors":["Shumeng Zhang"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":-62135596800,"objectID":"dfacefd764097d0c31ea51c0c87ea645","permalink":"/zh/author/shumeng-zhang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/zh/author/shumeng-zhang/","section":"authors","summary":"","tags":null,"title":"Shumeng Zhang","type":"authors"},{"authors":["Xianyou Yang"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":-62135596800,"objectID":"cd648034ef671f25213ccebfd1c34034","permalink":"/zh/author/xianyou-yang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/zh/author/xianyou-yang/","section":"authors","summary":"","tags":null,"title":"Xianyou Yang","type":"authors"},{"authors":["Yining Peng"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":-62135596800,"objectID":"d7cab398cdc0b23dcf9ef132a37cdbbf","permalink":"/zh/author/yining-peng/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/zh/author/yining-peng/","section":"authors","summary":"","tags":null,"title":"Yining Peng","type":"authors"},{"authors":["Yuchao Zhuo"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":-62135596800,"objectID":"76389f40c7ad1f7d4a3ecc654baf8bd5","permalink":"/zh/author/yuchao-zhuo/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/zh/author/yuchao-zhuo/","section":"authors","summary":"","tags":null,"title":"Yuchao Zhuo","type":"authors"},{"authors":["Yucheng Liu"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":-62135596800,"objectID":"e90704ed37c90c0a3a2d1328af695b3f","permalink":"/zh/author/yucheng-liu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/zh/author/yucheng-liu/","section":"authors","summary":"","tags":null,"title":"Yucheng Liu","type":"authors"},{"authors":["Yukai Zhang"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":-62135596800,"objectID":"feee1b9fb2fb75d621d56d265bd1e870","permalink":"/zh/author/yukai-zhang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/zh/author/yukai-zhang/","section":"authors","summary":"","tags":null,"title":"Yukai Zhang","type":"authors"},{"authors":["Yuru Huang"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":-62135596800,"objectID":"2622d9d4025dd8af3bdcbc5edbad8780","permalink":"/zh/author/yuru-huang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/zh/author/yuru-huang/","section":"authors","summary":"","tags":null,"title":"Yuru Huang","type":"authors"},{"authors":["Zeyu Xiong"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":-62135596800,"objectID":"78d76c7925c6450749d5e64d6bad5a86","permalink":"/zh/author/zeyu-xiong/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/zh/author/zeyu-xiong/","section":"authors","summary":"","tags":null,"title":"Zeyu Xiong","type":"authors"},{"authors":["Ziyan Wang"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":-62135596800,"objectID":"be31ed81a95aeafbc302e23d1d482375","permalink":"/zh/author/ziyan-wang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/zh/author/ziyan-wang/","section":"authors","summary":"","tags":null,"title":"Ziyan Wang","type":"authors"},{"authors":["Jingling Zhang"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":-62135596800,"objectID":"21e0d1a1ca0dcbd807e9c84a6b453ab2","permalink":"/zh/author/jingling-zhang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/zh/author/jingling-zhang/","section":"authors","summary":"","tags":null,"title":"Jingling Zhang","type":"authors"},{"authors":["Lingyun (Julie) Zhu"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":-62135596800,"objectID":"755595727d917c3d38b649ef893ec0e7","permalink":"/zh/author/lingyun-julie-zhu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/zh/author/lingyun-julie-zhu/","section":"authors","summary":"","tags":null,"title":"Lingyun (Julie) Zhu","type":"authors"},{"authors":["Zhitong (Klara) Guan"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":-62135596800,"objectID":"ffbb4adbbb3c2b9694f4dfafb4ac1541","permalink":"/zh/author/zhitong-klara-guan/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/zh/author/zhitong-klara-guan/","section":"authors","summary":"","tags":null,"title":"Zhitong (Klara) Guan","type":"authors"},{"authors":["Qiwen Zhao"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":-62135596800,"objectID":"50b748768440ce8f8f99320ddd2d7aca","permalink":"/zh/author/qiwen-zhao/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/zh/author/qiwen-zhao/","section":"authors","summary":"","tags":null,"title":"Qiwen Zhao","type":"authors"},{"authors":["Vinita Tibdewal"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":-62135596800,"objectID":"e39b505f9ee5f0972f49c522cf84868f","permalink":"/zh/author/vinita-tibdewal/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/zh/author/vinita-tibdewal/","section":"authors","summary":"","tags":null,"title":"Vinita Tibdewal","type":"authors"},{"authors":["Apoorv Vekhande"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":-62135596800,"objectID":"f41f9632dc62eb1eb016a2324af45312","permalink":"/zh/author/apoorv-vekhande/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/zh/author/apoorv-vekhande/","section":"authors","summary":"","tags":null,"title":"Apoorv Vekhande","type":"authors"},{"authors":["Esha Shandilya"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":-62135596800,"objectID":"17fa6d7bcd4d114a9812f2945d005245","permalink":"/zh/author/esha-shandilya/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/zh/author/esha-shandilya/","section":"authors","summary":"","tags":null,"title":"Esha Shandilya","type":"authors"},{"authors":["Urvashi Kokate"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":-62135596800,"objectID":"12568e8b8672f6ad10219b09e363a78f","permalink":"/zh/author/urvashi-kokate/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/zh/author/urvashi-kokate/","section":"authors","summary":"","tags":null,"title":"Urvashi Kokate","type":"authors"},{"authors":["Vanny Chao"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":-62135596800,"objectID":"b13c94058f71e6c347c3f6ddf380f293","permalink":"/zh/author/vanny-chao/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/zh/author/vanny-chao/","section":"authors","summary":"","tags":null,"title":"Vanny Chao","type":"authors"},{"authors":["Andrea Batch","Yipeng Ji","Mingming Fan","Jian Zhao","Niklas Elmqvist"],"categories":null,"content":" ","date":1677888e3,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1677888e3,"objectID":"b6f7f2f842e92807cb33eed0c9ed2af3","permalink":"/zh/publication/tvcg-2023-uxsense/","publishdate":"2023-03-04T00:00:00Z","relpermalink":"/zh/publication/tvcg-2023-uxsense/","section":"publication","summary":"Analyzing user behavior from usability evaluation can be a challenging and time-consuming task, especially as the number of participants and the scale and complexity of the evaluation grows. We propose UXSENSE, a visual analytics system using machine learning methods to extract user behavior from audio and video recordings as parallel time-stamped data streams. Our implementation draws on pattern recognition, computer vision, natural language processing, and machine learning to extract user sentiment, actions, posture, spoken words, and other features from such recordings. These streams are visualized as parallel timelines in a web-based front-end, enabling the researcher to search, filter, and annotate data across time and space. We present the results of a user study involving professional UX researchers evaluating user data using uxSense. In fact, we used uxSense itself to evaluate their sessions.","tags":["Visualization","visual analytics","evaluation","video analytics","machine learning","deep learning","computer vision"],"title":"uxSense: Supporting User Experience Analysis with Visualization and Computer Vision","type":"publication"},{"authors":["Li Feng","Zeyu Xiong","Xinyi Li","Mingming Fan"],"categories":null,"content":" ","date":1677369600,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1677369600,"objectID":"4bd093797b19e8cda64d1eae59db7316","permalink":"/zh/publication/chi23-copractter/","publishdate":"2023-03-04T00:00:00Z","relpermalink":"/zh/publication/chi23-copractter/","section":"publication","summary":"口吃是一种语言障碍，它影响着全世界7000多万人，其中包括中国的1300万人。口吃给人造成了很多有害影响，例如使他们的自尊受挫等。尽管之前的工作已经探索了一些帮助口吃患者的方法，但它们主要集中在西方的背景下。我们通过初步研究，发现了中国口吃者的一些独特做法和他们面临的挑战。为此，我们设计了一个在线工具CoPracTter，该工具包含了：1）有针对性的诱导压力的练习场景，2）实时客观反馈，以及3）来自社区的个性化及时反馈。我们进一步用该工具进行了为期七天的部署研究（参与者共11人），以了解参与者如何利用这些关键功能。据我们所知，这是第一个该类型的口吃者交流训练辅助工具，并且是第一次同时对多名口吃者进行长时间的同时在线测试。结果表明，有针对性的训练场景个性化练习，结合来自支持性社群的及时反馈，有助于口吃者训练讲话流利度，保持积极的心态，并帮助他们面对类似的现实生活环境中的情景。","tags":["口吃","无障碍","田野调查","辅助技术"],"title":"CoPracTter: Toward Integrating Personalized Practice Scenarios, Timely Feedback and Social Support into An Online Support Tool for Coping with Stuttering in China","type":"publication"},{"authors":["Xiaoying Wei","Yizheng Gu","Emily Kuang","Xian Wang","Beiyan Cao","Xiaofu Jin","Mingming Fan"],"categories":null,"content":" ","date":1677283200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1677283200,"objectID":"ae740d39e6767b5f59302142dce672f8","permalink":"/zh/publication/chi23-vr-intergenerational-gap/","publishdate":"2023-03-03T00:00:00Z","relpermalink":"/zh/publication/chi23-vr-intergenerational-gap/","section":"publication","summary":"An optional shortened abstract","tags":["老龄化","虚拟现实","家庭","田野调查"],"title":"Bridging the Generational Gap : Exploring How Virtual Reality Supports Remote Communication Between Grandparents and Grandchildren","type":"publication"},{"authors":["Emily Kuang","Ruihuan Chen","Mingming Fan"],"categories":null,"content":" ","date":1677196800,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1677196800,"objectID":"10e2a31a6cb295a20cfee159101f2aa3","permalink":"/zh/publication/chi23-oa-typing/","publishdate":"2023-03-02T00:00:00Z","relpermalink":"/zh/publication/chi23-oa-typing/","section":"publication","summary":"老年人越来越多地采用超小屏设备，如智能手表。因为年龄或其他因素导致的运动灵活性下降，他们在超小屏设备上使用手势打字时，往往更容易出错（如点错键）。老人相对熟悉的9键（T9）键盘为每个键分配了更大的空间，降低了点错的概率，但是因为每个键上有多个连续字母，用户在打连续字母时必须中断他们的手势，低下打字效率和用户体验。为解决老人在超小屏幕上使用手势打字的这些局限，我们提出了一种新键盘设计。该设计利用目前打字方法未使用的键1了，动态的在键1上复制用户手指触摸到的前一个键上的字母。在使用新键盘设计时，用户在输入同一个键上的字母时，只需将手指划到键1，而不必抬起手指中断当前手势。我们邀请了12名老年人使用我们的新型键盘和另两种键盘设计（传统T9，基于抖动手势的输入法）。研究结果表明，我们的新型键盘设计在打字速度、KSPC、插入错误和每个单词删除等方面明显优于T9和基于抖动手势的输入法，同时与传统的T9达到了可比性能。我们还对12名年轻人重复了同样的打字任务研究。结果表明，新的T9的优势在老人和年轻人当中是一致的。我们进一步提供了打字错误分析和改进T9手势打字的设计考虑因素。","tags":["手势输入","小型触摸屏","老年人","T9 键盘"],"title":"Enhancing Older Adults’ Gesture Typing Experience Using the T9 Keyboard on Small Touchscreen Devices","type":"publication"},{"authors":["Emily Kuang","Ehsan Jahangirzadeh Soure","Mingming Fan","Jian Zhao","Kristen Shinohara"],"categories":null,"content":" ","date":1677110400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1677110400,"objectID":"29db3d87c71bbae890a8ad5b58ffcf41","permalink":"/zh/publication/chi23-ux-conversational-ai-assistant/","publishdate":"2023-03-01T00:00:00Z","relpermalink":"/zh/publication/chi23-ux-conversational-ai-assistant/","section":"publication","summary":"An optional shortened abstract","tags":["用户体验","可用性测试","人工智能协作","对话助手"],"title":"Collaboration with Conversational AI Assistants for UX Evaluation: Questions and How to Ask them (Voice vs. Text)","type":"publication"},{"authors":["Beiyan Cao","Changyang He","Muzhi Zhou","Mingming Fan"],"categories":null,"content":" ","date":1677024e3,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1677024e3,"objectID":"2359b132c3138341b357bdcd0c476fee","permalink":"/zh/publication/chi23-dhh-livestreaming/","publishdate":"2023-02-28T00:00:00Z","relpermalink":"/zh/publication/chi23-dhh-livestreaming/","section":"publication","summary":"了解直播平台对少数群体（如残疾人）的无障碍挑战，对提高这些平台的多样性和包容性至关重要。虽然之前的工作调查了有视力或听力损失的流媒体人的经历，但对于必须使用严重依赖音频的直播平台的失聪或听力困难的流媒体人的经历所知甚少。我们对聋哑人流媒体人进行了半结构化的采访，以了解他们为什么要进行直播，他们如何浏览直播平台以及相关的挑战。我们的研究结果显示，他们希望通过直播打破对DHH群体的刻板印象，以及手语、文字、唇语、背景音乐和观众特征等互动方式之间的激烈互动。主要的无障碍挑战包括缺乏实时字幕，手语阅读窗口小，以及手语的误读。我们提出了改善直播平台无障碍性的设计考虑。","tags":["聋哑人","直播","无障碍","社交媒体/线上社群"],"title":"Sparkling Silence: Practices and Challenges of Livestreaming Among Deaf or Hard of Hearing Streamers","type":"publication"},{"authors":["Yan Zhang","Ziang Li","Haole Guo","Luyao Wang","Qihe Chen","Wenjie Jiang","Mingming Fan","Guyue Zhou","Jiangtao Gong"],"categories":null,"content":" ","date":1676937600,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1676937600,"objectID":"6461af6f5edb553ee3e77bc1f0bed3cd","permalink":"/zh/publication/chi23-blv-robot-navigation/","publishdate":"2023-02-27T00:00:00Z","relpermalink":"/zh/publication/chi23-blv-robot-navigation/","section":"publication","summary":"Guiding robots, in the form of canes or cars, have recently been explored to assist blind and low vision (BLV) people. Such robots can provide full or partial autonomy when guiding. However, the pros and cons of different forms and autonomy for guiding robots remain unknown. We sought to fill this gap. We designed autonomy- switchable guiding robotic cane and car. We conducted a controlled lab-study (N=12) and a field study (N=9) on BLV. Results showed that full autonomy received better walking performance and sub- jective ratings in the controlled study, whereas participants used more partial autonomy in the natural environment as demanding more control. Besides, the car robot has demonstrated abilities to provide a higher sense of safety and navigation efficiency compared with the cane robot. Our findings offered empirical evidence about how the BLV community perceived different machine forms and autonomy, which can inform the design of assistive robots.","tags":["guiding robot","visual impairment","navigation","level of autonomy","machine form","control","trust","safety"],"title":"\"I am the follower, also the boss\": Exploring Different Levels of Autonomy and Machine Forms of Guiding Robots for the Visually Impaired","type":"publication"},{"authors":["Zisu Li","Chen Liang","Yuntao Wang","Yue Qin","Chun Yu","Yukang Yan","Mingming Fan","Yuanchun Shi"],"categories":null,"content":" ","date":1676851200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1676851200,"objectID":"c1eb82cd7c9d9c3309bee8701fd39f53","permalink":"/zh/publication/chi23-voice-hand-face/","publishdate":"2023-02-26T00:00:00Z","relpermalink":"/zh/publication/chi23-voice-hand-face/","section":"publication","summary":"伴随着语音进行的手势对于语音交互来说是必不可少的，它可以为交互目的传达补充语义，如唤醒状态和输入模式。在本文中，我们研究了用于语音交互的语音伴奏手势（VAHF）。我们的目标是手对脸的手势，因为这种手势与语音密切相关，并产生重要的声学特征（例如，阻碍语音传播）。我们进行了一项用户研究，以探索VAHF手势的设计空间，我们首先收集了候选手势，然后从不同的维度（如接触位置和类型）对其进行了结构分析，共输出了8种具有良好可用性和最少混淆的VAHF手势。为了促进VAHF手势的识别，我们提出了一种新的跨设备传感方法，利用商品设备（耳塞、手表和戒指）的异质渠道（声乐、超声波和IMU）的数据。我们的识别模型对3种手势的识别准确率达到97.3%，对8种手势（不包括 \"空 \"手势）的识别准确率达到91.5%，证明了其高度适用性。定量分析也阐明了每个传感器通道的识别能力和它们的不同组合。最后，我们说明了可行的用例及其设计原则，以证明我们的系统在各种情况下的适用性。","tags":["手势","声学传感","传感器融合"],"title":"Enabling Voice-Accompanying Hand-to-Face Gesture Recognition with Cross-Device Sensing","type":"publication"},{"authors":null,"categories":null,"content":"APEX人机交互团队的七篇论文被 ACM CHI 2023, 人机交互领域的顶级会议录用！\n人机交互顶会ACM CHI 2023今早公布了录用结果。我们APEX课题组有7篇论文被录用，其中有6篇的一作是范明明教授指导的博士生，当中有5位是一年级博士生。祝贺课题组的同学们和合作者们！我们被录用的工作贡献在两个大方向：1）提升科技对老年人和边缘障碍人群的可访问性，为实现信息无障碍的大目标努力；2）探索智能用户体验分析的理论与方法，改善传统人机交互中用户体验分析方法的不足。具体项目后续逐步分享给大家！\n","date":1673654400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1673654400,"objectID":"74020c3113e634cd254e5e5e68203e31","permalink":"/zh/post/chi2023/","publishdate":"2023-01-14T00:00:00Z","relpermalink":"/zh/post/chi2023/","section":"post","summary":"APEX人机交互团队的七篇论文被 ACM CHI 2023, 人机交互领域的顶级会议录用！\n","tags":null,"title":"七篇长论文被 CHI 2023 录用","type":"post"},{"authors":["Mingming Fan","Vinita Tibdewal","Qiwen Zhao","Lizhou Cao","Chao Peng","Runxuan Shu","Yujia Shan"],"categories":null,"content":" ","date":1671926400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1671926400,"objectID":"8c94c156c4c1e9787db6a6f3834d2c9d","permalink":"/zh/publication/iwc2022-oa-vr-game-ta/","publishdate":"2022-12-25T00:00:00Z","relpermalink":"/zh/publication/iwc2022-oa-vr-game-ta/","section":"publication","summary":"While virtual reality (VR) games are beneficial for older adults to improve their physical functions and cognitive abilities, VR research often does not include older adults. Our review of the proceedings of major HCI conferences (i.e. ASSETS, CHI, CHI PLAY, CSCW and DIS) between 2016 and 2020 shows that only three out of 352 VR-related papers involved older adults. Consequently, older adults tend to encounter user experience (UX) problems with VR. One common way to identify UX problems is to conduct usability testing with think-aloud (TA) protocols. As VR games tend to be perceptually and physically demanding, older adults might need to allocate more resources to VR content and interaction and thus have fewer resources for thinking aloud. This raises the question of whether TA protocols are still a viable approach to detecting UX problems of VR games for older adult participants. To answer this question, we conducted usability testing with older adults who played two common types of VR games (i.e. the exergame and experience game) using concurrent and retrospective TA protocols (i.e. CTA and RTA), which are widely used in the industry. We analyzed participants’ TA verbalizations and uncovered how different categories of verbalizations indicate UX problems. We further show how older adults perceived the effects of thinking aloud on their game experiences in two TA protocols and offer design implications.","tags":["older adults","think-aloud protocols","virtual reality","VR games","verbalization","UX problems","user experience"],"title":"Older Adults’ Concurrent and Retrospective Think-Aloud Verbalizations for Identifying User Experience Problems of VR Games","type":"publication"},{"authors":["Xiaofu Jin","Xiaozhu Hu","Xiaoying Wei","Mingming Fan"],"categories":null,"content":" ","date":167184e4,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":167184e4,"objectID":"d3271ee4d9809ceb5bf961a6927be1e6","permalink":"/zh/publication/imwut22-oa-interactive-guidance/","publishdate":"2022-12-24T00:00:00Z","relpermalink":"/zh/publication/imwut22-oa-interactive-guidance/","section":"publication","summary":"As smartphones are widely adopted, mobile applications (apps) are emerging to provide critical services such as food delivery and telemedicine. While bring convenience to everyday life, this trend may create barriers for older adults who tend to be less tech-savvy than young people. In-person or screen sharing support is helpful but limited by the help-givers’ availability. Video tutorials can be useful but require users to switch contexts between watching the tutorial and performing the corresponding actions in the app, which is cumbersome to do on a mobile phone. Although interactive tutorials have been shown to be promising, none was designed for older adults. Furthermore, the trial-and-error approach has been shown to be beneficial for older adults, but they often lack support to use the approach. Inspired by both interactive tutorials and trial-and-error approach, we designed an app-independent mobile service, Synapse, for help-givers to create a multimodal interactive tutorial on a smartphone and for help-receivers (e.g., older adults) to receive interactive guidance with trial-and-error support when they work on the same task. We conducted a user study with 18 older adults who were 60 and over. Our quantitative and qualitative results show that Synapse provided better support than the traditional video approach and enabled participants to feel more confident and motivated. Lastly, we present further design considerations to better support older adults with trial-and-error on smartphones.","tags":["interactive guidance","demonstration","trial-and-error","multi-modal older adults"],"title":"Synapse: Interactive Guidance by Demonstration with Trial-and-Error Support for Older Adults to Use Smartphone Apps","type":"publication"},{"authors":["Mingming Fan","Xianyou Yang","Tsz Tung Yu","Vera Q. Liao","Jian Zhao"],"categories":null,"content":" ","date":1671753600,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1671753600,"objectID":"03458b8a5b5ae4c9437527c77b145805","permalink":"/zh/publication/cscw22-hai-ux-evaluation/","publishdate":"2022-12-23T00:00:00Z","relpermalink":"/zh/publication/cscw22-hai-ux-evaluation/","section":"publication","summary":"Analyzing usability test videos is arduous. Although recent research showed the promise of AI in assisting with such tasks, it remains largely unknown how AI should be designed to facilitate effective collaboration between user experience (UX) evaluators and AI. Inspired by the concepts of agency and work context in human and AI collaboration literature, we studied two corresponding design factors for AI-assisted UX evaluation: explanations and synchronization. Explanations allow AI to further inform humans how it identifies UX problems from a usability test session; synchronization refers to the two ways humans and AI collaborate: synchronously and asynchronously. We iteratively designed a tool—AI Assistant—with four versions of UIs corresponding to the two levels of explanations (with/without) and synchronization (sync/async). By adopting a hybrid wizard-of-oz approach to simulating an AI with reasonable performance, we conducted a mixed-method study with 24 UX evaluators identifying UX problems from usability test videos using AI Assistant. Our quantitative and qualitative results show that AI with explanations, regardless of being presented synchronously or asynchronously, provided better support for UX evaluators’ analysis and was perceived more positively; when without explanations, synchronous AI better improved UX evaluators’ performance and engagement compared to the asynchronous AI. Lastly, we present the design implications for AI-assisted UX evaluation and facilitating more effective human-AI collaboration.","tags":["human-AI collaboration","user experience (UX)","AI-assisted UX evaluation","explainable AI","intelligent user interface (UI) design","synchronization","explanation","think-aloud usability test"],"title":"Human-AI Collaboration for UX Evaluation: Effects of Explanation and Synchronization","type":"publication"},{"authors":["Can Liu","Siying Hu","Li Feng","Mingming Fan"],"categories":null,"content":" ","date":1671667200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1671667200,"objectID":"31e83d3e744180f9223d501864b49057","permalink":"/zh/publication/cscw22-typist/","publishdate":"2022-12-22T00:00:00Z","relpermalink":"/zh/publication/cscw22-typist/","section":"publication","summary":"Voice dictation is increasingly used for text entry, especially in mobile scenarios. However, the speech-based experience gets disrupted when users must go back to a screen and keyboard to review and edit the text. While existing dictation systems focus on improving transcription and error correction, little is known about how to support speech input for the entire text creation process, including composition, reviewing and editing. We conducted an experiment in which ten pairs of participants took on the roles of authors and typists to work on a text authoring task. By analysing the natural language patterns of both authors and typists, we identified new challenges and opportunities for the design of future dictation interfaces, including the ambiguity of human dictation, the differences between audio-only and with screen, and various passive and active assistance that can potentially be provided by future systems.","tags":["dictation","speech","text input","authoring","role-play","intelligent interface"],"title":"Typist Experiment: an Investigation of Human-to-Human Dictation via Role-play to Inform Voice-based Text Authoring","type":"publication"},{"authors":["Xiaofu Jin","Mingming Fan"],"categories":null,"content":" ","date":1671580800,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1671580800,"objectID":"986c981b7c9a1f50f3cca1d7f58e253c","permalink":"/zh/publication/assets22-oa-banking/","publishdate":"2022-12-21T00:00:00Z","relpermalink":"/zh/publication/assets22-oa-banking/","section":"publication","summary":"Managing finances is crucial for older adults who are retired and may rely on savings to ensure their lives’ quality. As digital banking platforms (e.g., mobile apps, electronic payment) gradually replace physical ones, it is critical to understand how they adapt to digital banking and the potential frictions they experience. We conducted semi-structured interviews with 16 older adults in China, where the aging population is the largest and digital banking grows fast. We also interviewed bank employees to gain complementary perspectives of these help givers. Our findings show that older adults used both physical and digital platforms as an ecosystem based on perceived pros and cons. Perceived usefulness, self-confidence, and social influence were key motivators for learning digital banking. They experienced app-related (e.g., insufficient error-recovery support) and user-related challenges (e.g., trust, security and privacy concerns, low perceived self-efficacy) and developed coping strategies. We discuss design considerations to improve their banking experiences.","tags":["Older adults","elderly","seniors","aging","banking","virtual bank","electronic payment","mobile banking","accessibility","technology use","digital inclusion","digital equity"],"title":"\"I Used To Carry A Wallet, Now I Just Need To Carry My Phone\": Understanding Current Banking Practices and Challenges Among Older Adults in China","type":"publication"},{"authors":["Jin Tian","Yifan Cao","Lingyi Feng","Dongting Fu","Linping Yuan","Huaming Qu","Yang Wang","Mingming Fan*"],"categories":null,"content":" ","date":1671494400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1671494400,"objectID":"28373aa80465a337018b835015a7f6d5","permalink":"/zh/publication/ijhci-poeticar/","publishdate":"2022-12-20T00:00:00Z","relpermalink":"/zh/publication/ijhci-poeticar/","section":"publication","summary":"As a famed Chinese classical garden, the Jichang Garden was a constant inspiration to many poets in its hundreds of years’ history, who composed a rich body of poems—a valuable intangible cultural heritage. While tourists tend to pay attention to tangible natural scenery and historical architectures, they often neglect intangible cultural heritage—poems. We interviewed 23 tourists and found that augmented reality (AR) was viable for tourists to enjoy the physical scenery and the poetry simultaneously. We developed an initial prototype of PoeticAR, which presents poems based on physical scenery to enhance tourists’ cultural and aesthetic experience. We further revised the prototype based on the ideas generated from a workshop with 18 tourists. We conducted a between-subject user study with 30 tourists to compare PoeticAR with Video. Results showed that PoeticAR significantly motivated tourists’ interest in poems, enhanced the cultural and aesthetic tour experience in Jichang Garden, and increased awareness of Intangible Cultural Heritage of Cultural Heritage sites.","tags":null,"title":"PoeticAR: Reviving Traditional Poetry of the Heritage Site of Jichang Garden via Augmented Reality","type":"publication"},{"authors":["Esha Shandilya","Mingming Fan"],"categories":null,"content":" ","date":1671408e3,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1671408e3,"objectID":"bbf8ab61920a5b30628ae898e98dccf8","permalink":"/zh/publication/chinesechi2022-olderadults-ai/","publishdate":"2022-12-19T00:00:00Z","relpermalink":"/zh/publication/chinesechi2022-olderadults-ai/","section":"publication","summary":"Artificial intelligence (AI)-enabled everyday technologies could help address age-related challenges like physical impairments and cognitive decline. While recent research studied older adults’ experiences with specific AI-enabled products (e.g., conversational agents and assistive robots), it remains unknown how older adults perceive and experience current AI-enabled everyday technologies in general, which could impact their adoption of future AI-enabled products. We conducted a survey study (N=41) and semi-structured interviews (N=15) with older adults to understand their experiences and perceptions of AI. We found that older adults were enthusiastic about learning and using AI-enabled products, but they lacked learning avenues. Additionally, they worried when AI-enabled products outwitted their expectations, intruded on their privacy, or impacted their decision-making skills. Therefore, they held mixed views towards AI-enabled products such as AI, an aid, or an adversary. We conclude with design recommendations that make older adults feel inclusive, secure, and in control of their interactions with AIenabled products.","tags":["AI-enabled everyday technologies","Older Adults","Interview","Perceptions"],"title":"Understanding Older Adults' Perceptions and Challenges in Using AI-enabled Everyday Technologies","type":"publication"},{"authors":["Xiaoying Wei","Xiaofu Jin","Mingming Fan"],"categories":null,"content":" ","date":1671321600,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1671321600,"objectID":"a0af14f04dc14a81eeacd4671b9b44ab","permalink":"/zh/publication/chinesechi2022-lit-review-vr-communication/","publishdate":"2022-12-18T00:00:00Z","relpermalink":"/zh/publication/chinesechi2022-lit-review-vr-communication/","section":"publication","summary":"As virtual reality (VR) technologies have improved in the past decade, more research has investigated how they could support more effective communication in various contexts to improve collaboration and social connectedness. However, there was no literature to summarize the uniqueness VR provided and put forward guidance for designing social VR applications for better communication. To understand how VR has been designed and used to facilitate communication in different contexts, we conducted a systematic review of the studies investigating communication in social VR in the past ten years by following the PRISMA guidelines. We highlight current practices and challenges and identify research opportunities to improve the design of social VR to better support communication and make social VR more accessible.","tags":["Virtual Reality","Social VR","Communication","Evaluation"],"title":"Communication in Immersive Social Virtual Reality: A Systematic Review of 10 Years' Studies","type":"publication"},{"authors":["Xian Wang","Xiaoyu Wang","Mingming Fan","Lik-Hang Lee","Bertram E. Shi","Pan Hui"],"categories":null,"content":" ","date":1671235200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1671235200,"objectID":"e5311485675fe1694ea04db524e2b470","permalink":"/zh/publication/chinesechi2022-lit-review-vr-meditation/","publishdate":"2022-12-17T00:00:00Z","relpermalink":"/zh/publication/chinesechi2022-lit-review-vr-meditation/","section":"publication","summary":"Meditation, or mindfulness, is widely used to improve mental health. With the emergence of Virtual Reality technology, many studies have provided evidence that meditation with VR can bring health benefits. However, to our knowledge, there are no guidelines and comprehensive reviews in the literature on how to conduct such research in virtual reality. In order to understand the role of VR technology in meditation and future research opportunities, we conducted a systematic literature review in the IEEE and ACM databases. Our process yielded 19 eligible papers and we conducted a structured analysis. We understand the state-of-art of meditation type, design consideration and VR and technology through these papers and conclude research opportunities and challenges for the future.","tags":["Meditation","Mindfulness","Virtual Reality","Literature Review","Interaction","Metaverse"],"title":"Reducing Stress and Anxiety in the Metaverse: A Systematic Review of Meditation, Mindfulness and Virtual Reality","type":"publication"},{"authors":["Zhiyi Rong","Mo Zhou","Zhicong Lu","Mingming Fan"],"categories":null,"content":" ","date":1671148800,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1671148800,"objectID":"17cebd82c3ad453e9ddc3b013c397c36","permalink":"/zh/publication/dis22-blv-livestreaming/","publishdate":"2022-12-16T00:00:00Z","relpermalink":"/zh/publication/dis22-blv-livestreaming/","section":"publication","summary":"Blind or low vision (BLV) people were recently reported to be live streamers on the online platforms that employed content curation algorithms. Recent research uncovered perceived algorithmic biases suppressing the content created by marginalized populations (e.g., people of color, the LGBT+ community, and content creators of lower socioeconomic status). However, little is known about how BLV streamers, as a marginalized population , perceive the effects of the algorithms adopted by live streaming platforms. We interviewed BLV streamers (N=19) of Douyin — a popular live stream platform in China — to understand their perceptions of algorithms, perceived challenges, and mitigation strategies. Our findings show the perceived factors contributing to disadvantages under algorithmic evaluation of BLV streamers’ content (e.g., issues with filming and timely interaction with viewers) and perceived algorithmic suppression (e.g., content not amplified to sighted users but suppressed within the BLV community). Their mitigation strategies (e.g., not watching other BLV streamers’ shows) tended to be passive. We discuss design considerations to design a more inclusive and fair live streaming platform.","tags":["Algorithms","Algorithmic Experience","Perceptions of Algorithms","Accessibility","Individuals with Disabilities \u0026 Assistive Technologies","Social Media/Online Communities","Interview"],"title":" 'It Feels Like Being Locked In A Cage': Understanding Blind or Low Vision Streamers' Perceptions of Content Curation Algorithms","type":"publication"},{"authors":["Mingming Fan","Yiwen Wang","Yuni Xie","Franklin Li","Chunyang Chen"],"categories":null,"content":" ","date":1671062400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1671062400,"objectID":"ed9a70d23109aa1eeb23b5bda14aa9da","permalink":"/zh/publication/ijhci-olderadults-covid-vis/","publishdate":"2022-12-15T00:00:00Z","relpermalink":"/zh/publication/ijhci-olderadults-covid-vis/","section":"publication","summary":"Older adults have been hit disproportionally hard by the COVID-19 pandemic. One critical way for older adults to minimize the negative impact of COVID-19 and future pandemics is to stay informed about its latest information, which has been increasingly presented through online interactive visualizations (e.g., live dashboards and websites). Thus, it is imperative to understand how older adults interact with and comprehend online COVID-19 interactive visualizations and what challenges they might encounter to make such visualizations more accessible to older adults. We adopted a user-centered approach by inviting older adults to interact with COVID-19 interactive visualizations while at the same time verbalizing their thought processes using a think-aloud protocol. By analyzing their think-aloud verbalizations, we identified four types of thought processes representing how older adults comprehended the visualizations and uncovered the challenges they encountered. Furthermore, we also identified the challenges they encountered with seven common types of interaction techniques adopted by the visualizations. Based on the findings, we present design guidelines for making interactive visualizations more accessible to older adults.","tags":["Older adults","interactive visualization","COVID-19","accessibility","visualization comprehension","digital inequality","think aloud","aging","elderly","seniors"],"title":"Understanding How Older Adults Comprehend COVID-19 Interactive Visualizations via Think-Aloud Protocol","type":"publication"},{"authors":["Emily Kuang","Xiaofu Jin","Mingming Fan"],"categories":null,"content":" ","date":1651190400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1651190400,"objectID":"494bf3e0d26709bc420edd738a1d1496","permalink":"/zh/publication/chi22-ux-survey/","publishdate":"2022-03-12T00:00:00Z","relpermalink":"/zh/publication/chi22-ux-survey/","section":"publication","summary":"Analysis is a key part of usability testing where UX practitioners seek to identify usability problems and generate redesign suggestions. Although previous research reported how analysis was conducted, the findings were typically focused on individual analysis or based on a small number of professionals in specific geographic regions. We conducted an online international survey of 279 UX practitioners on their practices and challenges while collaborating during data analysis. We found that UX practitioners were often under time pressure to conduct analysis and adopted three modes of collaboration: independently analyze different portions of the data and then collaborate, collaboratively analyze the session with little or no independent analysis, and independently analyze the same set of data and then collaborate. Moreover, most encountered challenges related to lack of resources, disagreements with colleagues regarding usability problems, and difficulty merging analysis from multiple practitioners. We discuss design implications to better support collaborative data analysis.","tags":["User experience","Usability testing","Survey","Collaboration"],"title":"\"Merging Results Is No Easy Task\": An International Survey Study of Collaborative Data Analysis Practices Among UX Practitioners","type":"publication"},{"authors":["Wentao Lei","Mingming Fan","Juliann Thang"],"categories":null,"content":" ","date":1647216e3,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1647216e3,"objectID":"abe6e61d8dfd00b467123a0a6f975844","permalink":"/zh/publication/chi22-kuaidigui/","publishdate":"2022-03-14T00:00:00Z","relpermalink":"/zh/publication/chi22-kuaidigui/","section":"publication","summary":"With about 230 million packages delivered per day in 2020, fetching packages has become a routine for many city dwellers in China. When fetching packages, people usually need to go to collection sites of their apartment complexes or a KuaiDiGui, an increasingly popular type of self-service package pickup machine. However, little is known whether such processes are accessible to blind and low vision (BLV) city dwellers. We interviewed BLV people (N=20) living in a large metropolitan area in China to understand their practices and challenges of fetching packages. Our findings show that participants encountered difficulties in finding the collection site and localizing and recognizing their packages. When fetching packages from KuaiDiGuis, they had difficulty in identifying the correct KuaiDiGui, interacting with its touch screen, navigating the complex on-screen workflow, and opening the target compartment. We discuss design considerations to make the package fetching process more accessible to the BLV community.","tags":["Package delivery","KuaiDiGui","Blind and low vision","People with vision impairments","Qualitative study","Interview","China","Accessibility"],"title":" 'I Shake The Package To Check If It's Mine': A Study of Package Fetching Practices and Challenges of Blind and Low Vision People in China","type":"publication"},{"authors":["Wentao Lei","Mingming Fan","Juliann Thang"],"categories":null,"content":" ","date":1647129600,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1647129600,"objectID":"788ee98d0b7e9e2007cb10c2ed811bbe","permalink":"/zh/publication/chi22-userdefinedgestures/","publishdate":"2022-03-13T00:00:00Z","relpermalink":"/zh/publication/chi22-userdefinedgestures/","section":"publication","summary":"With about 230 million packages delivered per day in 2020, fetching packages has become a routine for many city dwellers in China. When fetching packages, people usually need to go to collection sites of their apartment complexes or a KuaiDiGui, an increasingly popular type of self-service package pickup machine. However, little is known whether such processes are accessible to blind and low vision (BLV) city dwellers. We interviewed BLV people (N=20) living in a large metropolitan area in China to understand their practices and challenges of fetching packages. Our findings show that participants encountered difficulties in finding the collection site and localizing and recognizing their packages. When fetching packages from KuaiDiGuis, they had difficulty in identifying the correct KuaiDiGui, interacting with its touch screen, navigating the complex on-screen workflow, and opening the target compartment. We discuss design considerations to make the package fetching process more accessible to the BLV community.","tags":["Package delivery","KuaiDiGui","Blind and low vision","People with vision impairments","Qualitative study","Interview","China","Accessibility"],"title":" 'I Don't Want People to Look At Me Differently': Designing User-Defined Above-the-Neck Gestures for People with Upper Body Motor Impairments","type":"publication"},{"authors":["Xian Xu","Leni Yang","David Yip","Mingming Fan","Zheng Wei","Huamin Qu"],"categories":null,"content":" ","date":1646956800,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1646956800,"objectID":"ec1f4f923381dee2db4affeaf76be77b","permalink":"/zh/publication/chi22-storytelling/","publishdate":"2022-03-11T00:00:00Z","relpermalink":"/zh/publication/chi22-storytelling/","section":"publication","summary":"Data videos are an increasingly popular storytelling form. The opening of a data video critically influences its success as the opening either attracts the audience to continue watching or bores them to abandon watching. However, little is known about how to create an attractive opening. We draw inspiration from the openings of famous films to facilitate designing data video openings. First, by analyzing over 200 films from several sources, we derived six primary cinematic opening styles adaptable to data videos. Then, we consulted eight experts from the film industry to formulate 28 guidelines. To validate the usability and effectiveness of the guidelines, we asked participants to create data video openings with and without the guidelines, which were then evaluated by experts and the general public. Results showed that the openings designed with the guidelines were perceived to be more attractive, and the guidelines were praised for clarity and inspiration.","tags":["Visualization","Storytelling","Interview","Lab Study","Data Video","Guideline"],"title":"From WOW to WHY: Guidelines for Creating the Opening of a Data Video with Cinematic Styles","type":"publication"},{"authors":["Esha Shandilya","Mingming Fan","Garreth Tigwell"],"categories":null,"content":" ","date":1646870400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1646870400,"objectID":"a155285ddb1a65d84e28863d38db18c9","permalink":"/zh/publication/chi22-non-textual-communication/","publishdate":"2022-03-10T00:00:00Z","relpermalink":"/zh/publication/chi22-non-textual-communication/","section":"publication","summary":"Virtual workspaces rapidly increased during the COVID-19 pandemic, and for many new collaborators, working remotely was their frst introduction to their colleagues. Building rapport is essential for a healthy work environment, and while this can be achieved through non-textual responses within chat-based systems (e.g., emoji, GIF, stickers, memes), those non-textual responses are typically associated with personal relationships and informal settings. We studied the experiences of new collaborators (questionnaire N=49; interview N=14) in using non-textual responses to communicate with unacquainted teams and the efect of non-textual responses on new collaborators’ interpersonal bonds. We found new collaborators selectively and progressively use non-textual responses to establish interpersonal bonds. Moreover, the use of non-textual responses has exposed several limitations when used on various platforms. We conclude with design recommendations such as expanding the scope of interpretable non-textual responses and reducing selection time.","tags":["Virtual workspaces","Computer-mediated Communication","Nontextual Communication"],"title":" 'I need to be professional until my new team uses emoji, GIFs, or memes first': New Collaborators’ Perspectives on Using Non-Textual Communication in Virtual Workspaces","type":"publication"},{"authors":["Mingming Fan","Zhen Li","Franklin Mingzhe Li"],"categories":null,"content":" ","date":1646784e3,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1646784e3,"objectID":"704154e77e1c01a48d3268928203ef83","permalink":"/zh/publication/cacm-eyelidgestures/","publishdate":"2022-03-09T00:00:00Z","relpermalink":"/zh/publication/cacm-eyelidgestures/","section":"publication","summary":"Blind or low vision (BLV) people were recently reported to be live streamers on the online platforms that employed content curation algorithms. Recent research uncovered perceived algorithmic biases suppressing the content created by marginalized populations (e.g., people of color, the LGBT+ community, and content creators of lower socioeconomic status). However, little is known about how BLV streamers, as a marginalized population , perceive the effects of the algorithms adopted by live streaming platforms. We interviewed BLV streamers (N=19) of Douyin — a popular live stream platform in China — to understand their perceptions of algorithms, perceived challenges, and mitigation strategies. Our findings show the perceived factors contributing to disadvantages under algorithmic evaluation of BLV streamers’ content (e.g., issues with filming and timely interaction with viewers) and perceived algorithmic suppression (e.g., content not amplified to sighted users but suppressed within the BLV community). Their mitigation strategies (e.g., not watching other BLV streamers’ shows) tended to be passive. We discuss design considerations to design a more inclusive and fair live streaming platform.","tags":["Algorithms","Algorithmic Experience","Perceptions of Algorithms","Accessibility","Individuals with Disabilities \u0026 Assistive Technologies","Social Media/Online Communities","Interview"],"title":"Eyelid Gestures for People with Motor Impairments","type":"publication"},{"authors":["Mingming Fan","Lingyun (Julie) Zhu"],"categories":null,"content":" ","date":1638921600,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1638921600,"objectID":"9c31a241fdec13d2c206d27b011c265c","permalink":"/zh/publication/verbalizations-chinese/","publishdate":"2021-12-08T00:00:00Z","relpermalink":"/zh/publication/verbalizations-chinese/","section":"publication","summary":"Subtle patterns in users’ think-aloud (TA) verbalizations (i.e., utterances) are shown to be telltale signs of user experience (UX) problems and used to build artificial intelligence (AI) models or AI-assisted tools to help UX evaluators identify UX problems automatically or semi-automatically. Despite the potential of such verbalization patterns, they were uncovered with native English speakers. As most people who speak English are non-native speakers, it is important to investigate whether similar patterns exist in non-native English speakers’ TA verbalizations. As a first step to answer this question, we conducted think-aloud usability testing with Chinese non-native English speakers and native English speakers using three common TA protocols. We compared their verbalizations and UX problems that they encountered to understand the effects of language and TA protocols. Our findings show that both language groups had similar amounts and proportions of verbalization categories, encountered similar problems, and had similar verbalization patterns that indicate UX problems. Furthermore, TA protocols did not significantly affect the correlations between verbalizations and problems. Based on the findings, we present three design implications for UX practitioners and the design of AI-assisted analysis tools.","tags":["language proficiency","think aloud protocols","verbalization","Chinese non-native English speakers"],"title":"Think-Aloud Verbalizations for Identifying User Experience Problems: Effects of Language Proficiency with Chinese Non-Native English Speakers","type":"publication"},{"authors":["Ehsan Jahangirzadeh Soure","Emily Kuang","Mingming Fan","Jian Zhao"],"categories":null,"content":" ","date":1638835200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1638835200,"objectID":"3b3032bada5f80a1c80edb1d8ba89e5e","permalink":"/zh/publication/coux/","publishdate":"2021-12-07T00:00:00Z","relpermalink":"/zh/publication/coux/","section":"publication","summary":"Reviewing a think-aloud video is both time-consuming and demanding as it requires UX (user experience) professionals to attend to many behavioral signals of the user in the video. Moreover, challenges arise when multiple UX professionals need to collaborate to reduce bias and errors. We propose a collaborative visual analytics tool, CoUX, to facilitate UX evaluators collectively reviewing think-aloud usability test videos of digital interfaces. CoUX seamlessly supports usability problem identification, annotation, and discussion in an integrated environment. To ease the discovery of usability problems, CoUX visualizes a set of problem-indicators based on acoustic, textual, and visual features extracted from the video and audio of a think-aloud session with machine learning. CoUX further enables collaboration amongst UX evaluators for logging, commenting, and consolidating the discovered problems with a chatbox-like user interface. We designed CoUX based on a formative study with two UX experts and insights derived from the literature. We conducted a user study with six pairs of UX practitioners on collaborative think-aloud video analysis tasks. The results indicate that CoUX is useful and effective in facilitating both problem identification and collaborative teamwork. We provide insights into how different features of CoUX were used to support both independent analysis and collaboration. Furthermore, our work highlights opportunities to improve collaborative usability test video analysis.","tags":["User experience","Video analysis","Visual analytics","Collaboration"],"title":"CoUX: Collaborative Visual Analysis of Think-Aloud Usability Test Videos for Digital Interfaces","type":"publication"},{"authors":["Chutian Jiang","Yanjun Chen","Mingming Fan","Liuping Wang","Luyao Shen","Nianlong Li","Wei Sun","Yu Zhang","Feng Tian","Teng Han"],"categories":null,"content":" ","date":1638748800,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1638748800,"objectID":"6631170314d11d2fcb278febc96a2104","permalink":"/zh/publication/vr-pain/","publishdate":"2021-12-06T00:00:00Z","relpermalink":"/zh/publication/vr-pain/","section":"publication","summary":"The imitation of pain sensation in Virtual Reality is considered valuable for safety education and training but has been seldom studied. This paper presents Douleur, a wearable haptic device that renders intensity-adjustable pain sensations with chemical stimulants. Different from mechanical, thermal, or electric stimulation, chemical-induced pain is more close to burning sensations and long-lasting. Douleur consists of a microfluidic platform that precisely emits capsaicin onto the skin and a microneedling component to help the stimulant penetrate the epidermis layer to activate the trigeminal nerve efficiently. Moreover, it embeds a Peltier module to apply the heating or cooling stimulus to the affected area to adjust the level of pain on the skin. To better understand how people would react to the chemical stimulant, we conducted a first study to quantify the enhancement of the sensation by changing the capsaicin concentration, skin temperature, and time and to determine suitable capsaicin concentration levels. In the second study, we demonstrated that Douleur could render a variety of pain sensations in corresponding virtual reality applications. In sum, Douleur is the first wearable prototype that leverages a combination of capsaicin and Peltier to induce rich pain sensations and opens up a wide range of applications for safety education and more.","tags":["Virtual reality","pain sensation","prototype","capsaicin","user experience","safety education"],"title":"Douleur: Creating Pain Sensation with Chemical Stimulant to Enhance User Experience in Virtual Reality","type":"publication"},{"authors":["Sen Chen","Chunyang Chen","Lingling Fan","Mingming Fan","Xian Zhan","Yang Liu"],"categories":null,"content":" ","date":1638662400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1638662400,"objectID":"b99c5f67aed646dc9f242f082429413b","permalink":"/zh/publication/appaccessibility/","publishdate":"2021-12-05T00:00:00Z","relpermalink":"/zh/publication/appaccessibility/","section":"publication","summary":"Mobile apps provide new opportunities to people with disabilities to act independently in the world. Following the law of theUS, EU, mobile OS vendors such as Google and Apple have included accessibility features in their mobile systems and provide a set of guidelines and toolsets for ensuring mobile app accessibility. Motivated by this trend, researchers have conducted empirical studies by using the inaccessibility issue rate of each page (i.e., screen level) to represent the characteristics of mobile app accessibility. However, there still lacks an empirical investigation directly focusing on the issues themselves (i.e., issue level) to unveil more fine-grained findings, due to the lack of an effective issue detection method and a relatively comprehensive dataset of issues. To fill in this literature gap, we first propose an automated app page exploration tool, named Xbot, to facilitate app accessibility testing and automatically collect accessibility issues by leveraging the instrumentation technique and static program analysis. Owing to the relatively high activity coverage (around 80%) achieved by Xbot when exploring apps, Xbot achieves better performance on accessibility issue collection than existing testing tools such as Google Monkey. With Xbot, we are able to collect a relatively comprehensive accessibility issue dataset and finally collect 86,767 issues from 2,270 unique apps including both closed-source and open-source apps, based on which we further carry out an empirical study from the perspective of accessibility issues themselves to investigate novel characteristics of accessibility issues. Specifically, we extensively investigate these issues by checking 1) the overall severity of issues with multiple criteria, 2) the in-depth relation between issue types and app categories, GUI component types, 3) the frequent issue patterns quantitatively, and 4) the fixing status of accessibility issues. Finally, we highlight some insights to the community and hope to raise the attention to maintaining mobile app accessibility for users especially the elderly and disabled.","tags":["移动端无障碍","实证研究","自动化无障碍测试","安卓应用","Xbot"],"title":"Accessibile or Not? An Empirical Investigation of Android App Accessibility","type":"publication"},{"authors":["Xiaofu Jin","Emily Kuang","Mingming Fan"],"categories":null,"content":" ","date":1638576e3,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1638576e3,"objectID":"192734fd3476986031179f08d9ee6303","permalink":"/zh/publication/oa-banking-survey/","publishdate":"2021-12-04T00:00:00Z","relpermalink":"/zh/publication/oa-banking-survey/","section":"publication","summary":"The banking industry has been integrating digital technologies globally. However, accepting new technologies is challenging in particular for older adults. We focus on older adults’ banking experiences in China, where digital transactions have been growing rapidly, to provide a perspective on how they adapt to this trend. We conducted an online survey with 155 older adults who are 60 or above (𝑀 = 70, 𝑆𝐷 = 9) from 18 provinces to explore their banking practices and challenges. Our results show that older adults conduct banking transactions frequently. However, few do so using digital platforms despite long wait times in physical banks. The main concerns reported by them are about security and usability. Nonetheless, they hold a positive attitude towards digital platforms (e.g., apps, virtual banks). Interestingly, age and gender have significant effects on particular banking behaviors. We discuss our findings in the context of prior studies and highlight design opportunities for improving banking accessibility for older adults","tags":["older adults","elderly","seniors","aging","banking","electronic payment","accessibility","technology use","digital inclusion","digital equity"],"title":"\"Too Old to Bank Digitally?\": A Survey of Banking Practices and Challenges among Older Adults in China","type":"publication"},{"authors":["Mingming Fan","Qiwen Zhao","Vinita Tibdewal"],"categories":null,"content":" ","date":1638489600,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1638489600,"objectID":"cd52de6a6cf29ec70f62d2a4e7029a3a","permalink":"/zh/publication/oa-ta/","publishdate":"2021-12-03T00:00:00Z","relpermalink":"/zh/publication/oa-ta/","section":"publication","summary":"Subtle patterns in users’ think-aloud (TA) verbalizations and speech features are shown to be telltale signs of User Experience (UX) problems. However, such patterns were uncovered among young adults. Whether such patterns apply for older adults remains unknown. We conducted TA usability testing with older adults using physical and digital products. We analyzed their verbalizations, extracted speech features, identified UX problems, and uncovered the patterns that indicate UX problems. Our results show that when older adults encounter problems, their verbalizations tend to include observations (remarks), negations, question words and words with negative sentiments; and their voices tend to include high loudness, high pitch and high speech rate. We compare these subtle patterns with those of young adults uncovered in recent studies and discuss the implications of these patterns for the design of Human-AI collaborative UX analysis tools to better pinpoint UX problems.","tags":["older adults","elderly","seniors","think-aloud","verbalization","speech features","UX problems","usability testing","remote usability testing","AI-assisted UX analysis, human-AI collaboration for UX analysis"],"title":"Older Adults' Think-Aloud Verbalizations and Speech Features for Identifying User Experience Problems","type":"publication"},{"authors":["Franklin Mingzhe Li","Di Laura Chen","Mingming Fan","Khai N. Truong"],"categories":null,"content":" ","date":1620691200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1620691200,"objectID":"5763d55757a2392d716eec27bdf45843","permalink":"/zh/publication/at-china/","publishdate":"2021-05-11T00:00:00Z","relpermalink":"/zh/publication/at-china/","section":"publication","summary":"Despite the potential benefits of assistive technologies (ATs) for people with various disabilities, only around 7% of Chinese with disabilities have had an opportunity to use ATs. Even for those who have used ATs, the abandonment rate was high. Although China has the world’s largest population with disabilities, prior research exploring how ATs are used and perceived, and why ATs are abandoned have been conducted primarily in North America and Europe. In this paper, we present an interview study conducted in China with 26 people with various disabilities to understand their practices, challenges, perceptions, and misperceptions of using ATs. From the study, we learned about factors that influence AT adoption practices (e.g., misuse of accessible infrastructure, issues with replicating existing commercial ATs), challenges using ATs in social interactions (e.g., Chinese stigma), and misperceptions about ATs (e.g., ATs should overcome inaccessible social infrastructures). Informed by the findings, we derive a set of design considerations to bridge the existing gaps in AT design (e.g., manual vs. electronic ATs) and to improve ATs’ social acceptability in China.","tags":["辅助科技","无障碍","残障人士","定性实验","采访","中国","误解"],"title":"\"I Choose Assistive Devices That Save My Face\": A Study on Perceptions of Accessibility and Assistive Technology Use Conducted in China","type":"publication"},{"authors":["Nianlong Li","Zhengquan Zhang","Can Liu","Zengyao Yang","Yinan Fu","Feng Tian","Teng Han","Mingming Fan"],"categories":null,"content":" ","date":1620604800,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1620604800,"objectID":"a3d1dff263a9ae7b81da867aa802901c","permalink":"/zh/publication/vmirror/","publishdate":"2021-05-10T00:00:00Z","relpermalink":"/zh/publication/vmirror/","section":"publication","summary":"Interacting with out of reach or occluded VR objects can be cumbersome. Although users can change their position and orientation, such as via teleporting, to help observe and select, doing so frequently may cause loss of spatial orientation or motion sickness. We present vMirror, an interactive widget leveraging reflection of mirrors to observe and select distant or occluded objects. We first designed interaction techniques for placing mirrors and interacting with objects through mirrors. We then conducted a formative study to explore a semi-automated mirror placement method with manual adjustments. Next, we conducted a target-selection experiment to measure the effect of the mirror’s orientation on users’ performance. Results showed that vMirror can be as efficient as direct target selection for most mirror orientations. We further compared vMirror with teleport technique in a virtual treasure hunt game and measured participants’ task performance and subjective experiences. Finally, we discuss vMirorr user experience and present future directions.","tags":["Virtual mirror","vMirror","Virtual Reality","VR","target selection","occlusion","out of reach","DOF","raycasting"],"title":"vMirror: Enhancing the Interaction with Occluded or Distant Objects in VR with Virtual Mirrors","type":"publication"},{"authors":["Jian Zhao","Mingming Fan","Mi Feng"],"categories":null,"content":" ","date":1620518400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1620518400,"objectID":"1a306f0ab3c2198d572dd3bfa455efc9","permalink":"/zh/publication/chartseer/","publishdate":"2021-05-09T00:00:00Z","relpermalink":"/zh/publication/chartseer/","section":"publication","summary":"During exploratory visual analysis (EVA), analysts need to continually determine which subsequent activities to perform, such as which data variables to explore or how to present data variables visually. Due to the vast combinations of data variables and visual encodings that are possible, it is often challenging to make such decisions. Further, while performing local explorations, analysts often fail to attend to the holistic picture that is emerging from their analysis, leading them to improperly steer their EVA. These issues become even more impactful in the real world analysis scenarios where EVA occurs in multiple asynchronous sessions that could be completed by one or more analysts. To address these challenges, this work proposes ChartSeer, a system that uses machine intelligence to enable analysts to visually monitor the current state of an EVA and effectively identify future activities to perform. ChartSeer utilizes deep learning techniques to characterize analyst-created data charts to generate visual summaries and recommend appropriate charts for further exploration based on user interactions. A case study was first conducted to demonstrate the usage of ChartSeer in practice, followed by a controlled study to compare ChartSeer’s performance with a baseline during EVA tasks. The results demonstrated that ChartSeer enables analysts to adequately understand current EVA status and advance their analysis by creating charts with increased coverage and visual encoding diversity.","tags":["Exploratory visual analysis","interactive steering","visualization recommendation","machine learning"],"title":"ChartSeer: Interactive Steering Exploratory Visual Analysis with Machine Intelligence","type":"publication"},{"authors":["Mingming Fan","Zhen Li","Franklin Mingzhe Li"],"categories":null,"content":" ","date":1605052800,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1605052800,"objectID":"ceb2ec8ff8dc21593b2d465ab9eee84e","permalink":"/zh/publication/eyelidgestures4assets/","publishdate":"2020-11-11T00:00:00Z","relpermalink":"/zh/publication/eyelidgestures4assets/","section":"publication","summary":"Eye-based interactions for people with motor impairments have often used clunky or specialized equipment (e.g., eye-trackers with non-mobile computers) and primarily focused on gaze and blinks. However, two eyelids can open and close for different duration in different orders to form various eyelid gestures. We take a first step to design, detect, and evaluate a set of eyelid gestures for people with motor impairments on mobile devices. We present an algorithm to detect nine eyelid gestures on smartphones in realtime and evaluate it with twelve able-bodied people and four people with severe motor impairments in two studies. The results of the study with people with motor-impairments show that the algorithm can detect the gestures with .76 and .69 overall accuracy in userdependent and user-independent evaluations. Moreover, we design and evaluate a gesture mapping scheme allowing for navigating mobile applications only using eyelid gestures. Finally, we present recommendations for designing and using eyelid gestures for people with motor impairments.","tags":["eyelid gestures","people with motor impairments","mobile interaction"],"title":"Eyelid Gestures on Mobile Devices for People with Motor Impairments","type":"publication"},{"authors":["Teng Han","Sirui Wang","Sijia Wang","Xiangmin Fan","Jie Liu","Feng Tian","and Mingming Fan"],"categories":null,"content":" ","date":1589068800,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1589068800,"objectID":"bc8cb53a1206ce56182494c4e5c42c52","permalink":"/zh/publication/mouille/","publishdate":"2020-05-10T00:00:00Z","relpermalink":"/zh/publication/mouille/","section":"publication","summary":"Providing users with rich sensations is beneficial to enhance their immersion in Virtual Reality (VR) environments. Wetness is one such imperative sensation that helps humans avoid health-risking conditions and adjust grip force when interacting with objects. Recently, researchers have begun to explore ways to create wetness illusion, primarily on face or body skin. In this work, we extended this line of research by creating wetness illusion on users’ fingertips. We first conducted a user study to understand the effect of thermal and tactile feedback on users’ perceived wetness sensation. Informed by the findings, we designed and evaluated a prototype—Mouillé—that provides various levels of wetness illusion on fingertips for both hard and soft items of varied weights when users squeeze, lift, or scratch it. We further presented demo applications that simulate an ice cube, iced cola bottle, a wet sponge, etc, to show its use in VR.","tags":["Wetness illusion","virtual reality","prototype","user study"],"title":"Mouillé: Exploring Wetness Illusion on Fingertips to Enhance Immersive Experience in VR","type":"publication"},{"authors":["Mingming Fan","Yue Li","Khai N. Truong"],"categories":null,"content":" ","date":1588982400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1588982400,"objectID":"22d6a7e31762e034ebd42aeb076e894e","permalink":"/zh/publication/uxproblemsdetection/","publishdate":"2020-05-09T00:00:00Z","relpermalink":"/zh/publication/uxproblemsdetection/","section":"publication","summary":"Think-aloud protocols are a highly valued usability testing method for identifying usability problems. Despite the value of conducting think-aloud usability test sessions, analyzing think-aloud sessions is often timeconsuming and labor-intensive. Consequently, previous research has urged the community to develop techniques to support fast-paced analysis. In this work, we took the first step to design and evaluate machine learning (ML) models to automatically detect usability problem encounters based on users’ verbalization and speech features in think-aloud sessions. Inspired by recent research that shows subtle patterns in users’ verbalizations and speech features tend to occur when they encounter problems, we examined whether these patterns can be utilized to improve the automatic detection of usability problems. We first conducted and recorded think-aloud sessions and then examined the effect of different input features, ML models, test products, and users on usability problem encounters detection. Our work uncovers several technical and user interface design challenges and sets a baseline for automating usability problem detection and integrating such automation into UX practitioners’ workflow.","tags":["Think aloud","usability problem","verbalization","speech features","machine learning","user experience (UX)","AI-assisted UX analysis method"],"title":"Automatic Detection of Usability Problem Encounters in Think-Aloud Sessions","type":"publication"},{"authors":["Mingming Fan","Serina Shi","Khai N. Truong"],"categories":null,"content":" ","date":1578700800,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1578700800,"objectID":"d9f18210948eddea4cb54c54d57c9d01","permalink":"/zh/publication/ta-industry/","publishdate":"2020-01-11T00:00:00Z","relpermalink":"/zh/publication/ta-industry/","section":"publication","summary":"Think-aloud protocols are one of the classic methods often taught in universities for training UX designers and researchers. Although previous research reported how these protocols were used in industry, the findings were typically based on the practices of a small number of professionals in specific geographic regions or on studies conducted years ago. As UX practices continuously evolve to address new challenges emerging in industry, it is important to understand the challenges faced by current UX practitioners around the world when using think-aloud protocols. Such an understanding is beneficial for UX professionals to reflect on and learn from the UX community’s practices. It is also invaluable for academic researchers and educators to understand the challenges faced by professionals when carrying out the protocols in a wide range of practical contexts and to better explore methods to address these challenges. We conducted an international survey study with UX professionals in various sized companies around the world. We found that think-aloud protocols are widely and almost equally used in controlled lab studies and remote usability testing; concurrent protocols are more popular than retrospective protocols. Most UX practitioners probe participants during test sessions, explicitly request them to verbalize particular types of content, and do not administer practice sessions. The findings also offer insights on practices and challenges in analyzing think-aloud sessions. In sum, UX practitioners often deal with the tension between validity and efficiency in their analysis and demand better fast-paced and reliable analysis methods than merely reviewing observation notes or session recordings.","tags":["Think-aloud protocols","usability test","user experience","industry practices and challenges international survey"],"title":"Practices and Challenges of Using Think-aloud Protocols in Industry: An International Survey","type":"publication"},{"authors":["Mingming Fan","Ke Wu","Jian Zhao","Yue Li","Winter Wei","Khai N. Truong"],"categories":null,"content":" ","date":1556496e3,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1556496e3,"objectID":"91f6625be2363daad13832f718c9e5cb","permalink":"/zh/publication/vista/","publishdate":"2019-04-29T00:00:00Z","relpermalink":"/zh/publication/vista/","section":"publication","summary":"Think-aloud protocols are widely used by user experience (UX) practitioners in usability testing to uncover issues in user interface design. It is often arduous to analyze large amounts of recorded think-aloud sessions and few UX practitioners have an opportunity to get a second perspective during their analysis due to time and resource constraints. Inspired by the recent research that shows subtle verbalization and speech patterns tend to occur when users encounter usability problems, we take the first step to design and evaluate an intelligent visual analytics tool that leverages such patterns to identify usability problem encounters and present them to UX practitioners to assist their analysis. We first conducted and recorded think-aloud sessions, and then extracted textual and acoustic features from the recordings and trained machine learning (ML) models to detect problem encounters. Next, we iteratively designed and developed a visual analytics tool, VisTA, which enables dynamic investigation of think-aloud sessions with a timeline visualization of ML predictions and input features. We conducted a between-subjects laboratory study to compare three conditions, i.e., VisTA, VisTASimple (no visualization of the ML’s input features), and Baseline (no ML information at all), with 30 UX professionals. The findings show that UX professionals identified more problem encounters when using VisTA than Baseline by leveraging the problem visualization as an overview, anticipations, and anchors as well as the feature visualization as a means to understand what ML considers and omits. Our findings also provide insights into how they treated ML, dealt with (dis)agreement with ML, and reviewed the videos (i.e., play, pause, and rewind).","tags":["Think aloud","visual analytics","machine intelligence","user study","usability problems","session review behavior","UX practices"],"title":"VisTA: Integrating Machine Intelligence with Visualization to Support the Investigation of Think-Aloud Sessions","type":"publication"},{"authors":["Mingming Fan","Jinglan Lin","Christina Chung","Khai N. Truong"],"categories":null,"content":" ","date":1556409600,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1556409600,"objectID":"f210808ba4d68dafa759f2b1362d2c14","permalink":"/zh/publication/verbalizationproblems/","publishdate":"2019-04-28T00:00:00Z","relpermalink":"/zh/publication/verbalizationproblems/","section":"publication","summary":"The concurrent think-aloud protocol—in which participants verbalize their thoughts when performing tasks - is a widely employed approach in usability testing. Despite its value, analyzing think-aloud sessions can be onerous because it often entails assessing all of a user’s verbalizations. This has motivated previous research on developing categories to segment verbalizations into manageable units of analysis. However, the way in which a category might relate to usability problems is currently unclear. In this research, we sought to address this gap in our understanding. We also studied how speech features might relate to usability problems. Through two studies, this research demonstrates that certain patterns of verbalizations are more telling of usability problems than others and that these patterns are robust to different types of test products (i.e., physical devices and digital systems), access to different types of information (i.e., video and audio modality), and the presence or absence of a visualization of verbalizations. The implication is that the verbalization and speech patterns can potentially reduce the time and effort required for analysis by enabling evaluators to focus more on the important aspects of a user’s verbalizations. The patterns could also potentially be used to inform the design of systems to automatically detect when in the recorded think-aloud sessions users experience problems.","tags":["Concurrent think-aloud","usability testing","verbalization","verbalization categories","speech features","silence","verbal fillers","sentiment","speech rate","loudness","pitch","usability problems"],"title":"Concurrent Think-Aloud Verbalizations and Usability Problems","type":"publication"},{"authors":["Franklin Mingzhe Li","Di Laura Chen","Mingming Fan","Khai N. Truong"],"categories":null,"content":" ","date":1556323200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1556323200,"objectID":"eb49d05436580a1fd66d973cdf441eb3","permalink":"/zh/publication/fmt/","publishdate":"2019-04-27T00:00:00Z","relpermalink":"/zh/publication/fmt/","section":"publication","summary":"Older adults sometimes forget about whether or not they have completed routine actions and the states of objects that they have interacted with (e.g., the kitchen stove is on or off). In this work, we explore whether video clips captured from a bodyworn camera every time objects of interest are found within its field of view can help older adults determine if they have completed certain actions with these objects and what their states are. We designed FMT (“Fiducial Marker Tracker” —a real-time capture and access application that opportunistically captures video clips of objects the user interacts with. To do this, the user places fiducial markers close to objects which would be captured when the marker enters the user’s body-worn camera’s field of view. We examine and discuss what objects this system would be best suited to track, and the usefulness and usability of this approach. FMT successfully captured direct interactions with an object at an average rate of 75.6% across all participants (SD = 9.9%). Our results also reveal how, what, and why users would use such a system for help.","tags":["Memory aid","older adults","wearable camera","object tracking","technology probe"],"title":"FMT: A Wearable Camera-Based Object Tracking Memory Aid for Older Adults","type":"publication"},{"authors":["Zhicong Lu","Michelle Annett","Mingming Fan","Daniel Wigdor"],"categories":null,"content":" ","date":1556236800,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1556236800,"objectID":"2ffc9780e414c06f0d1c6a79a8e86e66","permalink":"/zh/publication/chi19-ich/","publishdate":"2019-04-26T00:00:00Z","relpermalink":"/zh/publication/chi19-ich/","section":"publication","summary":"Globalization has led to the destruction of many cultural practices, expressions, and knowledge found within local communities. These practices, defined by UNESCO as Intangible Cultural Heritage (ICH), have been identified, promoted, and safeguarded by nations, academia, organizations and local communities to varying degrees. Despite such efforts, many practices are still in danger of being lost or forgotten forever. With the increased popularity of livestreaming in China, some streamers have begun to use livestreaming to showcase and promote ICH activities. To better understand the practices, opportunities, and challenges inherent in sharing and safeguarding ICH through livestreaming, we interviewed 10 streamers and 8 viewers from China. Through our qualitative investigation, we found that ICH streamers had altruistic motivations and engaged with viewers using multiple modalities beyond livestreams. We also found that livestreaming encouraged real-time interaction and sociality, while non-live curated videos attracted attention from a broader audience and assisted in the archiving of knowledge.","tags":["Livestreaming","intangible cultural heritage","user engagement","social media","cultural preservation"],"title":"\"I feel it is my responsibility to stream”: Streaming and Engaging with Intangible Cultural Heritage through Livestreaming","type":"publication"},{"authors":["Teng Han","Jie Liu","Khalad Hasan","Mingming Fan","Junhyeok Kim","Jiannan Li","Xiangmin Fan","Feng Tian","Edward Lank","Pourang Irani"],"categories":null,"content":" ","date":1556150400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1556150400,"objectID":"c9bf1c74a14ac0ed22de8bd9021ed0ab","permalink":"/zh/publication/pinchlist/","publishdate":"2019-04-25T00:00:00Z","relpermalink":"/zh/publication/pinchlist/","section":"publication","summary":"Intensive exploration and navigation of hierarchical lists on smartphones can be tedious and time-consuming as it often requires users to frequently switch between multiple views. To overcome this limitation, we present PinchList, a novel interaction design that leverages pinch gestures to support seamless exploration of multi-level list items in hierarchical views. With PinchList, sub-lists are accessed with a pinch-out gesture whereas a pinch-in gesture navigates back to the previous level. Additionally, pinch and flick gestures are used to navigate lists consisting of more than two levels. We conduct a user study to refine the design parameters of PinchList such as a suitable item size, and quantitatively evaluate the target acquisition performance using pinch-in/out gestures in both scrolling and non-scrolling conditions. In a second study, we compare the performance of PinchList in a hierarchal navigation task with two commonly used touch interfaces for list browsing: pagination and expand-and-collapse interfaces. The results reveal that PinchList is significantly faster than other two interfaces in accessing items located in hierarchical list views. Finally, we demonstrate that PinchList enables a host of novel applications in list-based interaction.","tags":["Pinch gesture","Hierarchical list navigation","touchscreen"],"title":"PinchList: Leveraging Pinch Gestures for Hierarchical List Navigation on Smartphones","type":"publication"},{"authors":["Shang Ma","Qiong Liu","Mingming Fan","Phillip Sheu"],"categories":null,"content":" ","date":1556064e3,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1556064e3,"objectID":"0e57527ef1bdf58ef5b4fb0d5ff88fa2","permalink":"/zh/publication/3d-finger-tracking/","publishdate":"2019-04-24T00:00:00Z","relpermalink":"/zh/publication/3d-finger-tracking/","section":"publication","summary":"Recent advances on the Internet of Things (IoT) lead to an explosion of physical objects being connected to the Internet. These objects sense, compute, interpret what is occurring within themselves and the world, and preferably interact with users. In this work, we present a visible light-enabled finger tracking technique allowing users to perform freestyle multi-touch gestures on everyday object’s surface. By projecting encoded patterns onto an object’s surface (e.g. paper, display, or table) through a projector, and localizing the user’s fingers with light sensors, the proposed system offers users a richer interactive space than the device’s existing interfaces. More importantly, results from our experiments indicate that this system can localize ten fingers simultaneously with an accuracy of 1.7 mm and an refresh rate of 84 Hz with only 31 ms delay on WiFi or 23 ms delay on serial communication, easily supporting multi-finger gesture interaction on everyday objects. We also develop two example applications to demonstrate possible scenarios. Finally, we conduct a preliminary exploration of 3D depth inference using the same setup and achieve 2.43 cm depth estimation accuracy.","tags":["Coded visible light","Finger tracking","Projector-based interaction","Object augmentation"],"title":"Projected Visible Light for 3D Finger Tracking and Device Augmentation on Everyday Objects","type":"publication"},{"authors":["Zhicong Lu","Mingming Fan","Yun Wang","Jian Zhao","Michelle Annett","Daniel Wigdor"],"categories":null,"content":" ","date":1545523200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1545523200,"objectID":"ac162739bbaf786a218f9319eb3bc044","permalink":"/zh/publication/inkplanner/","publishdate":"2018-12-23T00:00:00Z","relpermalink":"/zh/publication/inkplanner/","section":"publication","summary":"Prewriting is the process of generating and organizing ideas before drafting a document. Although often overlooked by novice writers and writing tool developers, prewriting is a critical process that improves the quality of a final document. To better understand current prewriting practices, we first conducted interviews with writing learners and experts. Based on the learners’ needs and experts’ recommendations, we then designed and developed InkPlanner, a novel pen and touch visualization tool that allows writers to utilize visual diagramming for ideation during prewriting. InkPlanner further allows writers to sort their ideas into a logical and sequential narrative by using a novel widget— NarrativeLine. Using a NarrativeLine, InkPlanner can automatically generate a document outline to guide later drafting exercises. Inkplanner is powered by machine-generated semantic and structural suggestions that are curated from various texts. To qualitatively review the tool and understand how writers use InkPlanner for prewriting, two writing experts were interviewed and a user study was conducted with university students. The results demonstrated that InkPlanner encouraged writers to generate more diverse ideas and also enabled them to think more strategically about how to organize their ideas for later drafting.","tags":["Writing","prewriting","diagraming","content and structure recommendation","pen and touch interfaces"],"title":"InkPlanner: Supporting Prewriting via Intelligent Visual Diagramming","type":"publication"},{"authors":["Mingming Fan","Khai N. Truong"],"categories":null,"content":" ","date":1524355200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1524355200,"objectID":"2e6be20764d1f9b0831e27ba1f9586f6","permalink":"/zh/publication/seniorguidelines/","publishdate":"2018-04-22T00:00:00Z","relpermalink":"/zh/publication/seniorguidelines/","section":"publication","summary":"Although older adults feel generally positive about technologies, many face difficulties when using them and need support during the process. One common form of support is the product instructions that come with devices. Unfortunately, when using them, older adults often feel confused, overwhelmed, or frustrated. In this work, we sought to address the issues that affect older adults’ ability to successfully complete tasks using product instructions. By observing how older adults used the product instructions of various devices and how they made modifications to simplify the use of the instructions, we identified 11 guidelines for creating senior-friendly product instructions. We validated the usability and effectiveness of the guidelines by evaluating how older adults used instruction manuals that were modified to adhere to these guidelines against the originals and those that were modified by interaction design researchers. Results show that, overall, participants had the highest task success rate and lowest task completion time when using guideline-modified user instructions. Participants also perceived these instructions to be the most helpful, the easiest to follow, the most complete, and the most concise among the three. We also compared the guidelines derived from this research to existing documentation guidelines and discussed potential challenges of applying them.","tags":["Guidelines","seniors","older adults","product instructions","user manuals","user-friendly","senior-friendly","user-centered design","technology support","instruction design"],"title":"Guidelines for Creating Senior-Friendly Product Instructions","type":"publication"},{"authors":["Franklin Li","Mingming Fan","Khai N. Truong"],"categories":null,"content":" ","date":1503273600,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1503273600,"objectID":"496bf68d59186722f837b0aeb3d507bf","permalink":"/zh/publication/braillesketch/","publishdate":"2017-08-21T00:00:00Z","relpermalink":"/zh/publication/braillesketch/","section":"publication","summary":"In this paper, we present BrailleSketch, a gesture-based text input method on touchscreen smartphones for people with visual impairments. To input a letter with BrailleSketch, a user simply sketches a gesture that passes through all dots in the corresponding Braille code for that letter. BrailleSketch allows users to place their fingers anywhere on the screen to begin a gesture and draw the Braille code in many ways. To encourage users to type faster, BrailleSketch does not provide immediate letter-level audio feedback but instead provides word-level audio feedback. It uses an auto-correction algorithm to correct typing errors. Our evaluation of the method with ten participants with visual impairments who each completed five typing sessions shows that BrailleSketch supports a text entry speed of 14.53 word per min (wpm) with 10.6% error. Moreover, our data suggests that the speed had not begun to plateau yet by the last typing session and can continue to improve. Our evaluation also demonstrates the positive effect of the reduced audio feedback and the auto-correction algorithm.","tags":["Blind","Braille","text input","mobile devices","sketch; gesture"],"title":"BrailleSketch: A Gesture-based Text Entry Method for People with Visual Impairments","type":"publication"},{"authors":["Yue Zhao","Zhongtian Qiu","Yiqing Yang","Weiwei Li","Mingming Fan"],"categories":null,"content":" ","date":1503187200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1503187200,"objectID":"7a1e60893180d7f6e95d2925a77e012f","permalink":"/zh/publication/mobileauthentication/","publishdate":"2017-08-20T00:00:00Z","relpermalink":"/zh/publication/mobileauthentication/","section":"publication","summary":"The emergence of smartwatches poses new challenges to information security. Although there are mature touch-based authentication methods for smartphones, the effectiveness of using these methods on smartwatches is still unclear. We conducted a user study (n=16) to evaluate how authentication methods (PIN and Pattern), UIs (Square and Circular), and display sizes (38mm and 42mm) affect authentication accuracy, speed, and security. Circular UIs are tailored to smartwatches with fewer UI elements. Results show that 1) PIN is more accurate and secure than Pattern; 2) Pattern is much faster than PIN; 3) Square UIs are more secure but less accurate than Circular UIs; 4) display size does not affect accuracy or speed, but security; 5) Square PIN is the most secure method of all. The study also reveals a security concern that participants' favorite method is not the best in any of the measures. We finally discuss implications for future touch-based smartwatch authentication design.","tags":["Authentication","security","smartwatch","empirical study"],"title":"An Empirical Study of Touch-based Authentication Methods on Smartwatches","type":"publication"},{"authors":["Yue Zhao","Zhongtian Qiu","Yiqing Yang","Weiwei Li","Mingming Fan"],"categories":null,"content":" ","date":1499731200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1499731200,"objectID":"ffba083093a5748e3e29af17eb8365bd","permalink":"/zh/publication/foot/","publishdate":"2017-07-11T00:00:00Z","relpermalink":"/zh/publication/foot/","section":"publication","summary":"User-defined foot-gesture is a promising approach to interacting with mobile devices when both hands are occupied. In this research, we first present a survey to identify how often interacting with mobile devices is needed when both hands are busy and how many tasks people commonly would like to do on the devices in such situations. We then present a study to compare a traditional approach with foot-gesture interaction in a simulated hands-occupied scenario. Results show that foot-gesture saved over 70% of the time compared with the traditional approach and was perceived more useful and satisfying.","tags":["Foot gestures","hands-busy","user-defined gestures"],"title":"An Empirical Study of Foot Gestures for Hands-Occupied Mobile Interaction","type":"publication"},{"authors":["Mingming Fan","Khai N. Truong","Abhishek Ranjan"],"categories":null,"content":" ","date":1486771200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1486771200,"objectID":"804a4855417f7ce39f2f0cea5d12d5d3","permalink":"/zh/publication/liquid/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/zh/publication/liquid/","section":"publication","summary":"One potential way to track or infer the amount of intake of different fluids—ranging from water to caffeinated beverages as well as liquid medication—is to determine the level of these fluids in the containers from which the user may consume them. To do so, we propose four capacitive sensor designs that can be easily added to the outside of containers of different shapes and sizes. Our evaluation of these four sensor designs with containers made of different materials (i.e., ceramic, glass, paper, plastic) show that a multi-layer perceptron model can be learned to accurately predict liquid level with correlation coefficients higher than 0.98 and relative absolute error less than 16%. We then demonstrate that a prototype of this sensor can be constructed and affixed at the bottom of a liquid medication bottom to measure the amount of liquid medication in it","tags":["capacitive sensing","water intake","liquid medication","liquid level","design","measurement","container"],"title":"Exploring the Use of Capacitive Sensing to Externally Measure Liquid Level in Fluid Containers","type":"publication"},{"authors":["Mingming Fan","Khai N. Truong"],"categories":null,"content":" ","date":1423612800,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1423612800,"objectID":"a9dbd29d2c0ce3ce9bea84e200731a70","permalink":"/zh/publication/soqr/","publishdate":"2015-02-11T00:00:00Z","relpermalink":"/zh/publication/soqr/","section":"publication","summary":"In this paper, we present SoQr, a sensor that can be attached to an external surface of a household item to estimate the amount of content inside it. The sensor consists of a speaker and a microphone. It outputs a short duration sine wave probing sound to excite a container and its content, and then records the container’s impulse response. SoQr then extracts Mean Mel-Frequency Cepstral Coefficients from impulse response recordings of a container with different content levels and learns a support vector machine classifier. Results from a 10-fold cross validation of the prediction models on 19 common household items demonstrate that SoQr can correctly estimate the content level for these products with an average overall F-Measure above 0.96. We then further evaluated SoQr’s robustness in different usage scenarios to gain an understanding of how the system performs and specific challenges that might arise when users interact with these products and the sensor.","tags":["Content level measurement","container","active probing","impulse response"],"title":"SoQr: Sonically Quantifying the Content Level inside Containers","type":"publication"},{"authors":["Mingming Fan","Alexander T. Adams","Khai N. Truong"],"categories":null,"content":" ","date":1392076800,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1392076800,"objectID":"4ae9664b3b5d0158fb516d6b92210b3e","permalink":"/zh/publication/restroom/","publishdate":"2014-02-11T00:00:00Z","relpermalink":"/zh/publication/restroom/","section":"publication","summary":"Although there are clear benefits to automatic image capture services by wearable devices, image capture sometimes happens in sensitive spaces where camera use is not appropriate. In this paper, we tackle this problem by focusing on detecting when the user of a wearable device is located in a specific type of private space---the public restroom---so that the image capture can be disabled. We present an infrastructure-independent method that uses just the microphone and the speaker on a commodity mobile phone. Our method actively probes the environment by playing a 0.1 seconds sine wave sweep sound and then analyzes the impulse response (IR) by extracting MFCCs features. These features are then used to train an SVM model. Our evaluation results show that we can train a general restroom model which is able to recognize new restrooms. We demonstrate that this approach works on different phone hardware. Furthermore, the volume levels, occupancy and presence of other sounds do not affect recognition in significant ways. We discuss three types of errors that the prediction model has and evaluate two proposed smoothing algorithms for improving recognition.","tags":["Restroom detection","room impulse response","active probing","pattern recognition"],"title":"Public Restroom Detection on Mobile Phone via Active Probing","type":"publication"},{"authors":["Mingming Fan","Qiong Liu","Hao Tang","Patrick Chiu"],"categories":null,"content":" ","date":1391990400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1391990400,"objectID":"760308690608f5688a3332e80b6cb8fe","permalink":"/zh/publication/hifi/","publishdate":"2014-02-10T00:00:00Z","relpermalink":"/zh/publication/hifi/","section":"publication","summary":"In this paper, we propose the HiFi which enables users to interact with surrounding physical objects. It uses coded light to encode the position of user's mobile device in an environment. By attaching a tiny light sensor on a user's mobile device, the user can attach digital information to arbitrary static physical objects or retrieve/modify the attached information as well. With this system, a family member may attach a digital maintenance schedule to a fish tank or indoor plants so that all family members may retrieve that for maintenance reference. In a store, a store manager may use such a system to attach price tag, discount information and multimedia content to any products and customers can get the attached information by moving their phone close to the focused product. Similarly, a museum can use this system to provide extra information of displayed items to visitors. Different from computer vision based systems, HiFi does not depend on object's texture and illumination, etc. Different from regular barcode approaches, HiFi does not require extra physical attachments that may change an object's native appearance. HiFi has much higher spatial resolution for distinguishing close objects or attached parts of the same object. As the HiFi system can track a mobile device at 80 positions per second, it also has much faster response than any above listed system.","tags":["Coded Light","Mobile Computing","Interactive Space","AR"],"title":"HiFi: Hide and Find Digital Content Associated with Physical Objects via Coded Light","type":"publication"},{"authors":["Mingming Fan","Dana Gravem","Dan Cooper","Donald J Patterson"],"categories":null,"content":" ","date":1328918400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1328918400,"objectID":"1914a1f1040810d4d947a5facca48d2a","permalink":"/zh/publication/erlang-cox/","publishdate":"2012-02-11T00:00:00Z","relpermalink":"/zh/publication/erlang-cox/","section":"publication","summary":"In this paper we demonstrate a Markov model based technique for recognizing gestures from accelerometers that explicitly represents duration. We do this by embedding an Erlang-Cox state transition model, which has been shown to accurately represent the first three moments of a general distribution, within a Dynamic Bayesian Network (DBN). The transition probabilities in the DBN can be learned via Expectation-Maximization or by using closed-form solutions. We test this modeling technique on 10 hours of data collected from accelerometers worn by babies pre-categorized as high-risk in the Newborn Intensive Care Unit (NICU) at UCI. We show that by treating instantaneous machine learning classification values as observations and explicitly modeling duration, we improve the recognition of Cramped Synchronized General Movements, a motion highly correlated with an eventual diagnosis of Cerebral Palsy.","tags":["Gesture Recognition","Health","Sensors","User Modeling"],"title":"Augmenting Gesture Recognition with Erlang-Cox Models to Identify Neurological Disorders in Premature Babies","type":"publication"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":-62135596800,"objectID":"8576ec274c98b3831668a172fa632d80","permalink":"/zh/about/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/zh/about/","section":"","summary":"","tags":null,"title":"","type":"widget_page"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":-62135596800,"objectID":"c1d17ff2b20dca0ad6653a3161942b64","permalink":"/zh/people/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/zh/people/","section":"","summary":"","tags":null,"title":"","type":"widget_page"}]